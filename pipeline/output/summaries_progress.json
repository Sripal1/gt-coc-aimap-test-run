{"-JPZ21IAAAAJ": {"keywords": "robotics, robot modeling, robot control, visual servoing, image-based visual servo control (IBVS), position-based visual servo control (PBVS), robot motion planning, multi-robot systems, SLAM (simultaneous localization and mapping), biologically inspired aerial robotics", "summary": "## Overview\nSeth Hutchinson is a senior robotics researcher and academic leader at **Northeastern University**, where he contributes to the university’s research and graduate training mission in **robotics** and autonomous systems. His work centers on the theory and practice of *robot autonomy*, with particular emphasis on <u>visual servoing</u>, <u>robot motion planning</u>, and the mathematical foundations that connect **perception**, **control**, and **real-world robot behavior**. Across highly cited books and tutorial papers, Hutchinson has helped define how modern robots are modeled, how they use vision in feedback loops, and how they plan and coordinate motion in complex environments—bridging *rigorous theory* with **implementable algorithms**.\n\n---\n\n## Research Areas\nHutchinson’s research spans core areas of robotics in which perception and decision-making must be tightly integrated with control. A central theme is <u>vision-based control</u>, especially **visual servo control**, where camera measurements are used directly in the feedback loop to regulate robot motion. His tutorial and survey-style works—most notably *“A tutorial on visual servo control”* (2002) and the two-part series *“Visual servo control. I. Basic approaches”* (2006) and *“Visual servo control. II. Advanced approaches [Tutorial]”* (2007)—systematize key formulations such as image-based vs. position-based servoing, stability considerations, and practical controller design choices. Complementing this, he has developed methods that address known limitations of image-based servoing, including trajectory distortions and indirect Cartesian behavior, as in *“A new partitioned approach to image-based visual servo control”* (2002) and task-level formulations such as *“Visual compliance: Task-directed visual servo control”* (2002). Beyond visual servoing, Hutchinson has made substantial contributions to <u>robot motion planning</u> and multirobot coordination, including planning in dynamic or changing environments (e.g., *“A framework for real-time path planning in changing environments”* (2002)) and optimization/coordination for teams (e.g., *“Optimal motion planning for multiple robots having independent goals”* (2002)). His later work also extends to bio-inspired aerial robotics and long-duration autonomy, exemplified by bat-inspired platforms (*“A biomimetic robotic platform to study flight specializations of bats”* (2017); *“Bat Bot (B2), a biologically inspired flying machine”* (2016)) and constraint-based perspectives on sustained operation (*“Robot ecology: Constraint-based control design for long duration autonomy”* (2018)).\n\n---\n\n## Notable Works\n- **Foundational textbooks shaping robotics education and practice:** Hutchinson is a principal author of *“Robot Modeling and Control”* (2006; and *2nd edition*, 2020), a widely used reference that formalizes <u>kinematics</u>, <u>dynamics</u>, and **feedback control** for robotic systems and has become a standard foundation for graduate and advanced undergraduate robotics curricula.\n- **Defining modern visual servoing through canonical tutorials and frameworks:** His highly cited *“A tutorial on visual servo control”* (2002) and the tutorial series *“Visual servo control. I. Basic approaches”* (2006) and *“Visual servo control. II. Advanced approaches [Tutorial]”* (2007) consolidated the field’s core models, stability intuitions, and design tradeoffs, establishing a common language for <u>vision-in-the-loop control</u>.\n- **Advances in robot motion planning and multi-robot optimization:** In *“Principles of robot motion: theory, algorithms, and implementations.”* (2005), Hutchinson (with coauthors) connected the mathematical structure of planning to implementable algorithms; related highly cited papers such as *“Optimal motion planning for multiple robots having independent goals”* (2002) further developed optimization-based viewpoints for coordinated multi-robot behavior.\n\n---\n\n## Academic Background\nAffiliated with **Northeastern University**, Hutchinson’s academic trajectory reflects sustained, field-defining impact across multiple decades of robotics research, as indicated by his extensive publication record and **33,106** total citations. His career includes influential periods producing seminal work in *vision-based robot control* and *motion planning*, with early contributions to sensing strategy planning in robot work cells (e.g., *“Planning Sensing Strategies in a Robot Work Cell with Multi-Sensor Capabilities”* (1989)) and later landmark tutorial syntheses that helped codify visual servoing as a core robotics discipline. The breadth of his publications—from rigorous tutorials and textbooks to algorithmic planning papers and bio-inspired robotics systems—suggests deep interdisciplinary engagement across **robotics**, **computer vision**, and **control theory**, alongside ongoing leadership in shaping research directions and educating generations of robotics researchers through widely adopted scholarly resources."}, "-ZqZyP0AAAAJ": {"keywords": "Human-Computer Interaction, Health, HCI for Development (HCI4D), Global Health Informatics, Digital Mental Health, Feminist HCI, Intersectionality in Design, Menstrual Health Technologies, Mobile and Social Media Communication (Emojis/Stickers), ICTD and Low-Resource Infrastructures (Community Networks/Offline Internet)", "summary": "## Overview\nNeha Kumar is an Associate Professor at **Georgia Tech**, where she leads research at the intersection of **Human-Computer Interaction (HCI)** and *global health*, with a sustained commitment to understanding technology design and use in under-resourced contexts. Her scholarship centers on <u>HCI for Development (HCI4D)</u> and <u>critical, justice-oriented approaches to computing</u>, combining rigorous fieldwork with theory-driven analysis to examine how digital systems shape—and are shaped by—everyday life, care infrastructures, and sociocultural norms. Across projects spanning South Asia, China, Cuba, and beyond, Kumar’s work foregrounds *equity*, *contextual accountability*, and the lived realities of communities often excluded from dominant technology narratives.\n\n---\n\n## Research Areas\nKumar’s research areas encompass qualitative and mixed-methods inquiry into communication, care, and sociotechnical infrastructures in the Global South, with particular depth in health and development settings. In mobile communication and cultural expression, she has examined how platforms reconfigure language and meaning-making, as in “**Goodbye Text, Hello Emoji: Mobile Communication on WeChat in China**” (2017), which documents the rapid uptake of emoji and stickers and the shifting role of text across rural-to-urban contexts. In **digital health and community health**, her work investigates how technologies mediate access, trust, and labor in frontline systems—ranging from maternal health interventions in Uttar Pradesh (“**Mobile phones for maternal health in rural India**,” 2015; “**Projecting health: community-led video education for maternal health**,” 2015) to the politics of data collection and knowledge coordination in community health ecosystems (“**Engaging Solidarity in Data Collection Practices for Community Health**,” 2018; “**Bridging disconnected knowledges for community health**,” 2018). Kumar also advances critical and feminist perspectives in HCI, articulating frameworks for <u>intersectional design</u> (“**Designing for Intersections**,” 2018) and interrogating how structural power shapes technology-mediated participation (“**Privacy, Patriarchy, and Participation on Social Media**,” 2019). More recently, she has contributed to critical scholarship on <u>digital mental health</u>—including decolonial critiques (“**From treatment to healing: envisioning a decolonial digital mental health**,” 2022), assessments of emerging AI deployments (“**Benefits and Harms of Large Language Models in Digital Mental Health**,” 2023), and empirical attention to how people in distress engage LLM chatbots (“**The typing cure**,” 2025). Complementing these themes, her work on constrained and alternative infrastructures—such as Cuba’s offline internet (“**El Paquete Semanal**,” 2018) and the maintenance practices of Havana’s community networks (“**If it rains, ask grandma to disconnect the nano**,” 2019)—extends HCI4D beyond access metrics toward a nuanced account of <u>infrastructure, care, and repair</u>.\n\n---\n\n## Notable Works\n- **Mapping and shaping the HCI4D field:** In “**The ins and outs of HCI for development**” (2016), Kumar provides a field-defining empirical analysis of HCI4D literature, clarifying research trajectories, methodological patterns, and persistent tensions in designing for under-served and under-resourced communities.  \n- **Community-led maternal health interventions:** Through “**Projecting health: community-led video education for maternal health**” (2015) and the complementary mixed-methods account in “**Mobile phones for maternal health in rural India**” (2015), Kumar advances a model of *Community-led Video Education (CVE)* and documents how locally grounded media practices can support maternal and newborn health under real-world constraints.  \n- **Critical reorientation of digital mental health and AI:** In “**From treatment to healing: envisioning a decolonial digital mental health**” (2022) and “**Benefits and Harms of Large Language Models in Digital Mental Health**” (2023), Kumar and collaborators articulate how technology-mediated care can reproduce historical injustices, offering a justice-centered agenda for evaluating and designing AI-enabled mental health systems.\n\n---\n\n## Academic Background\nAs a highly cited scholar (over **6,000** citations), Kumar’s academic trajectory reflects sustained influence across **HCI**, **CSCW**, and *global health* research communities, with a publication record spanning empirical field studies, design interventions, and conceptual frameworks. Her work’s geographic breadth—across India, Bangladesh, China, Cuba, and other contexts—signals long-term collaborations with local organizations, community stakeholders, and interdisciplinary partners in public health, development, and social science. The evolution of her scholarship from early studies of mobile media ecosystems and gendered access (e.g., “**The mobile media actor-network in urban India**,” 2013; “**The gender-technology divide or perceptions of non-use?**,” 2015) to later contributions on feminist and decolonial theory in HCI (“**Designing for Intersections**,” 2018; “**Braving citational justice in human-computer interaction**,” 2021) suggests a career marked by increasing emphasis on <u>power, representation, and justice in knowledge production</u>. At Georgia Tech, she is affiliated with the broader HCI research ecosystem and contributes to the intellectual leadership of the field through widely referenced analyses of HCI4D, health equity, and the societal implications of emerging AI in care contexts."}, "06hgT8gAAAAJ": {"keywords": "Security, Operating Systems, Systems, Fuzzing, Concolic Execution, Symbolic Execution, Trusted Execution Environments (Intel SGX), Side-Channel Attacks, Control-Flow Integrity, Persistent Memory File Systems", "summary": "## Overview\nTaesoo Kim is a faculty researcher at the **Georgia Institute of Technology**, where he leads a systems security research program focused on building *practical, high-impact defenses and testing techniques* for real-world software stacks. His work sits at the intersection of **security**, **operating systems**, and **systems**, with a consistent emphasis on translating fundamental insights into deployable mechanisms for commodity platforms. Across a body of work exceeding **12,000 citations**, Kim has advanced <u>trusted execution environments</u>, <u>fuzzing and concolic execution</u>, and <u>OS-level hardening</u>, often targeting the gap between theoretical security guarantees and the realities of modern kernels, runtimes, and cloud infrastructure.\n\n---\n\n## Research Areas\nKim’s research spans several tightly connected thrusts in systems security. A major theme is the security of <u>trusted execution environments</u>, particularly Intel **SGX**, where his papers demonstrate both powerful attacks and robust mitigations: he helped reveal fine-grained enclave control-flow leakage via **branch shadowing** (“Inferring fine-grained control flow inside SGX enclaves with branch shadowing,” 2017) and contributed defenses that harden enclaves against <u>controlled-channel attacks</u> (“T-SGX,” 2017) while also improving exploit resistance through enclave-oriented randomization (“SGX-shield,” 2017) and studying exploitation techniques against enclaves (“Hacking in darkness,” 2017). Complementing this, Kim has made influential contributions to <u>software testing for security</u>, especially **hybrid fuzzing** that combines fuzzing with symbolic/concolic execution; his work on **QSYM** (“QSYM: A practical concolic execution engine tailored for hybrid fuzzing,” 2018) addresses longstanding scalability barriers that limited concolic methods in practice, and related efforts propose OS-level primitives to improve fuzzing throughput (“Designing new operating primitives to improve fuzzing performance,” 2017). He has also advanced <u>system-level side-channel defenses</u> for multi-tenant environments, notably cloud cache side channels (“STEALTHMEM,” 2012), and contributed to kernel and low-level hardening research, including breaking widely deployed randomization defenses (“Breaking kernel address space layout randomization with Intel TSX,” 2016) and strengthening control-flow protection (“Enforcing unique code target property for control-flow integrity,” 2018). In parallel, his systems work extends into performance-critical OS subsystems—especially file systems and persistent memory—where he has helped reshape the design space for PM-aware storage stacks (“SplitFS,” 2019) and reliability transformations for concurrent indexes (“Recipe,” 2019), reflecting a broader agenda of security *and* efficiency co-design in modern systems.\n\n---\n\n## Notable Works\n- **Hybrid fuzzing at scale via practical concolic execution:** In “**QSYM: A practical concolic execution engine tailored for hybrid fuzzing**” (2018), Kim and collaborators deliver a concolic execution engine engineered for hybrid fuzzing workflows, substantially improving scalability and making symbolic reasoning effective in real vulnerability discovery pipelines.\n- **Breaking SGX confidentiality through control-flow inference:** “**Inferring fine-grained control flow inside SGX enclaves with branch shadowing**” (2017) demonstrates how microarchitectural leakage can expose sensitive enclave behavior, sharpening the community’s understanding of <u>side-channel and controlled-channel risks</u> in TEEs.\n- **System-level mitigation of cloud cache side channels:** “**STEALTHMEM: System-Level protection against Cache-Based side channel attacks in the cloud**” (2012) provides an influential defense framework for multi-tenant settings, addressing practical threat models created by shared hardware resources in cloud platforms.\n\n---\n\n## Academic Background\nBased at **Georgia Tech**, Kim has developed a research portfolio characteristic of a senior systems-security academic, marked by sustained publication in top-tier security and systems venues and broad influence across both communities. The trajectory of his work—from early system recovery and provenance-style approaches (e.g., “Intrusion recovery using selective re-execution,” 2010) to cloud side-channel defenses (“STEALTHMEM,” 2012), to intensive contributions in kernel hardening, SGX/TEE security, and modern fuzzing—suggests deep training in **operating systems** and **security engineering** with strong ties to the systems research ecosystem. His repeated, high-impact contributions to SGX (attacks, mitigations, and enabling platforms such as “OpenSGX,” 2016), together with widely cited advances in hybrid fuzzing (“QSYM,” 2018) and persistent-memory file systems (“SplitFS,” 2019), indicate substantial recognition and leadership in the field, including collaborations spanning security, architecture, and storage systems communities."}, "1GsJvtwAAAAJ": {"keywords": "Cyber Forensics, Systems Security, Malware Analysis, Memory Forensics, Smartphone Security, Android Forensics, iOS App Vetting, Supply Chain Security, Binary Rewriting, Attack Reconstruction from Logs", "summary": "## Overview\nBrendan D. Saltaformaggio is an Associate Professor at the Georgia Institute of Technology, where he leads a research program at the intersection of **cyber forensics** and **systems security**. His work centers on building *practical, evidence-driven* security techniques that can withstand real adversaries and real operational constraints, with a particular emphasis on <u>memory forensics</u>, <u>attack reconstruction</u>, and <u>malware analysis</u>. Across a publication record exceeding **1,800 citations**, Saltaformaggio’s research consistently advances methods for extracting high-fidelity security insights from complex systems artifacts—such as logs, runtime traces, and memory images—while maintaining *deployability* in production environments.\n\n---\n\n## Research Areas\nSaltaformaggio’s research spans multiple, tightly connected themes in modern security and forensics. A major pillar is <u>forensic reconstruction from heterogeneous evidence</u>, exemplified by work on correlating dispersed telemetry to recover multi-stage intrusions, such as **Hercule: Attack story reconstruction via community discovery on correlated log graph** (2016), which targets the spatio-temporal fragmentation typical of advanced attacks. A second pillar is <u>mobile and memory forensics</u>, where his group demonstrated how volatile artifacts can reveal both user actions and application state—ranging from reconstructing Android GUI history in **Screen after Previous Screens** (2016) and recovering semantic app context in **GUITAR** (2015), to extracting photographic evidence in **VCR** (2015) and rendering interpretable forensic structures via **DSCRETE** (2014). A third pillar addresses <u>systems security mechanisms that connect detection to causality</u>, including causality inference in **LDX: Causality Inference by Lightweight Dual Execution** (2016) and exploit root-cause localization in production through **ARCUS** (2021). Complementing these lines, his work also examines ecosystem-scale abuse and software supply chain risk, notably in **Towards Measuring Supply Chain Attacks on Package Managers for Interpreted Languages** (2021), as well as empirical security measurement in web and CMS ecosystems (e.g., malicious plugin markets and CMS rollback/compromise-window analysis). Collectively, these contributions reflect a sustained focus on turning low-level system artifacts into *actionable* security conclusions for investigators, defenders, and developers.\n\n---\n\n## Notable Works\n- **Quantifying software supply-chain abuse in package ecosystems**: In *“Towards Measuring Supply Chain Attacks on Package Managers for Interpreted Languages”* (2021), Saltaformaggio and collaborators develop measurement methodology to characterize how attackers exploit interpreted-language package managers—helping transform supply-chain security from anecdote-driven reporting into evidence-based risk assessment.\n- **Attack-story reconstruction from correlated logs at scale**: *“Hercule: Attack story reconstruction via community discovery on correlated log graph”* (2016) advances intrusion forensics by modeling cross-log relationships as graphs and recovering coherent attack narratives from footprints that are otherwise fragmented across data sources and time.\n- **Bridging detection and developer action through symbolic root-cause analysis**: In *“ARCUS: Symbolic Root Cause Analysis of Exploits in Production Systems”* (2021), his group addresses the gap between runtime monitor alerts and the true vulnerability trigger, enabling more dependable bug reports and accelerating remediation by linking late-stage symptoms back to earlier root causes.\n\n---\n\n## Academic Background\nAt Georgia Tech, Saltaformaggio has built a research portfolio that reflects both depth in core **systems security** and breadth across applied **forensics** and **malware analysis**, with sustained impact indicated by his citation record and repeated contributions to high-visibility security problems (mobile device evidence recovery, intrusion reconstruction, exploit analysis, and supply-chain measurement). His publication trajectory—from early work on virtualization and cloud-side channels and storage (e.g., hypervisor-based defenses and virtualized I/O systems) to later efforts in memory-centric forensics, production exploit diagnosis, and ecosystem-scale measurement—suggests a career shaped by close engagement with real-world constraints and operational security needs. His academic affiliations are anchored at the Georgia Institute of Technology, where he contributes to the broader cybersecurity research community through a body of work spanning mobile platforms (Android/iOS), cloud and virtualized infrastructure, and software ecosystems, consistently emphasizing *scientific rigor* in measurement and <u>deployable</u> approaches to security and forensic investigation."}, "1kW15d8AAAAJ": {"keywords": "software engineering, software reuse, software component classification and retrieval, requirements engineering, viewpoint resolution, software design methods, software maintenance and reverse engineering, artificial intelligence in software engineering, software engineering education and curriculum, science and technology policy", "summary": "## Overview\nPeter A. Freeman is a senior researcher and academic leader associated with **Georgia Tech** and **UC Irvine**, whose career spans foundational contributions to **software engineering** and later influential work at the intersection of *computing institutions* and **science policy**. His scholarship centers on making software development more systematic and scalable through <u>software reuse</u>, <u>requirements engineering</u>, and *design-centered* methods, while also engaging broader questions about the computing profession and national capacity in areas such as the <u>IT workforce</u> and research infrastructure. Across these themes, Freeman’s work is marked by an emphasis on **rigorous conceptual frameworks** and *practical mechanisms* that connect theory to real-world software-intensive systems.\n\n---\n\n## Research Areas\nFreeman’s research areas cohere around the idea that complex software systems require disciplined representations of knowledge, process, and design intent. In **software reuse**, his highly cited work “**Classifying software for reusability**” (1987) addresses the retrieval bottleneck that precedes reuse, proposing a faceted classification approach to make components discoverable—an early and enduring contribution to <u>component classification and retrieval</u>. Complementing this, “**Reusable software engineering: Concepts and research directions**” (1983) broadens reuse beyond code to include specifications, designs, and other development artifacts, framing reuse as an ecosystem of reusable information and processes rather than a single technique. In **requirements engineering**, Freeman advanced early validation for complex systems via “**Requirements validation through viewpoint resolution**” (1991), which treats requirements as inherently multi-perspective and uses structured reconciliation of differences to reduce downstream failures—an approach aligned with <u>viewpoint-based elicitation</u> and *conflict resolution* in requirements. His long-standing interest in <u>design as a central intellectual activity</u> is visible in earlier work such as “**A model for functional reasoning in design**” (1971) and later reflections like “**A science of design for software-intensive systems**” (2004), arguing for teachable, analytical design processes. Freeman also explored the relationship between **artificial intelligence** and software development in “**A framework for incremental progress in the application of AI to software engineering**” (1988), emphasizing stepwise, feasible integrations of AI methods into SE practice. Beyond technical SE, his policy-oriented scholarship includes “**The Supply of Information Technology Workers in the United States.**” (1999), which situates IT labor supply and demand within political and economic context, and later institutional history work such as “Computing and the National Science Foundation, 1950–2016” (2019), reflecting sustained engagement with *how computing capacity is built at national scale*. (Notably, some later highly cited biomedical-informatics papers in the list—e.g., “VariantValidator” (2018)—suggest cross-domain collaborations in computational tooling and standards, though Freeman’s core citation anchors remain in software engineering and policy.)\n\n---\n\n## Notable Works\n- **Software reuse through discoverability:** In “**Classifying software for reusability**” (1987), Freeman tackled the prerequisite to reuse—*finding* reusable components—by proposing a faceted classification and retrieval scheme that became a touchstone for <u>reuse repositories</u> and component discovery research.\n- **Requirements validation via multi-viewpoint reconciliation:** “**Requirements validation through viewpoint resolution**” (1991) introduced viewpoint resolution (VPR) as a structured method for early requirements validation in complex systems, leveraging differences among stakeholder viewpoints as actionable signals for <u>requirements defects</u> and inconsistencies.\n- **Framing reuse as a comprehensive engineering discipline:** “**Reusable software engineering: Concepts and research directions**” (1983) articulated reuse as the broad reuse of diverse software-development information (not only code), defining classes of reusable artifacts and outlining research tasks that helped shape <u>reuse-oriented software engineering</u> agendas.\n\n---\n\n## Academic Background\nFreeman’s publication record indicates a long academic trajectory beginning with foundational work on design reasoning in the early 1970s (e.g., “A model for functional reasoning in design,” 1971), expanding through the 1980s into major contributions on **software design**, **maintenance**, and especially <u>software reusability</u> (including tutorials and conceptual syntheses), and maturing in the 1990s into **requirements engineering** and systems-scale concerns. His affiliations with **Georgia Tech** and **UC Irvine**, alongside influential policy-facing publications such as “The Supply of Information Technology Workers in the United States.” (1999) and later historical/institutional analyses of computing’s public funding ecosystem (2019), suggest a career that bridges *research, education, and national-level computing policy discourse*. With **4,887 total citations**, Freeman’s impact reflects both enduring technical contributions—particularly in reuse and requirements validation—and a broader role in shaping how the field understands software engineering as a disciplined, design-centered profession with societal and workforce implications."}, "1vwuDM4AAAAJ": {"keywords": "Cryptography, security, secure computation, zero knowledge proofs, garbled circuits, secure two-party computation (2PC), secure multi-party computation (MPC), oblivious transfer (OT) extension, private set intersection (PSI), oblivious pseudorandom functions (OPRF)", "summary": "## Overview\nVladimir Kolesnikov is a researcher at the Georgia Institute of Technology, where he advances the design and implementation of **modern cryptographic protocols** with an emphasis on *practical efficiency and deployability*. His work sits at the intersection of **applied cryptography** and **systems security**, with a sustained focus on <u>secure computation</u>—especially **two-party secure function evaluation (SFE)** and **secure multi-party computation (MPC)**—as well as *efficient* <u>zero-knowledge proofs</u> and privacy-preserving data systems. Across a highly cited body of work (8626 total citations), he is particularly known for foundational optimizations to **garbled circuits** and for bridging theory-to-practice in privacy technologies that protect data, queries, and computation.\n\n---\n\n## Research Areas\nKolesnikov’s research centers on making <u>secure computation</u> fast enough for real-world use, spanning both protocol design and system building. In **garbled-circuit-based 2PC**, he co-developed major efficiency breakthroughs, most notably the *free-XOR* technique in “**Improved garbled circuit: Free XOR gates and applications**” (2008), which enables XOR gates to be evaluated essentially without cryptographic cost and reshaped the performance profile of Yao-style protocols. He extended this line through improved circuit components and arithmetic/comparison building blocks in “**Improved garbled circuit building blocks and applications to auctions and computing minima**” (2009), enabling efficient secure comparisons and numeric operations central to auctions and decision tasks. Complementing garbling advances, his work on **oblivious transfer (OT) extension** (e.g., “**Improved OT extension for transferring short secrets**,” 2013) targets the communication/computation bottlenecks that dominate large-scale secure computation deployments.  \nA second major thrust is **private set operations**—especially <u>private set intersection (PSI)</u>—using lightweight symmetric-key and OT-hybrid techniques. In “**Efficient batched oblivious PRF with applications to private set intersection**” (2016), he develops efficient batched OPRF mechanisms that directly power scalable PSI, and this direction broadens into multi-party settings in “**Practical multi-party private set intersection from symmetric-key techniques**” (2017) and adjacent set functionality such as private set union (2019).  \nA third area is <u>zero-knowledge</u>, with an emphasis on *efficient* proof systems for circuit statements and post-quantum-relevant constructions. In “**Improved non-interactive zero knowledge with applications to post-quantum signatures**” (2018), he advances the “MPC-in-the-head” paradigm to obtain more practical NIZK proofs based largely on symmetric primitives. This protocol-level work connects to system-building efforts in privacy-preserving databases, exemplified by “**Blind Seer: A scalable private DBMS**” (2014), which addresses query privacy and data protection for structured search at scale, reflecting a consistent theme: translating cryptographic techniques into usable security systems.\n\n---\n\n## Notable Works\n- **Free-XOR garbling optimization for SFE/2PC** — “*Improved garbled circuit: Free XOR gates and applications*” (2008): introduced the influential <u>free-XOR</u> technique, substantially reducing the cost of garbled-circuit evaluation and enabling faster, more practical secure computation implementations.\n- **Batched OPRF enabling efficient PSI** — “*Efficient batched oblivious PRF with applications to private set intersection*” (2016): developed lightweight batched OPRF protocols that became a key building block for high-performance <u>private set intersection</u> and related privacy-preserving analytics.\n- **Scalable privacy-preserving database querying** — “*Blind seer: A scalable private DBMS*” (2014): demonstrated a systems-oriented application of cryptography to build a private DBMS supporting query privacy and server-side data protection, helping define practical design points for secure search over large datasets.\n\n---\n\n## Academic Background\nBased at the Georgia Institute of Technology, Kolesnikov has built an academic profile characteristic of a senior applied-cryptography researcher with sustained contributions across premier cryptography and security venues over multiple decades (from early work on conditional oblivious transfer and interval computation in 2004 through major advances in garbling, OT extension, PSI, and NIZK in the 2010s and beyond). His publication trajectory—from foundational protocol primitives (e.g., gate-evaluation secret sharing and OT/GC optimizations) to full systems (e.g., private DBMS and privacy-preserving infrastructures)—suggests deep, long-term affiliations with both the **cryptography** and **computer security** research communities. The breadth and impact of his work, reflected in 8626 citations and multiple widely adopted techniques (notably free-XOR and efficient PSI/OPRF constructions), indicate strong recognition within the field and sustained influence on how *efficient* <u>secure computation</u> and <u>zero-knowledge</u> are engineered in practice."}, "2--dZP0AAAAJ": {"keywords": "Equity in CS Education, Coaching inclusive pedagogy, Broadening Participation in Computing, AP CS, AP Computer Science Principles (AP CSP), K-12 computer science education, Inclusive computer science pedagogy, CS teacher professional development and instructional coaching, Social justice and intersectionality in computing, Equity-focused CS education research methods and quality standards", "summary": "## Overview\nLien Diaz is the Director of the **Constellations Center for Equity in Computing** at Georgia Tech, where she leads research and field-building efforts focused on *transforming computing education systems* through <u>equity-centered policy, pedagogy, and teacher support</u>. Her scholarship examines how large-scale programs—especially **Advanced Placement (AP) Computer Science**—shape classroom practice and student opportunity, and how educators can be supported through **coaching** and professional learning to enact *inclusive* and *justice-oriented* computer science instruction. Across her work, Diaz emphasizes that broadening participation requires more than access; it requires <u>shared definitions of equity</u>, actionable instructional change, and sustained institutional accountability.\n\n---\n\n## Research Areas\nDiaz’s research spans **equity in CS education**, **broadening participation in computing (BPC)**, and the design of educator-facing supports that operationalize inclusive teaching. A major thread interrogates whether AP pathways deliver on their equity promises: in “Does the Advanced Placement Computer Science (CS) Principles course drive equitable and inclusive CS pedagogy, curriculum, and policy…” (2024), she analyzes AP CSP not only as a course but as a policy and credibility mechanism—probing how its adoption influences <u>equitable and inclusive pedagogy</u> and how curricular and assessment structures interact with participation goals. Complementing this systems lens, her conceptual work in “Going beyond the platitudes of equity…” (2019) argues for moving from rhetorical commitments to a *shared, practice-linked vision* of equity that addresses structural constraints (e.g., school organization, educator preparation, and curriculum). Diaz also contributes to the evolution of introductory CS at scale through scholarship on the **Computer Science Principles** movement—documenting course development, community formation, and alignment with college expectations (e.g., “CS principles: development and evolution of a course and a community,” 2013; “Advanced Placement computer science: the future of tracking the first year of instruction,” 2009; and “Re-imagining the first year of computing,” 2010). More recently, she extends this agenda into implementation infrastructures—advancing models for <u>equity-focused leadership</u> (“Setting the table for equity,” 2022), research quality and capacity-building (“Building Recommendations for Conducting Equity-Focused…K-12…Research,” 2023), and practitioner supports such as **instructional coaching** (“Coaching to Improve CS Teaching and Learning,” 2022), including emerging work on elementary coding and K–5 equity frameworks (2025).\n\n---\n\n## Notable Works\n- **Equity and AP CSP as a lever for systemic change:** In “Does the Advanced Placement Computer Science (CS) Principles course drive equitable and inclusive CS pedagogy, curriculum, and policy…” (2024), Diaz evaluates AP CSP’s role in shaping <u>equitable pedagogy</u> and broader policy/curriculum conditions tied to participation and inclusion.\n- **Defining equity beyond rhetoric in CS education:** “Going beyond the platitudes of equity: Developing a shared vision for equity in computer science education” (2019) is a foundational contribution that clarifies equity as an actionable, shared construct—linking systemic structures and educator preparation to inclusive learning environments.\n- **Building and documenting the CS Principles course-and-community movement:** In “CS principles: development and evolution of a course and a community” (2013), Diaz chronicles the development trajectory and community-building strategies behind CS Principles, helping establish a rigorous yet accessible introductory CS pathway designed to broaden participation.\n\n---\n\n## Academic Background\nDiaz’s publication record indicates long-standing leadership at the intersection of **K–12 computing education** and national-scale course design, with sustained contributions to AP Computer Science and the broader **CS Principles** initiative from the late 2000s onward (e.g., 2009–2017 work on AP CS alignment, first-year computing expectations, and the launch of AP CSP). Her trajectory reflects deep engagement with cross-sector communities that connect secondary education, higher education, and national organizations responsible for curriculum and assessment—experience that aligns with her current role directing an equity-focused center at Georgia Tech. As her scholarship shifted from course development and alignment (“Advanced Placement computer science: the future of tracking the first year of instruction,” 2009; “Re-imagining the first year of computing,” 2010) toward explicit **social justice**, **allyship**, and <u>equity governance</u> (“Social justice and equity in CS education…,” 2017; “Dismantling the Master’s House…,” 2021; “Setting the table for equity,” 2022), it also expanded to include research capacity-building and quality standards for equity-focused inquiry (“Building Recommendations…,” 2023). With **61 citations**, Diaz’s impact is concentrated in influential, community-facing venues that shape how educators, researchers, and policymakers conceptualize and implement equitable computer science education at scale."}, "2HLpBtQAAAAJ": {"keywords": "Pedagogical democratization, algorithmic transparency, graduate computer science research education, student research motivations, online graduate student research opportunities, equitable access to computing education, under-served group programming training, learning management systems (LMS) support models, migrant farmworker digital inclusion, broadening participation in computing", "summary": "## Overview\nDante Ciolfi (MS, MEd) is a researcher and practitioner affiliated with the **Georgia Institute of Technology**, working at the intersection of **computing education** and *equitable research participation*. His work centers on <u>pedagogical democratization</u> in graduate computer science—designing structures that expand who can access research opportunities and how research agendas are shaped—alongside commitments to **algorithmic transparency** as an educational and civic imperative. Across his recent scholarship, Ciolfi emphasizes *bottom-up student interest* as a driver of research engagement and advocates for models that make research pathways more navigable for diverse and distributed student populations.\n\n---\n\n## Research Areas\nCiolfi’s research spans inclusive pathways into computing and the redesign of research participation in graduate education. In **“Examining Student Interest and Motivations in Graduate Computer Science Research” (2025)**, he investigates how research topics are often set *top-down* by faculty priorities and grant structures, and contrasts this with the *bottom-up* motivations and interests of master’s students—highlighting misalignments that can limit student engagement and constrain the diversity of research questions pursued. Complementing this, **“Broadening CS Research Opportunities for Online Graduate Students” (2025)** addresses structural barriers faced by online learners, focusing on how conventional research experiences are organized around in-person lab norms and informal access to mentorship; the work foregrounds mechanisms to make research participation more equitable for remote master’s students seeking portfolio development and preparation for doctoral study. Earlier contributions on **computer programming training for under-served groups** (2017–2018) extend this equity orientation beyond the university, proposing an online learning and support framework that targets high-constraint contexts—beginning with U.S. migrant farm workers and their families—and arguing for design features (e.g., support and access considerations) that reduce friction for learners historically excluded from computing education.\n\n---\n\n## Notable Works\n- **Reframing graduate CS research participation around student-driven interests**: In *“Examining Student Interest and Motivations in Graduate Computer Science Research” (2025)*, Ciolfi surfaces tensions between faculty-defined research trajectories and student motivations, contributing evidence and framing for <u>pedagogical democratization</u> in master’s-level research engagement.\n- **Expanding research access for online master’s students**: In *“Broadening CS Research Opportunities for Online Graduate Students” (2025)*, he examines how traditional research opportunity structures can systematically disadvantage remote learners and articulates approaches to make research experiences more accessible, legible, and scalable in online programs.\n- **Designing equity-oriented online programming training with integrated support**: In *“Computer Programming Training for Under-Served Groups: the LMS/Support Model” (2017)* and the related lightning talk *“Computer Programming Training for Under-Served Groups” (2018)*, Ciolfi introduces and motivates a learning-management and support model aimed at delivering free programming education to under-served communities, with an initial focus on migrant farm worker families.\n\n---\n\n## Academic Background\nCiolfi holds graduate training denoted by **MS** and **MEd** credentials and is affiliated with the **Georgia Institute of Technology**, positioning him at the nexus of technical computing expertise and formal preparation in education. His publication trajectory suggests a sustained commitment to equity in computing education: early work (2017–2018) emphasizes community-facing instructional infrastructure for under-served groups, while his 2025 papers shift toward institutional change within graduate CS—especially the inclusion of online master’s students in research and the alignment of research pathways with student motivations. With emerging citation impact and a focused set of contributions, Ciolfi’s scholarly profile reflects an early- to mid-stage research agenda oriented toward <u>democratizing access</u> to computing education and research, informed by practical constraints, mentorship structures, and the broader values implied by **algorithmic transparency**."}, "2PxFftgAAAAJ": {"keywords": "Information Visualization, Visual Analytics, Human-Computer Interaction, Interaction Techniques for Visualization, Visualization Evaluation, Insight and Sensemaking, Algorithm Animation and Visualization, Software Visualization and Debugging, Natural Language Interfaces for Visualization, Ambient and Casual Visualization", "summary": "## Overview\nJohn Stasko is **Professor Emeritus of Interactive Computing** at the **Georgia Institute of Technology**, where he has been a long-standing contributor to the institute’s work at the intersection of **Human–Computer Interaction (HCI)** and **Information Visualization**. Across his career, he has advanced *human-centered* approaches to designing and evaluating interactive visual systems that help people think with data. His scholarship emphasizes <u>interaction as a core mechanism of visual reasoning</u>, developing principles, systems, and empirical evidence that explain how people explore, interpret, and communicate insights through **visual analytics** and **information visualization**.\n\n---\n\n## Research Areas\nStasko’s research spans foundational theory, empirical methods, and system building in visualization and HCI. A central theme is understanding <u>how interaction shapes analytic reasoning</u>, articulated in works such as “**Toward a deeper understanding of the role of interaction in information visualization**” (2007) and extended through reflective, theory-oriented perspectives on inquiry and interactive sensemaking (e.g., “**The science of interaction**,” 2009; “**Mental models, visual reasoning and interaction in information visualization: A top-down perspective**,” 2010). He has also helped formalize what people *do* during analysis—moving beyond representation-centered task taxonomies—through “**Low-level components of analytic activity in information visualization**” (2005), which characterizes recurring analytic actions that visualization systems should support. Complementing these conceptual contributions, Stasko has produced influential applied visual analytics systems, notably for investigative and text-centric analysis (“**Jigsaw: supporting investigative analysis through interactive visualization**,” 2008) and interactive multivariate exploration (“**Dust & magnet: multivariate information visualization using a magnet metaphor**,” 2005). Another major thread of his work addresses visualization evaluation and effectiveness, including evidence-focused examinations of algorithm visualization and animation (“**A meta-study of algorithm visualization effectiveness**,” 2002; “**Do algorithm animations assist learning?**,” 1993; “**Evaluating animations as student aids in learning computer algorithms**,” 1999) and broader critiques of evaluation goals that extend beyond efficiency and correctness (e.g., “**Beyond usability and performance**,” 2016). In later work, he has also explored natural language and communication support for visualization, including interfaces that combine visual analytics with generated explanatory statements (“**Augmenting visualizations with interactive data facts**,” 2018) and tools for natural-language-to-visualization specification (“**NL4DV**,” 2020). His publication record and high citation impact (29,049 citations) reflect sustained influence on both the intellectual foundations and practical design of interactive visualization.\n\n---\n\n## Notable Works\n- **Pioneering visualization for software debugging and fault localization:** In “**Visualization of test information to assist fault localization**” (2002), Stasko helped establish how interactive visual representations of execution/test behavior can guide developers toward suspicious code regions, shaping subsequent research on <u>software analytics</u> and debugging-oriented visualization.\n- **Defining interaction and analytic activity as first-class visualization concerns:** Through “**Toward a deeper understanding of the role of interaction in information visualization**” (2007) and “**Low-level components of analytic activity in information visualization**” (2005), he provided widely used conceptual frameworks for reasoning about *why* interaction matters and *what* analytic actions systems should support.\n- **Building influential visual analytics systems for investigative sensemaking:** “**Jigsaw: supporting investigative analysis through interactive visualization**” (2008) introduced a system-oriented vision for connecting entities, evidence, and hypotheses in document collections, becoming a canonical example of <u>interactive visual analytics for sensemaking</u>.\n\n---\n\n## Academic Background\nAs a senior academic at Georgia Tech’s Interactive Computing community, Stasko’s career reflects a sustained trajectory from early work in algorithm animation and software/program visualization (e.g., “**Tango: A framework and system for algorithm animation**,” 1990; “**Animating algorithms with XTANGO**,” 1992) to major contributions in information visualization, visual analytics, and HCI-centered evaluation. His publication history shows deep engagement with both *educational computing*—including empirical studies and meta-analyses on the effectiveness of algorithm animation (1993–2002)—and the maturation of modern InfoVis/VAST research agendas, including interaction theory, insight and sensemaking, and human-centered evaluation. His collaborations and edited volumes (e.g., “**Information visualization: human-centered issues and perspectives**,” 2008) indicate strong affiliations with the broader international visualization community and its premier venues (IEEE VIS and related conferences). The breadth of highly cited work across software visualization, interaction frameworks, investigative analytics, and visualization evaluation underscores a record of scholarly leadership consistent with a senior, award-recognized profile in the visualization and HCI fields, culminating in his emeritus appointment at Georgia Tech."}, "2QsddMIAAAAJ": {"keywords": "Materials discovery, Materials informatics, Computational materials science, AI for Science, Graph neural networks for materials, Density functional theory, Single-atom electrocatalysts, Metal nanoclusters, Perovskite oxide catalysis, Methane C–H activation", "summary": "## Overview\nVictor Fung is an **Assistant Professor** in **Computational Science and Engineering** at the **Georgia Institute of Technology**, where he leads the Fung Group (fung-group.org). His scholarship centers on *data- and physics-driven* approaches to accelerate **materials discovery**, combining **computational materials science** with **materials informatics** and **AI for Science**. Across catalytic and functional materials, his work emphasizes <u>structure–property relationships</u>, <u>mechanistic understanding</u>, and <u>predictive modeling</u>—linking **first-principles theory** with *modern machine learning* to guide the design of next-generation materials and catalysts.\n\n---\n\n## Research Areas\nFung’s research spans computational catalysis, nanocluster science, and machine learning for materials chemistry, with a recurring focus on extracting transferable principles from complex chemical systems. A major thrust of his work is <u>heterogeneous catalysis at the atomic limit</u>, including **single-atom catalysts** and atomically precise active sites, as reflected in studies of methane transformation under mild conditions (e.g., *“Single rhodium atoms anchored in micropores for efficient transformation of methane under mild conditions”*, 2018) and descriptor-driven understanding of methane activation on doped single atoms (*“Low-temperature activation of methane on doped single atoms: descriptor and prediction”*, 2018). In parallel, he has contributed to <u>atomically precise nanoclusters</u> and their growth, stability, and electronic structure, integrating first-principles insight with experimentally relevant questions (e.g., *“Insights into interfaces, stability, electronic properties, and catalytic activities of atomically precise metal nanoclusters from first principles”*, 2018; *“Understanding seed-mediated growth of gold nanoclusters at molecular level”*, 2017). More recently, his group has helped shape <u>AI-enabled materials modeling</u> by evaluating and advancing graph-based learning for chemistry and materials (e.g., *“Benchmarking graph neural networks for materials chemistry”*, 2021), as well as developing representations tied to electronic structure—using density-of-states-derived features for adsorption predictions (*“Machine learned features from density of states for accurate adsorption energy prediction”*, 2021) and extending to physically informed learning of electronic density of states (*“Physically informed machine learning prediction of electronic density of states”*, 2022). Collectively, these efforts connect *mechanism, descriptors, and scalable prediction* to support discovery workflows in catalysis, nanomaterials, and beyond.\n\n---\n\n## Notable Works\n- **Advancing machine learning foundations for materials chemistry** through rigorous evaluation of graph neural networks and their practical performance limits and opportunities, as established in *“Benchmarking graph neural networks for materials chemistry”* (2021).  \n- **Integrating electronic-structure-informed descriptors with machine learning** to enable accurate, data-efficient prediction of catalytic energetics—particularly adsorption energies—via *“Machine learned features from density of states for accurate adsorption energy prediction”* (2021).  \n- **Elucidating and enabling atomic-scale catalysis for methane conversion** by demonstrating efficient methane transformation under mild conditions using isolated metal sites, highlighted by *“Single rhodium atoms anchored in micropores for efficient transformation of methane under mild conditions”* (2018).  \n\n---\n\n## Academic Background\nFung’s academic trajectory reflects a sustained emphasis on *first-principles modeling* and *catalysis-informed materials design*, evolving into leadership in **materials informatics** and **AI for Science**. His publication record—spanning density functional theory studies of perovskites and transition-metal oxides, mechanistic and descriptor-based catalysis (including methane activation and oxidative dehydrogenation), and atomically precise nanoclusters—suggests deep training at the interface of computational chemistry, chemical engineering, and materials science. The breadth and influence of his contributions are evidenced by **~6020 total citations**, with widely cited works in single-atom catalysis, nanocluster interfaces and stability, and machine-learning methodology for materials chemistry. As a faculty member at **Georgia Tech**, he is affiliated with the institute’s interdisciplinary ecosystem in **computational science**, **materials research**, and **data-driven discovery**, where his group’s work bridges *theory, computation, and ML-enabled design* across catalytic and functional materials."}, "2a5XgNAAAAAJ": {"keywords": "Machine Learning, Perception, Robotics, Artificial Intelligence, Few-shot Learning, Continual Learning, Semi-Supervised Object Detection, Out-of-Distribution Detection, Embodied AI Navigation, Multi-Agent Collaborative Perception", "summary": "## Overview\nZsolt Kira is an Associate Professor at the Georgia Institute of Technology, where he leads a research program at the intersection of **machine learning**, **computer vision**, and **robotics** aimed at building *reliable* and *adaptive* intelligent systems. His group investigates how to endow embodied agents with <u>robust perception</u>, <u>data-efficient learning</u>, and <u>continual adaptation</u> so that robots and AI models can operate under distribution shift, limited supervision, and evolving task demands. Across his work, Kira emphasizes principled evaluation and scalable methods that translate from benchmarks to real-world settings, spanning **few-shot learning**, **out-of-distribution detection**, **semi-supervised learning**, and **embodied AI**.\n\n---\n\n## Research Areas\nKira’s research spans several tightly connected areas of modern AI, unified by the goal of improving generalization under realistic constraints. In **data-efficient recognition**, his work on few-shot learning critically examines the empirical foundations of meta-learning and episodic evaluation—most notably in *“A Closer Look at Few-shot Classification” (2019)*—clarifying how design and implementation choices affect reported gains and establishing stronger baselines for fair comparison. In **robustness and uncertainty-aware perception**, he has advanced <u>out-of-distribution (OoD) detection</u> without requiring OoD examples during training, as in *“Generalized ODIN: Detecting Out-of-distribution Image without Learning from Out-of-distribution Data” (2020)*, addressing the practical need for models that can *reject* unfamiliar inputs. In **learning with limited labels**, his contributions include semi-supervised detection, exemplified by *“Unbiased Teacher for Semi-Supervised Object Detection” (2021)*, which revisits teacher–student training to reduce bias in pseudo-labeling and extend SSL beyond classification to dense prediction. A major thematic pillar is **continual/lifelong learning**, where his work both systematizes evaluation (*“Re-evaluating Continual Learning Scenarios: A Categorization and Case for Strong Baselines” (2018)*) and proposes algorithmic approaches to mitigate catastrophic forgetting, including prompt-based methods such as *“Coda-prompt” (2023)* and data-free class-incremental strategies such as *“Always Be Dreaming” (2021)*. Finally, in **embodied AI and robotics**, he contributes to simulation and benchmarking infrastructure for interactive agents—e.g., *“Habitat 2.0” (2021)* and *“Habitat 3.0” (2024)*—and to multi-agent perception via communication (*“When2com” (2020)*; *“Who2com” (2020)*), enabling collaborative sensing policies that adapt communication structure to task context.\n\n---\n\n## Notable Works\n- **Few-shot learning evaluation and baselines:** In *“A Closer Look at Few-shot Classification” (2019)*, Kira and collaborators dissect methodological confounds in few-shot pipelines, strengthening <u>reproducibility</u> and clarifying which improvements stem from modeling versus implementation, thereby reshaping how the community benchmarks meta-learning and transfer-based approaches.\n- **Robust OoD detection without OoD training data:** *“Generalized ODIN” (2020)* extends confidence-based detection to more realistic settings by improving <u>distribution-shift awareness</u> without relying on curated OoD datasets, addressing deployment scenarios where unknown unknowns are unavoidable.\n- **Embodied AI platforms for interactive home environments:** Through *“Habitat 2.0” (2021)* (and later *“Habitat 3.0” (2024)*), he helped drive a comprehensive embodied AI stack—data, physics-enabled simulation, and benchmark tasks—supporting research on <u>interactive navigation and rearrangement</u> for home-assistant agents.\n\n---\n\n## Academic Background\nAs a faculty member at Georgia Tech, Kira has built an internationally visible research agenda reflected in broad impact across machine learning and robotics, with **15,642** total citations and highly cited contributions spanning few-shot learning, GAN theory, semi-supervised detection, OoD robustness, and embodied AI. His publication trajectory indicates sustained engagement with top-tier ML/CV venues and community-shaping efforts: foundational theoretical work on generative modeling dynamics (*“On convergence and stability of gans” (2017)*) complements later, systems- and benchmark-oriented work in embodied intelligence (the Habitat series) and rigorous evaluation in continual learning (*“Re-evaluating Continual Learning Scenarios” (2018)*). His collaborations—evident from multi-institutional, platform-scale projects (e.g., Habitat) and widely adopted learning frameworks (e.g., Unbiased Teacher; Coda-Prompt)—suggest strong ties to the broader AI research ecosystem spanning academia and industry labs, while his research themes align with major professional communities in **artificial intelligence**, **robotics**, and **computer vision**."}, "33ZWJmEAAAAJ": {"keywords": "deep structured semantic models (DSSM), neural information retrieval, web search ranking, clickthrough data modeling, spoken language understanding (SLU), intent classification and slot filling, task-oriented dialogue systems, dialogue state tracking, speaker recognition and voice authentication, incremental and continual learning (catastrophic forgetting)", "summary": "## Overview\nLarry Heck is a researcher at the **Georgia Institute of Technology**, where he advances the science and engineering of *data-driven intelligent systems* spanning speech, language, and multimodal interaction. Drawing on a publication record with over **15,217 citations**, his work centers on building robust, scalable models for **information access** and **conversational AI**, with particular emphasis on <u>deep semantic representation learning</u>, <u>spoken language understanding</u>, and *continual adaptation* of deployed models. Across both academic and translational outputs, Heck has helped shape modern approaches to learning semantic meaning from interaction signals—especially clickthrough logs and conversational traces—while maintaining a consistent focus on practical, high-impact systems.\n\n---\n\n## Research Areas\nHeck’s research portfolio spans several tightly connected areas in modern machine learning and language technology. A central thread is **neural semantic modeling for search and retrieval**, exemplified by the influential deep structured semantic modeling line of work in “**Learning deep structured semantic models for web search using clickthrough data**” (2013), which framed large-scale representation learning from user interaction as a pathway to overcome the limits of keyword matching via <u>shared semantic embedding spaces</u> for queries and documents. In **spoken language understanding (SLU)**, he has contributed to sequence modeling methods for *intent and slot prediction*, including “**Using recurrent neural networks for slot filling in spoken language understanding**” (2014), which explores RNN architectures that capture temporal dependencies in utterances, and “**What is left to be understood in ATIS?**” (2010), which critically analyzes the long-standing ATIS benchmark to clarify remaining gaps in SLU evaluation and generalization. His work also addresses **scalability and domain expansion** in conversational systems, including approaches to semantic parsing and domain adaptation (e.g., “Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding,” 2016; “Towards zero-shot frame semantic parsing for domain scaling,” 2017) and methods that transform natural language into effective retrieval queries (“**Translating natural language utterances to keyword search queries**,” 2015). More recently, Heck has helped drive research on **continual and incremental learning**, tackling the challenge of *catastrophic forgetting* in “**Class-incremental learning via deep model consolidation**” (2020), and has contributed to **multimodal grounding** and human-centered supervision signals, including “**Taking a hint: Leveraging explanations to make vision and language models more grounded**” (2019), which targets the tendency of vision–language models to over-rely on language priors rather than visual evidence.\n\n---\n\n## Notable Works\n- **Deep semantic representation learning for web search**: Introduced and popularized deep structured semantic approaches to query–document matching trained on interaction logs in “**Learning deep structured semantic models for web search using clickthrough data**” (2013), establishing a widely adopted paradigm for <u>semantic retrieval</u> beyond keyword overlap.\n- **Neural sequence modeling for spoken language understanding**: Advanced RNN-based architectures for SLU sequence labeling in “**Using recurrent neural networks for slot filling in spoken language understanding**” (2014), helping to solidify recurrent modeling as a practical foundation for *slot filling* under real-world temporal dependencies.\n- **Continual learning without catastrophic forgetting**: Addressed class-incremental learning stability through consolidation strategies in “**Class-incremental learning via deep model consolidation**” (2020), contributing to the broader agenda of <u>deployable continual learning</u> in dynamic environments.\n\n---\n\n## Academic Background\nBased on the breadth and chronology of his publications—from early work on **hidden Markov models** for monitoring (“Mechanical system monitoring using hidden Markov models,” 1991) and foundational contributions to **speaker recognition** and robustness under channel mismatch (e.g., “Modeling dynamic prosodic variation for speaker verification,” 1998; “A model-based transformational approach to robust speaker recognition,” 2000; “Robust text-independent speaker identification over telephone channels,” 2002)—Heck’s academic trajectory reflects deep training in statistical signal processing and speech technology, later transitioning into large-scale **deep learning** for language and search. His record also indicates sustained engagement with high-impact, system-oriented research communities spanning speech, NLP, and machine learning, including work on toolkits and shared resources (e.g., “MSR identity toolbox v1.0,” 2013) that support reproducible experimentation. Now affiliated with **Georgia Institute of Technology**, he is recognized for influential contributions across multiple eras of the field—moving from probabilistic modeling to deep neural architectures and, more recently, to *incremental learning* and multimodal grounding—while maintaining a consistent emphasis on scalable methods, rigorous benchmarks, and real-world deployment constraints."}, "4G7pc2IAAAAJ": {"keywords": "High Performance Computing, Streaming Data, Flexible Communication, Enterprise Systems, Parallel Program Monitoring and Steering, Scalable I/O Middleware, In Situ Data Analytics and Visualization, Publish-Subscribe Event Middleware, Data Staging and I/O Pipelines, Scientific Workflow Coupling and Data Management (ADIOS2)", "summary": "## Overview\nGreg Eisenhauer is a senior researcher in the **College of Computing** at the **Georgia Institute of Technology**, where he has long contributed to the systems and middleware foundations that enable modern **High Performance Computing (HPC)** and data-intensive computing. Working at the intersection of **runtime systems**, **I/O middleware**, and *streaming data* infrastructures, his research centers on <u>flexible communication</u> and <u>end-to-end data movement</u> for large-scale applications—spanning classic online performance steering (e.g., *Falcon*) through contemporary frameworks for scalable scientific data management (e.g., *ADIOS 2*) and in situ/in transit analytics pipelines. Across this trajectory, Eisenhauer’s work has emphasized **adaptability**, **performance portability**, and *practical deployability* in both scientific and enterprise contexts.\n\n---\n\n## Research Areas\nEisenhauer’s research areas are anchored in scalable systems support for data production, transport, and analysis in distributed and extreme-scale environments. A major theme is **online monitoring and control** of parallel programs, exemplified by *“Falcon: On-line monitoring and steering of large-scale parallel programs”* (1995) and related work on interactive steering (e.g., *“Falcon—toward interactive parallel programs…”* 1994), which helped establish architectural patterns for collecting high-rate telemetry and feeding it back into runtime decision-making. A second, sustained thread is **high-performance data interchange and event-based communication**, including publish/subscribe and event delivery middleware for HPC and distributed laboratories (e.g., *“Event services for high performance computing”* 2000; *“Publish-subscribe for high-performance computing”* 2006), alongside foundational work on efficient, portable representations such as *“Fast heterogeneous binary data interchange”* (2000) and *“Native data representation: An efficient wire format…”* (2003). In the 2010s and beyond, his work strongly shaped **I/O staging, in situ/in transit analytics, and streaming workflows** for scientific computing—addressing filesystem bottlenecks and enabling concurrent analysis via systems such as *DataStager* (*“Datastager: scalable data staging services for petascale applications”* 2009), type-aware streaming and publish/subscribe for analytics (*“Flexpath…”* 2014), and location-flexible middleware (*“FlexIO…”* 2013). This line culminates in influential, community-facing data management infrastructure (*“ADIOS 2: The adaptable input output system…”* 2020) and complementary studies of extreme-scale in situ design tradeoffs (*“Performance analysis, design considerations, and applications of extreme-scale in situ infrastructures”* 2016). Parallel to scientific HPC, he has also advanced **monitoring/analytics for data centers and enterprise information flows**, integrating continuous telemetry with online analysis in systems such as *Monalytics* (2010) and related architectures (2011), reflecting a consistent emphasis on *streaming-first* systems thinking across domains.\n\n---\n\n## Notable Works\n- **Interactive monitoring and steering for parallel programs:** Introduced a scalable framework for *online observation and control* of large parallel applications in *“Falcon: On-line monitoring and steering of large-scale parallel programs”* (1995), establishing core concepts for runtime feedback loops in HPC.\n- **Scalable I/O staging for petascale workloads:** Developed data-staging services that mitigate I/O cost and variability for extreme-scale applications in *“Datastager: scalable data staging services for petascale applications”* (2009), shaping later in situ/in transit pipeline designs.\n- **Modern, adaptable scientific data management infrastructure:** Co-developed a unified, extensible framework for high-performance data movement and analysis across supercomputers and broader environments in *“ADIOS 2: The adaptable input output system. a framework for high-performance data management”* (2020), helping drive the shift from file-centric workflows toward more flexible data pipelines.\n\n---\n\n## Academic Background\nBased at Georgia Tech’s **College of Computing**, Eisenhauer’s publication record indicates a decades-long academic career spanning the evolution from early parallel-program steering and distributed laboratories in the 1990s (e.g., *Falcon* and distributed lab communication) to middleware for event services and heterogeneous data interchange in the 2000s, and then to petascale/exascale data management, staging, and in situ analytics in the 2010s–2020s (e.g., *DataStager*, *Flexpath/FlexIO*, and *ADIOS 2*). His impact is reflected in substantial scholarly uptake (over **4,000 citations**), consistent with sustained leadership in systems research communities that bridge **HPC**, **distributed systems**, and *streaming data* architectures. The breadth of topics—from enterprise operational information systems and autonomic/self-managing information flows to exascale-oriented scientific I/O and analytics—suggests deep collaborative engagement across interdisciplinary teams and large research initiatives, aligning his affiliations with both academic computing research and large-scale, production-oriented systems development."}, "4UZP8bMAAAAJ": {"keywords": "Cryptography, Hardware Security, Cyber Physical System Security, Electronic Design IP Protection, Circuit Design Reverse Engineering, Hardware Obfuscation, IEEE P1735 Standard Analysis, Industrial Control Systems (ICS) Security, PLC Device Discovery and Fingerprinting, Maritime Cybersecurity", "summary": "## Overview\nAnimesh Chhotaray is a **Research Scientist** at **Georgia Tech**, where he works at the intersection of **cryptography**, **hardware security**, and *cyber-physical system (CPS) security*. His research centers on <u>security failures that emerge when cryptographic and security mechanisms meet real-world engineering constraints</u>—from standards and toolchains used to protect electronic-design intellectual property (IP), to the measurement and hardening of industrial and critical infrastructure systems. Across his work, Chhotaray is particularly focused on translating rigorous security thinking into *practical, deployable* defenses and evaluations for high-impact, operational environments.\n\n---\n\n## Research Areas\nChhotaray’s research spans three tightly connected areas. First, he studies <u>applied cryptography and standardization</u> by analyzing how security breaks down in widely adopted specifications; his teardown of IEEE P1735 in “**Standardizing Bad Cryptographic Practice**” (2017) documents systemic cryptographic errors in a standard intended to protect EDA IP, highlighting how design choices around encryption, key management, and access control can institutionalize insecurity. Second, he advances <u>hardware/IP protection and anti-reverse-engineering</u>, developing more principled approaches to evaluate and strengthen design-hiding methods in “**Hardening Circuit-Design IP Against Reverse-Engineering Attacks**” (2022), motivated by the long history of broken “obfuscation-style” defenses in circuit protection. Third, he addresses <u>CPS and critical-infrastructure security</u> through both human-centered and measurement-driven methods: “**A Sea of Cyber Threats**” (2025) examines maritime cybersecurity from mariners’ perspectives to surface operational threat realities, while “**Release the Hounds!**” (2024) introduces automated inference and empirical evaluation techniques for field-deployed PLCs using active network data—supporting attack-surface management and patch-adoption measurement at scale. Complementing these strands, his recent work on “**One Video to Steal Them All**” (2025) extends CPS security into emerging manufacturing pipelines by demonstrating optical side channels that enable 3D-printing IP theft, and “**Brittle Features of Device Authentication**” (2021) critiques ML-based device fingerprinting by identifying fragility in purportedly identifying features.\n\n---\n\n## Notable Works\n- **“Standardizing Bad Cryptographic Practice: A Teardown of the IEEE Standard for Protecting Electronic-design Intellectual Property” (2017)** — A high-impact analysis of IEEE P1735 that exposes how <u>cryptographic misdesign in standards</u> can propagate insecure encryption and access-control practices across an ecosystem.\n- **“Hardening Circuit-Design IP Against Reverse-Engineering Attacks” (2022)** — Establishes a more principled foundation for evaluating and improving <u>design-hiding</u> techniques, addressing recurring failures in circuit IP protection schemes.\n- **“A Sea of Cyber Threats: Maritime Cybersecurity from the Perspective of Mariners” (2025)** — Brings an operational, stakeholder-grounded lens to <u>maritime CPS security</u>, characterizing real threats and constraints shaping security outcomes in ships and ports.\n\n---\n\n## Academic Background\nBased on his publication trajectory and role, Chhotaray’s academic development reflects sustained engagement with security research spanning **cryptographic engineering**, **hardware/IP protection**, and **CPS/ICS security**, with increasing emphasis on empirically grounded evaluation in real deployments. His early, widely cited contribution on IEEE P1735 (2017) indicates deep familiarity with cryptographic protocols and the security implications of standardization in electronic design workflows, while later work (2021–2025) demonstrates expanding leadership across device authentication, industrial control measurement, and sector-specific critical infrastructure (notably maritime systems). His affiliation as a **Research Scientist at Georgia Tech** situates him within a major research environment for systems security and cyber-physical resilience, and his authorship of the “**Cyber Informed Engineering (CIE) Curriculum Guide**” (2024) and related adoption study suggests active involvement in educational and community-building efforts to institutionalize <u>cyber-informed engineering</u> within university engineering programs. With **59 citations** across this body of work, his contributions show growing influence across both foundational critique (standards and assumptions) and applied security practice (measurement, hardening, and operational threat understanding)."}, "56fOepEAAAAJ": {"keywords": "Data Structures, Algebraic Algorithms, Convex Optimization, Dynamic Graph Algorithms, Linear Programming, Interior Point Methods, Minimum-Cost Flow, Bipartite Matching, Dynamic Matrix Inverse, Graph Sparsification", "summary": "## Overview\nJan van den Brand is a researcher at **Georgia Tech**, where he works at the intersection of theoretical computer science and efficient computation, with a particular emphasis on **data structures**, **algebraic algorithms**, and **convex optimization**. His work is characterized by the design of *provably fast* algorithms that leverage <u>advanced linear-algebraic primitives</u> and <u>optimization methods</u>—notably interior-point techniques and fast matrix operations—to obtain improved runtimes for foundational problems in flows, matchings, and dynamic graph computation. With over **1,500 citations**, he is recognized for pushing the algorithmic frontier on both randomized and deterministic approaches to <u>nearly-linear-time optimization</u> and dynamic maintenance of algebraic quantities.\n\n---\n\n## Research Areas\nVan den Brand’s research spans three tightly connected areas. First, in <u>convex optimization and linear programming</u>, he develops faster solvers by rethinking interior-point methodology and its dependence on linear-algebraic subroutines, as exemplified by *“A deterministic linear program solver in current matrix multiplication time”* (2020) and *“Solving tall dense linear programs in nearly linear time”* (2020), which focus on improving iteration complexity and the cost of Newton/linear-system steps in dense regimes. Second, in <u>combinatorial optimization via continuous methods</u>, he designs nearly-linear(-ish) time algorithms for classic graph problems by reducing them to structured linear programs and exploiting duality, including *“Minimum cost flows, MDPs, and ℓ1-regression in nearly linear time for dense instances”* (2021) and the deterministic breakthrough *“A deterministic almost-linear time algorithm for minimum-cost flow”* (2023). Third, in <u>dynamic and algebraic data structures</u>, he advances the state of the art in maintaining matrix and graph properties under updates—e.g., *“Dynamic matrix inverse: Improved algorithms and matching conditional lower bounds”* (2019), *“Fully-dynamic graph sparsifiers against an adaptive adversary”* (2022), and later work on dynamic shortest paths and adaptive-adversary robustness—highlighting a unifying theme: using *algebraic maintenance* to accelerate iterative and dynamic algorithms across optimization and graph problems.\n\n---\n\n## Notable Works\n- **Deterministic linear programming at matrix-multiplication time**: In *“A deterministic linear program solver in current matrix multiplication time”* (2020), van den Brand advances deterministic LP solving by aligning interior-point progress with the best-known bounds for fast matrix multiplication, sharpening the role of linear algebra in high-precision optimization.\n- **Nearly-linear-time algorithms for dense-instance minimum-cost flow and related problems**: In *“Minimum cost flows, MDPs, and ℓ1-regression in nearly linear time for dense instances”* (2021), he helps deliver improved randomized runtimes for two-sided-constraint linear programs, connecting minimum-cost flow, Markov decision processes, and robust regression through shared optimization structure.\n- **Dynamic algebraic primitives with tight algorithmic–lower-bound interplay**: In *“Dynamic matrix inverse: Improved algorithms and matching conditional lower bounds”* (2019), he contributes improved methods for maintaining matrix inverses under updates while also clarifying conditional limitations, establishing dynamic matrix inverse as a central tool and barrier for multiple dynamic problems.\n\n---\n\n## Academic Background\nAffiliated with **Georgia Tech**, van den Brand’s publication trajectory reflects sustained contributions to leading topics in algorithms—particularly fast optimization, flows/matchings, and dynamic graph algorithms—suggesting deep ties to the theoretical computer science community through collaborations across optimization, data structures, and graph algorithms. His 2021 thesis, *“Dynamic Matrix Algorithms and Applications in Convex and Combinatorial Optimization”*, consolidates a core intellectual arc of his work: building <u>dynamic linear-algebraic data structures</u> and deploying them as enabling technology for faster algorithms in both convex and combinatorial settings. Across papers from 2019–2025, he has contributed to both randomized and deterministic techniques (notably in minimum-cost flow and dynamic distances), and his citation record (over **1,500**) indicates broad uptake of these methods in modern algorithm design, particularly where *fine-grained runtime improvements* depend on careful integration of linear algebra, interior-point optimization, and dynamic maintenance."}, "5BWj5TgAAAAJ": {"keywords": "AI Powered Computing Education, Generative AI in Software Engineering Education, Intelligent Tutoring Systems, Automated Grading and Assessment, X-ray Micro-Computed Tomography, Granular Materials Particle Shape Characterization, Pore Network Modeling in Porous Media, Intelligent Geotechnics, Landslide Susceptibility Mapping, Disaster Damage Assessment using Remote Sensing and Social Media Images", "summary": "## Overview\nNimisha Roy is a Faculty member in the College of Computing at the Georgia Institute of Technology, where she works at the intersection of **AI-powered computing education** and *data-driven scientific and geotechnical computing*. Her scholarship spans both the design of <u>scalable instructional systems</u> for high-enrollment computing courses and the development of **image-based** and **X-ray CT–driven** computational pipelines for *porous media* and granular materials. Across these domains, Roy’s work emphasizes <u>measurement-to-model workflows</u>—linking rigorous digital characterization (e.g., particle morphology and pore-network topology) to predictive modeling and educational impact.\n\n---\n\n## Research Areas\nRoy’s research portfolio integrates three mutually reinforcing areas. First, in **computing education**, she investigates how <u>AI tools and automation</u> can improve learning, assessment, and instructional decision-making at scale, including automated evaluation of student work and the integration of generative AI into advanced software engineering curricula. This thread is reflected in work such as **“VISGRADER: Automatic Grading of D3 Visualizations”** (2023), which addresses the challenges of assessing interactive artifacts in large classes, and in curriculum-focused contributions including **“Empowering Future Software Engineers: Integrating AI Tools into Advanced CS Curriculum”** (2025) and related benchmarking and redesign efforts around GenAI-enabled software engineering workflows. Second, in **scientific computing for geomaterials**, Roy develops computational methods for <u>image-based shape characterization</u> and microstructure quantification, advancing how granular particle morphology (sphericity, roundness, roughness/angularity) is computed and connected to kinematics and mechanical response—exemplified by **“Image based shape characterization of granular materials and its effect on kinematics of particle motion”** (2018) and earlier digital image analysis of sand grains (2014). Third, in **porous media and intelligent geotechnics**, she leverages **X-ray micro-computed tomography** and network-based representations to analyze <u>pore bodies/throats</u>, contact networks, and shear-induced pore-space evolution, as seen in **“Pore space evolution of granular assemblies under shear”** (2022), **“Quantifying three-dimensional bodies and throats of particulate system pore space”** (2023), and **“3-D contact and pore network analysis of MICP cemented sands”** (2023). Complementing these core themes, she also applies machine learning to hazard and damage contexts—e.g., **positive–unlabeled learning** for landslide susceptibility mapping (2024) and the use of remote sensing signals such as nighttime lights for disaster damage assessment (2023)—showing a consistent emphasis on *robust inference under imperfect labels and complex real-world data*.\n\n---\n\n## Notable Works\n- **Image-based granular shape computation and kinematics linkage:** Roy’s highly cited paper **“Image based shape characterization of granular materials and its effect on kinematics of particle motion”** (2018) advances computational quantification of particle shape components and demonstrates how morphology influences motion, helping bridge <u>geometric descriptors</u> with observable granular behavior.  \n- **3D microstructure and pore-network characterization in cemented sands:** In **“3-D contact and pore network analysis of MICP cemented sands”** (2023), she contributes a comprehensive X-ray micro-CT methodology to compare natural versus bio-cemented states and to extract 3D microstructural descriptors that support mechanistic interpretation of <u>cementation-driven</u> changes in geomaterial behavior.  \n- **Scalable assessment infrastructure for computing education:** Through **“VISGRADER: Automatic Grading of D3 Visualizations”** (2023), Roy addresses a practical bottleneck in large computing courses by proposing automated approaches for evaluating interactive visualizations, supporting more consistent and scalable feedback aligned with <u>equitable assessment</u> goals.\n\n---\n\n## Academic Background\nBased on her publication trajectory, Roy’s academic formation reflects sustained training and collaboration across **computational geotechnics**, **image-based scientific computing**, and more recently **computing education research** within a major research university setting at Georgia Tech. Her earlier and foundational contributions (e.g., digital image analysis of sand grains in 2014 and subsequent morphology-focused studies through 2016–2019) indicate deep engagement with experimental–computational pipelines for granular materials, later expanding into 3D X-ray CT and pore-network analysis of sheared and MICP-treated sands (2020–2023). Her more recent body of work (2023–2026) demonstrates an active shift toward **AI-enabled pedagogy**, including curriculum redesign for software engineering, benchmarking of generative AI tools, and automated grading/feedback systems—suggesting strong affiliations with computing education initiatives and large-course instructional ecosystems at Georgia Tech. With **282 total citations**, her impact is anchored by influential methods papers in granular shape characterization while also building a growing, applied record in <u>AI-powered computing education</u> and data-driven hazard/damage assessment."}, "5HJzWMUAAAAJ": {"keywords": "Mathematics, Computer Science, probabilistic combinatorics, random graphs, statistical physics on graphs, spin systems (Ising/Potts/random-cluster), approximate counting and sampling (FPTAS), independent sets and matchings (#BIS), planted CSPs and random k-SAT, cavity method and belief propagation", "summary": "## Overview\nWill Perkins is a faculty researcher in **Mathematics and Computer Science** at the Georgia Institute of Technology, where he leads and collaborates within a broad theoretical community spanning discrete mathematics, probability, and theoretical computer science. His scholarship centers on *rigorous methods for random and complex discrete systems*, with particular emphasis on <u>phase transitions</u> and <u>computational–statistical gaps</u> in random constraint satisfaction and inference, as well as <u>approximate counting and sampling</u> for models from statistical physics on graphs and lattices. Drawing on tools from **probabilistic combinatorics**, **statistical mechanics**, and **algorithm design**, Perkins’ work is distinguished by translating physics predictions—especially those from the *cavity method* and related frameworks—into mathematically precise theorems with algorithmic consequences.\n\n---\n\n## Research Areas\nPerkins’ research program sits at the interface of modern probabilistic methods and algorithmic theory, with recurring themes of *threshold phenomena*, *structure of solution spaces*, and *efficient computation in regimes traditionally viewed as intractable*. In random inference and factor graph models, he has helped formalize information-theoretic predictions from physics, most prominently in “**Information-theoretic thresholds from the cavity method**” (2018), which establishes a general mutual-information formula for inference problems induced by random graphs and yields sharp consequences for information-theoretic thresholds. Closely related is his work on planted constraint satisfaction, including “**On the complexity of random satisfiability problems with planted solutions**” (2018), which analyzes the pronounced gap between information availability and algorithmic recovery in planted \\(k\\)-SAT-type settings. In approximate counting and sampling, Perkins has advanced algorithmic techniques for spin systems and graph polynomials in challenging parameter regimes: “**Algorithmic Pirogov–Sinai theory**” (2020) and subsequent works develop contour- and polymer-based algorithms for low-temperature models on \\(\\mathbb{Z}^d\\) and tori, while “**Algorithms for #BIS-hard problems on expander graphs**” (2020) identifies expander-graph structure that enables FPTAS and efficient sampling for models such as the hard-core and Potts models. A complementary strand of his work develops sharp extremal and analytic bounds for combinatorial/statistical-physics quantities—e.g., occupancy fractions and independence/matching polynomials (“**Independent sets, matchings, and occupancy fractions**,” 2017; “On the average size of independent sets in triangle-free graphs,” 2018)—and investigates high-dimensional geometric and coding questions (“On kissing numbers and spherical codes in high dimensions,” 2018). Across these areas, Perkins repeatedly connects *thermodynamic behavior* (free energy, coexistence, uniqueness, spatial mixing) to *algorithmic feasibility* (approximation schemes, sampling algorithms, and barriers such as overlap-gap phenomena).\n\n---\n\n## Notable Works\n- **Rigorous cavity-method thresholds for random inference:** In “**Information-theoretic thresholds from the cavity method**” (2018), Perkins and collaborators validate a non-rigorous physics framework by proving a general formula for mutual information in random-graph inference models, yielding precise <u>information-theoretic thresholds</u> and clarifying where statistical recovery becomes possible in principle.\n- **Algorithmic gaps in planted satisfiability:** In “**On the complexity of random satisfiability problems with planted solutions**” (2018), he analyzes planted \\(k\\)-SAT distributions exhibiting a stark separation between the clause density required for information-theoretic identifiability and what is known to be achievable by efficient algorithms, sharpening the study of <u>computational–statistical tradeoffs</u> in planted CSPs.\n- **Low-temperature counting/sampling via contours and polymers:** In “**Algorithmic Pirogov–Sinai theory**” (2020), Perkins helps develop an efficient algorithmic framework—grounded in Pirogov–Sinai contour representations—for approximate counting and sampling in the <u>low-temperature regime</u> of broad lattice spin systems, opening a route to polynomial-time algorithms beyond classical high-temperature correlation-decay methods.\n\n---\n\n## Academic Background\nBased at the Georgia Institute of Technology, Perkins has built an internationally visible research profile spanning theoretical computer science and mathematics, reflected in a substantial and highly cited body of work (over **2,100 citations**) across random structures, statistical physics on graphs, and algorithmic counting/sampling. His publication trajectory—from early contributions in hardness and random processes (e.g., “Hardness of finding independent sets in almost 3-colorable graphs,” 2010; “The Bohman–Frieze process near criticality,” 2013) to later foundational advances in rigorous statistical physics methods and algorithmic phase-transition theory—suggests a career shaped by deep cross-disciplinary training in **probability**, **combinatorics**, and **complexity theory**. He is strongly affiliated with the modern community connecting *spin systems, random graphs, and approximate computation*, frequently collaborating with researchers across combinatorics and TCS, and contributing both to mathematically rigorous foundations (e.g., replica/cavity-method justifications and Gibbs-measure analysis) and to algorithmic outcomes such as FPTAS and efficient sampling in regimes previously dominated by negative mixing-time or hardness results."}, "6VFGjPYAAAAJ": {"keywords": "creativity and cognition, computational media, computer science education, embodied interaction, AI, AI literacy, co-creative AI systems, interactive narrative and drama, game AI and player modeling, STEAM computing through music (EarSketch)", "summary": "## Overview\nBrian Magerko is **Regents Professor of Digital Media** at **Georgia Tech**, where he leads the <u>Expressive Machinery Lab</u> (Expressive Machinery) and advances research at the intersection of *human creativity* and **artificial intelligence**. His work centers on <u>co-creative computational systems</u>—AI that collaborates with people in real time—and on the design of *human-centered* learning experiences that build **AI literacy** for broad publics. Across computational media, embodied interaction, and computer science education, Magerko’s scholarship articulates how <u>creativity and cognition</u> can be operationalized in interactive systems, while also addressing the societal need for people to critically understand AI as it becomes embedded in everyday technologies.\n\n---\n\n## Research Areas\nMagerko’s research spans three tightly connected areas: **AI literacy and public understanding of AI**, **co-creative/embodied AI for artistic collaboration**, and **computational media for learning and engagement**. In AI literacy, his highly cited work “**What is AI literacy? Competencies and design considerations**” (2020) synthesizes competencies and design principles for helping non-experts interpret, critique, and effectively interact with AI systems, extending into informal and public settings through “**Co-designing AI literacy exhibits for informal learning spaces**” (2021) and studies of learning interaction such as “**Family learning talk in AI literacy learning activities**” (2022). In computational creativity and cognition, he develops theoretical and empirical foundations for <u>co-creation</u>, including “**An enactive model of creativity for computational collaboration and co-creation**” (2015), and implements these ideas in interactive systems like **Drawing Apprentice**, evaluated in “**Empirically studying participatory sense-making in abstract drawing with a co-creative cognitive agent**” (2016) and described technically in “**Drawing apprentice: An enactive co-creative agent for artistic collaboration**” (2015). This emphasis on *improvisation* and *sense-making* also appears in his studies of theatrical cognition (e.g., “**An empirical study of cognition and theatrical improvisation**,” 2009) and embodied interactive art (e.g., “**Designing for socially interactive systems**,” 2017). Complementing these strands, Magerko has contributed to computer science education and STEAM learning through music-based programming environments such as **EarSketch** (“**Earsketch: A steam-based approach for underrepresented populations in high school computer science education**,” 2016; and “**Engaging underrepresented groups… through computational remixing with EarSketch**,” 2014), and he continues to track emerging human-AI interaction practices, including users’ experiences with generative AI (“**Is it AI or is it me? Understanding users’ prompt journey with text-to-image generative AI tools**,” 2024; and “**Testing, socializing, exploring… conceptions of ChatGPT**,” 2024).\n\n---\n\n## Notable Works\n- **Defining and operationalizing <u>AI literacy</u> for HCI and education**: In “**What is AI literacy? Competencies and design considerations**” (2020), Magerko and colleagues articulate a competency-oriented framework that has become foundational for designing and evaluating AI literacy interventions across user-facing contexts.  \n- **Establishing an <u>enactive</u> theory of co-creative AI and validating it in practice**: “**An enactive model of creativity for computational collaboration and co-creation**” (2015) provides a theoretical account of creativity as interactional and embodied, which is instantiated and empirically examined through Drawing Apprentice in “**Empirically studying participatory sense-making in abstract drawing with a co-creative cognitive agent**” (2016).  \n- **Broadening participation in computing through music-centered programming and STEAM pedagogy**: Through EarSketch, detailed in “**Earsketch: A steam-based approach for underrepresented populations in high school computer science education**” (2016) and earlier evaluated in “**Engaging underrepresented groups… through computational remixing with EarSketch**” (2014), Magerko’s work demonstrates how *authentic creative practice* can support persistence and engagement in introductory computing.\n\n---\n\n## Academic Background\nAs a senior faculty member at **Georgia Tech** and **Regents Professor of Digital Media**, Brian Magerko’s career reflects sustained leadership across **HCI**, **AI**, **computational creativity**, and **learning sciences**, evidenced by a substantial citation record (over **9,657** total citations). His publication trajectory suggests an evolution from early contributions to interactive narrative, believable agents, and game AI—spanning interactive drama architectures and authoring tools (e.g., “Building an interactive drama architecture,” 2003; “Story Representation and Interactive Drama,” 2005; “Scribe,” 2006)—toward a distinctive research program integrating *improvisation*, <u>embodied interaction</u>, and co-creative cognition (e.g., “Robot improv,” 2000; “An empirical study of cognition and theatrical improvisation,” 2009). In later work, he bridges these foundations to contemporary societal needs by shaping the academic and design discourse around <u>AI literacy</u> and informal learning (2020–2022), while also engaging emerging generative-AI practices in the wild (2024). His affiliations and collaborations, anchored in Georgia Tech’s interdisciplinary strengths in digital media and computing, position him as a central figure linking computational media research with educational impact and public-facing AI understanding."}, "6WlU86IAAAAJ": {"keywords": "Observational Health Data Sciences and Informatics (OHDSI), OMOP Common Data Model, real-world evidence, comparative effectiveness research, pharmacovigilance and drug safety, drug–drug interaction detection, clinical decision support and alert fatigue, synthetic electronic health records (GANs/VAEs), clinical natural language processing for medical coding, healthcare data quality assessment and phenotyping", "summary": "## Overview\nJon Duke is a Georgia Tech–affiliated researcher whose work sits at the intersection of **biomedical informatics**, *clinical decision support*, and large-scale **real-world evidence** generation from electronic health records (EHRs). He is best known for advancing the open, collaborative ecosystem of **Observational Health Data Sciences and Informatics (OHDSI)**, helping operationalize the field’s ambition of producing accessible and reliable clinical evidence from heterogeneous healthcare data. Across his scholarship, Duke emphasizes <u>standardization</u>, <u>reproducible observational research</u>, and *human-centered* methods that make complex clinical data usable for discovery, safety surveillance, and decision-making at scale.\n\n---\n\n## Research Areas\nDuke’s research spans three tightly connected areas. First, he is a prominent contributor to <u>observational health data science</u> through OHDSI, articulating the network’s methodological and infrastructural opportunities for observational researchers in “**Observational Health Data Sciences and Informatics (OHDSI): opportunities for observational researchers**” (2015) and demonstrating how standardized analytics can characterize care at scale in “**Characterizing treatment pathways at scale using the OHDSI network**” (2016). Second, he has contributed to *privacy-aware* machine learning for health data sharing, including synthetic EHR generation via deep generative models in “**Generating multi-label discrete patient records using generative adversarial networks**” (2017) and longitudinal record synthesis in “**EVA: Generating longitudinal electronic health records using conditional variational autoencoders**” (2021), positioning <u>synthetic data</u> as a practical mechanism for collaboration under privacy constraints. Third, Duke’s work engages <u>clinical NLP and explainability</u> and the translation of informatics to clinical utility, exemplified by “**Explainable prediction of medical codes from clinical text**” (2018). Complementing these themes is an enduring focus on medication safety and decision support—ranging from drug–drug interaction (DDI) alert design and adherence (“**Adherence to drug—drug interaction alerts in high-risk patients: a trial of context-enhanced alerting**,” 2013; “**A successful model and visual design for creating context-aware drug-drug interaction alerts**,” 2011) to large-scale pharmacovigilance and evidence integration (e.g., LAERTES, 2017)—as well as multinational comparative effectiveness studies enabled by standardized observational frameworks (e.g., first-line antihypertensive class comparisons, 2019; ACE inhibitors vs ARBs, 2021).\n\n---\n\n## Notable Works\n- **Architecting and popularizing the OHDSI paradigm for reproducible observational research**, most prominently in “*Observational Health Data Sciences and Informatics (OHDSI): opportunities for observational researchers*” (2015), which helped define how common data models and open analytics can generate credible evidence from global clinical data at scale.  \n- **Pioneering synthetic EHR generation for privacy-preserving data sharing** through deep learning, notably “*Generating multi-label discrete patient records using generative adversarial networks*” (2017), a highly cited contribution that advanced the feasibility of sharing realistic, high-dimensional patient records while mitigating privacy risks.  \n- **Advancing explainable clinical language understanding for coding and documentation workflows** in “*Explainable prediction of medical codes from clinical text*” (2018), which connected state-of-the-art clinical NLP with interpretability requirements central to real-world clinical adoption.\n\n---\n\n## Academic Background\nBased on his publication trajectory and collaborations, Duke’s academic profile reflects sustained engagement with leading communities in **clinical informatics**, **patient safety**, and **real-world evidence** methodology, with Georgia Tech serving as his current institutional base. His record suggests deep involvement in international, multi-institutional research networks—especially OHDSI—where contributions often extend beyond individual studies to shared infrastructure, data quality tooling, and methodological benchmarking (e.g., “**Multisite evaluation of a data quality tool for patient-level clinical data sets**,” 2016; “**Defining a reference set to support methodological research in drug safety**,” 2013). The arc of his work—from early decision-support and visualization systems for adverse drug event review (2010–2015) to large-scale standardized observational studies and modern machine learning for EHRs (2016–2021)—indicates an academically interdisciplinary path bridging *human factors*, clinical workflow design, and computational methods. With **6,372 citations**, Duke’s influence is evidenced by high-impact, widely adopted contributions that have shaped both the scientific foundations and practical tooling of <u>evidence generation from observational healthcare data</u>."}, "6wCEmNYAAAAJ": {"keywords": "Algorithms, Theoretical Computer Science, Markov chain Monte Carlo (MCMC), Mixing times, Glauber dynamics, Spectral gap and coupling methods, Approximate counting and sampling, Statistical physics models (Ising/Potts/hard-core), Lattice tilings and monomer-dimer models, Programmable matter and self-organizing particle systems", "summary": "## Overview\nDana Randall is a senior researcher at the Georgia Institute of Technology, where she is widely associated with the theoretical computer science community and has helped shape a research program at the intersection of **algorithms** and **probabilistic methods**. Her work centers on *rigorous analysis* of randomized processes—especially **Markov chain Monte Carlo (MCMC)**—with an emphasis on proving when sampling and counting algorithms are efficient and when they provably fail. Across a highly cited body of work (over **2,600 citations**), Randall is particularly known for advancing the theory of <u>mixing times</u>, <u>Glauber dynamics</u>, and <u>decomposition/comparison techniques</u> that connect algorithmic sampling to models from statistical physics and combinatorics.\n\n---\n\n## Research Areas\nRandall’s research spans the design and analysis of randomized algorithms, with a sustained focus on <u>rapid mixing</u> versus <u>torpid mixing</u> phenomena in natural local Markov chains. A central theme is the development of general analytic frameworks for convergence-rate bounds, notably in “**Markov chain decomposition for convergence rate analysis**” (2002), which formalizes how to infer global spectral gaps from structured subchains, and in “**Analyzing Glauber dynamics by comparison of Markov chains**” (2000), which leverages chain comparison to study single-site dynamics used throughout statistical physics. These tools are repeatedly instantiated in concrete combinatorial and physical models: domino tilings and planar lattice structures (“**Markov chain algorithms for planar lattice structures**,” 2001; also an earlier 1995 version), biased lattice surfaces (“Sampling biased lattice configurations using exponential metrics,” 2009), and classic lattice counting problems such as monomer–dimer coverings (“**Approximating the number of monomer-dimer coverings of a lattice**,” 1996) and self-avoiding walks (“Sampling adsorbing staircase walks…,” 2000; “Self-testing algorithms for self-avoiding walks,” 2000). Complementing her positive results on efficient sampling, Randall has also produced influential impossibility/slow-mixing results that identify structural bottlenecks—e.g., “**Torpid mixing of simulated tempering on the Potts model**” (2004), “Slow mixing of Glauber dynamics via topological obstructions” (2006), and later work on 3-colorings of the discrete torus (2012) and phase coexistence for the hard-core model on \\(\\mathbb{Z}^2\\) (2013, 2019). In more recent years, her algorithmic perspective has extended into *programmable matter* and collective systems, using stochastic local rules and Markov chain viewpoints to analyze emergent behavior (“A Markov chain algorithm for compression in self-organizing particle systems,” 2016; “A stochastic approach to shortcut bridging…,” 2018), and into societally relevant computational methodology for redistricting via sampling of graph partitions (“Spanning tree methods for sampling graph partitions,” 2022; “Mathematically quantifying non-responsiveness…,” 2022).\n\n---\n\n## Notable Works\n- **Foundational tools for mixing analysis via decomposition:** In “**Markov chain decomposition for convergence rate analysis**” (2002), Randall develops a general methodology for bounding convergence rates of reversible chains by decomposing the state space into analyzable components and relating their behavior to the global <u>spectral gap</u>, providing a reusable framework that underpins later mixing-time analyses across models.\n- **Rigorous analysis of local dynamics (Glauber dynamics) through comparison:** “**Analyzing Glauber dynamics by comparison of Markov chains**” (2000) advances comparison techniques for establishing rapid mixing (or diagnosing slow mixing) in widely used local-update chains, strengthening the theoretical foundations of MCMC sampling in combinatorics and statistical physics.\n- **Markov-chain algorithms for lattice tilings and planar structures:** In “**Markov chain algorithms for planar lattice structures**” (2001), Randall (building on related earlier work) analyzes a natural local chain on domino tilings (via random \\(2\\times2\\) flips), helping establish the algorithmic tractability of sampling from large structured configuration spaces central to both discrete mathematics and physical modeling.\n\n---\n\n## Academic Background\nAt Georgia Institute of Technology, Dana Randall has built an academic profile characteristic of a leading figure in **theoretical computer science**, with sustained contributions to algorithms and probabilistic/analytic techniques for randomized computation. Her publication trajectory—from early work on combinatorial processes and sampling (e.g., “Efficient generation of random nonsingular matrices,” 1993; lattice counting and self-avoiding walks in the mid-1990s) through a major sequence of highly cited papers on **Markov chains**, **mixing times**, and **Glauber dynamics** (late 1990s–2000s), and onward to modern interdisciplinary applications in programmable matter and computational social science (2010s–2020s)—suggests a career-long commitment to bridging deep theory with impactful domains. The breadth of her collaborations and topics (tilings, Ising and Potts models, hard-core lattice gases, stable marriage lattices, tempering methods, and graph-partition sampling for redistricting) reflects strong affiliations with both the algorithms community and the discrete mathematics/statistical physics interface, with scholarly influence evidenced by her substantial citation record and recurring role in shaping how <u>MCMC convergence</u> is proved, compared, and decomposed in complex state spaces."}, "72afgH4AAAAJ": {"keywords": "Machine Learning, Systems Neuroscience, Neuromorphic Computing, Spiking Neural Networks, Neuromorphic VLSI, Brain-Inspired Hardware Architectures, Asynchronous Digital Circuits, On-Chip Learning and Plasticity (STDP), Address-Event Representation (AER) Communication Networks, Olfactory Bulb Modeling and Neuromorphic Olfaction", "summary": "## Overview\nNabil Imam is a researcher affiliated with the Georgia Institute of Technology and Cornell University whose work sits at the intersection of **machine learning** and **systems neuroscience**, with a sustained emphasis on *brain-inspired computing* and hardware realizations of spiking neural systems. Across highly cited contributions to neuromorphic engineering, Imam has helped advance <u>scalable spiking-neural-network (SNN) architectures</u> and *event-driven computation*—from foundational neurosynaptic core designs to large-scale manycore systems—bridging algorithmic principles drawn from biology with **non–von Neumann** implementations optimized for energy efficiency and real-time inference and learning.\n\n---\n\n## Research Areas\nImam’s research spans <u>neuromorphic VLSI and manycore architectures</u>, <u>spike-based learning</u>, and <u>biologically grounded sensory computation</u>. A central thread is the design of scalable, low-power neurosynaptic hardware and its supporting communication and routing infrastructure, exemplified by work on a “million spiking-neuron integrated circuit with a scalable communication network and interface” (2014) and the system-level design and tooling of **TrueNorth** (“Truenorth: Design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip,” 2015). Complementing these efforts are contributions to next-generation neuromorphic processors with *on-chip learning* and richer neuronal/synaptic dynamics, as in “Loihi: A neuromorphic manycore processor with on-chip learning” (2018), which highlights features such as hierarchical connectivity, synaptic delays, and learning mechanisms integrated in silicon. Imam has also pursued neuromorphic algorithms and demonstrations grounded in systems neuroscience—particularly olfaction—showing rapid online learning and robust recall under noise in “Rapid online learning and robust recall in a neuromorphic olfactory circuit” (2020), and extending neuromorphic methods to tasks such as nearest-neighbor search (“Neuromorphic nearest neighbor search using Intel’s Pohoiki Springs,” 2020). Parallel lines of work address asynchronous/event-driven circuit techniques and address-event communication (e.g., “Address-event communication using token-ring mutual exclusion,” 2011), reflecting a broader interest in <u>communication-efficient, timing-aware neural computation</u> across hardware and algorithmic levels.\n\n---\n\n## Notable Works\n- **Large-scale neurosynaptic integration and communication:** Contributed to the design and demonstration of a landmark brain-inspired chip in “**A million spiking-neuron integrated circuit with a scalable communication network and interface**” (2014), establishing a widely cited blueprint for <u>scalable neuromorphic integration</u> and intrachip spike communication.\n- **Programmable neuromorphic platform and ecosystem (TrueNorth):** Advanced the architecture and end-to-end enablement of low-power neurosynaptic computing through “**Truenorth: Design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip**” (2015), coupling chip design with a practical tool flow for mapping SNNs to hardware.\n- **On-chip learning neuromorphic processor (Loihi):** Helped define key architectural capabilities for learning-enabled neuromorphic systems in “**Loihi: A neuromorphic manycore processor with on-chip learning**” (2018), emphasizing <u>manycore SNN execution</u> with integrated plasticity and biologically inspired neuronal/synaptic features.\n\n---\n\n## Academic Background\nImam’s scholarly trajectory, reflected in publications spanning neuromorphic systems reviews, asynchronous/event-driven circuit design, and flagship neurosynaptic processors, indicates a long-standing engagement with **neuromorphic engineering** and *brain-inspired computation* across multiple generations of hardware. His affiliations with the Georgia Institute of Technology and Cornell University align with an interdisciplinary profile bridging electrical and computer engineering, computational neuroscience, and machine learning, and his publication record suggests extensive collaboration with major neuromorphic research programs that produced widely deployed platforms such as **TrueNorth** and **Loihi**. With total citations exceeding **13,789**, Imam’s work has achieved broad impact across both academic and applied communities, particularly in establishing <u>energy-efficient, scalable spiking computation</u> as a credible alternative to conventional architectures for real-time sensory processing and adaptive learning."}, "7OvZhQIAAAAJ": {"keywords": "Computer Architecture, Compiler, GPU Architecture, GPGPU Performance Modeling, Heterogeneous CPU-GPU Systems, Memory Hierarchy and Prefetching, Processing-in-Memory (PIM), 3D-Stacked DRAM and DRAM Caches, Secure Enclaves (Intel SGX) Side-Channel Attacks, Graph Analytics Acceleration", "summary": "## Overview\nHyesoon Kim is a faculty researcher at the **Georgia Institute of Technology (Georgia Tech)**, where she leads work at the intersection of **computer architecture** and **compiler/system support** for modern accelerators. Her scholarship centers on *understanding, modeling, and improving* the behavior of **GPUs** and **heterogeneous CPU–GPU systems**, with an emphasis on <u>performance bottleneck diagnosis</u>, <u>memory-system efficiency</u>, and <u>energy/power-aware design</u>. Across highly cited contributions—from analytical GPU modeling to adaptive runtime mapping and security analyses of trusted execution—Kim’s research advances the foundations needed to make massively parallel and heterogeneous computing both **predictable** and **practical**.\n\n---\n\n## Research Areas\nKim’s research spans rigorous modeling, architectural mechanisms, and compiler/runtime techniques for accelerators and heterogeneous platforms. A major thread is **GPU performance modeling and analysis**, exemplified by the analytical treatment of GPUs that explicitly accounts for *memory-level parallelism* and *thread-level parallelism* in “**An analytical model for a GPU architecture with memory-level and thread-level parallelism awareness**” (2009), and extended toward holistic design reasoning in “**An integrated GPU power and performance model**” (2010) as well as later modeling methodologies such as “**GPUMech: GPU performance modeling technique based on interval analysis**” (2014) and “**Power modeling for GPU architectures using McPAT**” (2014). Complementing modeling, she has contributed to **heterogeneous compilation/runtime mapping**, notably “**Qilin: exploiting parallelism on heterogeneous multiprocessors with adaptive mapping**” (2009), which targets automated, adaptive placement of computations across heterogeneous processing elements. Another sustained focus is **memory hierarchy and latency tolerance**, including influential work on prefetching—“**Feedback directed prefetching**” (2007) and “**When prefetching works, when it doesn’t, and why**” (2012)—and GPU-specific memory-latency techniques such as “**Many-thread aware prefetching mechanisms for GPGPU applications**” (2010) and heterogeneous cache management in “**TAP: A TLP-aware cache management policy for a CPU-GPU heterogeneous architecture**” (2012). Kim’s group has also pushed into **near-memory/processing-in-memory (PIM)** and **3D-stacked memory systems**, including “**Transparent hardware management of stacked DRAM as part of memory**” (2014), “**Cairo: A compiler-assisted technique for enabling instruction-level offloading of processing-in-memory**” (2017), and graph-centric PIM efforts such as “**Graphpim**” (2017). More recently, her portfolio extends to **systems for emerging workloads**—graph analytics (“**GraphBIG**,” 2015; unified memory studies for graph traversal, 2020), **edge and collaborative AI inference** (“**Characterizing the deployment of deep neural networks on commercial edge devices**,” 2019; “**Toward collaborative inferencing of deep neural networks on Internet-of-Things devices**,” 2020), and **hardware security**, including the widely recognized SGX side-channel study “**Inferring fine-grained control flow inside SGX enclaves with branch shadowing**” (2017).\n\n---\n\n## Notable Works\n- **GPU modeling that unifies parallelism awareness with predictive analysis:** Kim’s “*An analytical model for a GPU architecture with memory-level and thread-level parallelism awareness*” (2009) established a widely used framework for reasoning about how <u>MLP</u> and <u>TLP</u> jointly shape GPU performance, informing both architectural studies and optimization practice.  \n- **Adaptive heterogeneous mapping to make CPU–GPU platforms programmable at scale:** In “*Qilin: exploiting parallelism on heterogeneous multiprocessors with adaptive mapping*” (2009), she advanced compiler/runtime techniques for automatically mapping computations to heterogeneous resources, reducing the manual burden of performance portability while improving efficiency across diverse kernels.  \n- **Foundational security insight into trusted execution environments:** “*Inferring fine-grained control flow inside SGX enclaves with branch shadowing*” (2017) delivered a high-impact analysis of microarchitectural leakage in Intel SGX, clarifying how control-flow can be inferred despite enclave isolation and shaping subsequent research on <u>side-channel resilience</u> in secure processors.\n\n---\n\n## Academic Background\nAt **Georgia Tech**, Kim has built an internationally visible research program with broad impact, reflected in **10,321 citations** and sustained contributions across top-tier computer architecture and systems venues. Her publication trajectory suggests a progression from early work on **speculation and latency tolerance** in out-of-order processors (e.g., runahead execution and branch/predication techniques in the mid-2000s) to seminal leadership in **GPU architecture**, **heterogeneous computing**, and **memory-system innovation** as GPUs became central to mainstream and high-performance computing (late-2000s onward). The breadth of her collaborations—spanning GPU modeling, cache/prefetching, 3D-stacked memories and PIM, graph frameworks, edge AI deployment, and SGX security—indicates deep affiliations with the computer architecture and compiler communities and an active role in shaping cross-layer research that connects *workload characterization*, *architectural mechanisms*, and *tooling/simulation* (e.g., “Macsim,” 2012) for reproducible accelerator studies."}, "7P-gZioAAAAJ": {"keywords": "Systems for ML, soft real-time ML, LLM inference, distributed computing frameworks, cluster scheduling in heterogeneous clouds, serverless computing (FaaS), latency SLOs and tail latency QoS, ML inference serving pipelines, GPU inference scheduling and utilization, KV cache optimization for long-context LLMs", "summary": "## Overview\nAlexey Tumanov is an Associate Professor at the Georgia Institute of Technology, where he leads research at the intersection of **computer systems** and **machine learning** with an emphasis on *production-grade performance and predictability*. His group designs **systems for ML** that make modern AI practical under stringent operational constraints, focusing on <u>soft real-time ML</u>, scalable cloud and cluster scheduling, and high-efficiency <u>LLM inference</u> serving. Across his work, Tumanov advances the systems foundations required to run interactive and data-intensive AI workloads reliably—balancing *latency*, *throughput*, *cost*, and *elasticity* in heterogeneous, shared infrastructures.\n\n---\n\n## Research Areas\nTumanov’s research spans several tightly connected areas in systems and ML. A central theme is **distributed systems for emerging AI applications**, exemplified by *Ray: A distributed framework for emerging AI applications* (2018), which targets the demanding performance and flexibility requirements of AI systems that continuously interact with their environments. He has also made foundational contributions to understanding and managing **heterogeneous cloud clusters**, notably through large-scale workload characterization in *Heterogeneity and dynamicity of clouds at scale: Google trace analysis* (2012), informing scheduler design under real-world variability. Building on these insights, his work on **resource management and SLOs** (e.g., *Morpheus: Towards automated SLOs for enterprise clusters*, 2016; *TetriSched: global rescheduling with adaptive plan-ahead in dynamic heterogeneous clusters*, 2016; *3Sigma*, 2018) addresses the tension between utilization and predictability via runtime uncertainty modeling, reservation-aware planning, and adaptive rescheduling. Another major thread is **serverless and stateful cloud programming models**, critiquing limitations of first-generation platforms in *Serverless computing: One step forward, two steps back* (2018) and extending capabilities through systems such as *Cloudburst: Stateful functions-as-a-service* (2020) and *Cirrus: A serverless framework for end-to-end ML workflows* (2019), which target end-to-end ML pipelines rather than isolated stateless functions. In parallel, Tumanov’s group has driven advances in **latency-aware ML inference**, including pipeline provisioning in *Inferline* (2020) and GPU sharing/scheduling for interactive inference in *Dynamic space-time scheduling for GPU inference* (2018). Most recently, his work has focused on <u>LLM inference serving</u>, tackling the throughput–latency tension inherent in prefill/decode execution with *Sarathi-Serve* (2024), alongside simulation and evaluation methodology with *Vidur* (2024) and *Etalon* (2024), reflecting a broader agenda of making LLM systems both efficient and rigorously measurable.\n\n---\n\n## Notable Works\n- **Ray (2018)** — Co-developed *“Ray: A distributed framework for emerging AI applications”*, a widely adopted distributed execution framework that helped define systems support for modern AI workloads requiring both high performance and flexible programming abstractions.  \n- **Google trace analysis (2012)** — Authored influential characterization work in *“Heterogeneity and dynamicity of clouds at scale: Google trace analysis”*, providing empirical grounding for scheduler and resource-manager designs in large, heterogeneous production clusters.  \n- **LLM serving efficiency (2024)** — Advanced state-of-the-art LLM inference serving with *“Taming Throughput-Latency tradeoff in LLM inference with Sarathi-Serve”*, targeting the practical constraints of prefill/decode phases to improve utilization while respecting latency goals.\n\n---\n\n## Academic Background\nAcross a publication record spanning cloud infrastructure, scheduling, serverless computing, and ML/LLM serving—and reflected in a high citation impact (8,539 total citations)—Tumanov has developed a research profile characteristic of a systems scholar with deep engagement in both academic and production-facing problems. His early and mid-career work aligns with major systems venues and communities in **cluster resource management**, **distributed storage**, and **cloud computing**, including empirically driven studies of large-scale datacenter behavior (e.g., the Google trace analyses) and scheduler designs aimed at predictability under uncertainty (e.g., *Morpheus*, *TetriSched*, *3Sigma*). As his agenda expanded toward ML-centric infrastructure, he contributed to the evolution of **serverless** from stateless functions toward stateful, ML-ready platforms (*Cloudburst*, *Cirrus*) and to **latency/SLO-aware inference** (*Inferline*, GPU inference scheduling). In his current role at Georgia Tech, his recent work on **LLM inference systems** and evaluation (*Sarathi-Serve*, *Vidur*, *Etalon*) positions him within the leading research community shaping *real-time* and *cost-efficient* deployment of foundation models, with collaborations and topical breadth suggesting sustained engagement across systems, ML, and applied domains (including healthcare-serving settings such as *Holmes*, 2020)."}, "83SAwBsAAAAJ": {"keywords": "Brain-Computer Interfaces, Animal-Computer Interaction, Steady-State Visual Evoked Potentials (SSVEP), Assistive Technology for Severe Motor Disabilities, Direct Brain Interfaces (DBI), Neurofeedback and Emotion Regulation, Working Dog Wearable Interfaces, Canine-Human Communication Systems, Haptic Interfaces for Working Dogs, Inertial Sensor Gesture Recognition (IMU)", "summary": "## Overview\nMelody Moore Jackson is a **Professor of Computer Science** at **Georgia Tech**, where she leads research at the intersection of **Brain–Computer Interfaces (BCI)** and **Animal–Computer Interaction (ACI)**—two domains unified by a commitment to *assistive, communication-centered computing*. Her work emphasizes <u>nontraditional input and communication channels</u> for users whose needs are not met by conventional interfaces, spanning **nonmuscular neural control** for people with severe motor disabilities and **wearable, dog-activated systems** that enable *two-way communication* between working dogs and human handlers. Across this portfolio, Jackson is widely recognized for advancing <u>evaluation frameworks</u>, <u>practical interface designs</u>, and <u>field-aware methodologies</u> that translate laboratory innovations into high-stakes real-world settings.\n\n---\n\n## Research Areas\nJackson’s research in **brain–computer interfaces** has addressed both foundational and applied challenges in making BCIs usable outside idealized lab conditions. Early influential work examined how **steady-state visual evoked potentials (SSVEPs)** could support communication without relying on gaze shifting—an issue central to BCI accessibility for severely disabled users (e.g., *“Towards an independent brain–computer interface using steady state visual evoked potentials,”* 2008). Complementing system design, she contributed to the methodological maturation of the field through comparative and descriptive approaches to evaluation, including *“A general framework for characterizing studies of brain interface technology”* (2005) and *“Analyzing trends in brain interface technology: a method to compare studies”* (2006), which tackled inconsistent terminology and heterogeneous experimental reporting. Her BCI scholarship also extends to broader application and interface questions—captured in *“Applications for brain-computer interfaces”* (2010) and related work on individual differences in neural control (e.g., *“Individual characteristics and their effect on predicting mu rhythm modulation,”* 2010), reflecting a sustained interest in <u>who can effectively use BCI systems</u> and *why*.  \nIn parallel, Jackson has been a prominent contributor to **Animal–Computer Interaction**, particularly through the **FIDO** research program on **wearable communication interfaces for working dogs**. This line of work explores how dogs can intentionally trigger sensors and interfaces (e.g., *“FIDO-facilitating interactions for dogs with occupations: wearable dog-activated interfaces,”* 2013; *“FIDO—Facilitating interactions for dogs with occupations,”* 2015), how touch-based systems can be designed around canine capabilities (e.g., *“Going to the dogs: towards an interactive touchscreen interface for working dogs,”* 2014; *“Canine computer interaction,”* 2016), and how mobile/wearable systems can improve coordination in operational contexts such as **search and rescue** (e.g., *“Search and rescue: dog and handler collaboration through wearable and mobile interfaces,”* 2016). Her ACI work further includes rigorous assessment methods (e.g., *“A method to evaluate haptic interfaces for working dogs,”* 2017), gesture-based canine signaling (e.g., *“Creating collar-sensed motion gestures for dog-human communication,”* 2016; *“Towards a canine-human communication system based on head gestures,”* 2015), and data-driven approaches to predicting working-dog suitability (e.g., *“Predicting the suitability of service animals using instrumented dog toys,”* 2018). Collectively, these contributions position Jackson’s research around <u>assistive communication</u>, <u>embodied interaction</u>, and <u>evaluation under real constraints</u>—whether the user is a person relying on neural signals or a working animal operating in complex environments.\n\n---\n\n## Notable Works\n- **Advancing SSVEP-based BCI independence for severely disabled users:** In *“Towards an independent brain–computer interface using steady state visual evoked potentials”* (2008), Jackson and collaborators investigated SSVEP BCI interaction assumptions—especially reliance on gaze shifting—helping reframe design constraints for accessibility-oriented BCIs and influencing subsequent work on practical, reliable neural communication.\n- **Establishing shared evaluation language for brain interface research:** Through *“A general framework for characterizing studies of brain interface technology”* (2005) and the follow-on comparative approach in *“Analyzing trends in brain interface technology: a method to compare studies”* (2006), Jackson contributed tools for systematically describing and comparing BI/BCI studies, supporting more objective interpretation across diverse paradigms and reporting practices.\n- **Pioneering wearable, dog-activated communication interfaces for working dogs:** The FIDO line—exemplified by *“FIDO-facilitating interactions for dogs with occupations: wearable dog-activated interfaces”* (2013) and extended in *“FIDO—Facilitating interactions for dogs with occupations”* (2015)—helped define interaction techniques and design constraints for canine-worn systems, enabling intentional signaling and strengthening the foundations of modern ACI for high-stakes working contexts.\n\n---\n\n## Academic Background\nAs a **Professor of Computer Science at Georgia Tech**, Jackson has developed an interdisciplinary research trajectory spanning **human–computer interaction**, **neurotechnology**, and **animal-centered computing**, reflected in a publication record that moves from early contributions to **brain interface technology characterization and trend analysis** (2005–2006) into highly cited work on **SSVEP-based BCI design and applications** (2008–2010), and then into a sustained program of **Animal–Computer Interaction** research centered on working-dog communication and field deployment (2013–present). Her scholarship—accumulating **1704 citations**—indicates broad influence across both BCI and ACI communities, with impact driven not only by novel systems (e.g., wearable dog-activated interfaces and touchscreen interaction techniques) but also by *methodological leadership* in how emerging interface technologies are evaluated and compared. Given her long-running, application-oriented collaborations implied by search-and-rescue, service-dog, and police explosive-detection studies, her academic profile aligns with strong cross-sector partnerships and affiliations bridging computing research with assistive technology stakeholders and working-dog organizations, reinforcing her central emphasis on <u>deployable, evidence-based interaction design</u> for users operating at the limits of conventional interface assumptions."}, "8ThYcf8AAAAJ": {"keywords": "Privacy, Cybersecurity, EU Data Protection Directive, Cross-border Data Flows, Data Portability, Electronic Surveillance Law, Foreign Intelligence Surveillance Act (FISA), Encryption Policy, Mutual Legal Assistance Treaties (MLATs), Regulatory Compliance Requirements Engineering", "summary": "## Overview\nPeter Swire is the **Huang Professor of Law and Ethics** at the Georgia Institute of Technology, where he works at the intersection of **privacy** and **cybersecurity** with a sustained focus on how *legal institutions and technical change* reshape governance in the digital economy. His scholarship centers on <u>cross-border data flows</u>, <u>surveillance and intelligence law</u>, and <u>regulatory design for information systems</u>, combining doctrinal analysis with policy-oriented frameworks that speak to both public-sector accountability and private-sector compliance. With **4,102 citations**, Swire’s work has been especially influential in debates over transatlantic data protection, lawful access, and the evolving obligations of technology firms operating under multiple legal regimes.\n\n---\n\n## Research Areas\nSwire’s research spans three tightly connected domains. First, he is a leading voice on **international privacy and data protection**, analyzing how the EU’s regulatory architecture affects global commerce and governance; in “**None of your business: world data flows, electronic commerce, and the European privacy directive**” (2010), he examines the EU adequacy requirement and its implications for <u>international transfers of personal data</u> and the practical frictions it creates for electronic commerce. Second, he has developed an influential body of work on **surveillance, intelligence oversight, and constitutional privacy**, including “**The system of foreign intelligence surveillance law**” (2003) and “**Katz Is Dead—Long Live Katz**” (2003), which map the legal infrastructure of foreign intelligence collection and revisit Fourth Amendment doctrine as technologies alter the meaning of reasonable expectations of privacy. Third, Swire has advanced **privacy-and-security governance for regulated technologies**, including work on encryption policy (“**Encryption and globalization**,” 2011; and “From real-time intercepts to stored records…,” 2012) and on translating legal mandates into implementable requirements for software systems (e.g., “**Identifying and classifying ambiguity for regulatory requirements**,” 2014; and “Automated text mining for requirements analysis of policy documents,” 2013). Across these areas, he repeatedly interrogates how *well-intended rights and rules*—such as the EU’s “right to data portability”—can generate countervailing effects in competition and consumer welfare (“**Why the right to data portability likely reduces consumer welfare: antitrust and privacy critique**,” 2012).\n\n---\n\n## Notable Works\n- **Transatlantic data protection and global data flows:** In “**None of your business: world data flows, electronic commerce, and the European privacy directive**” (2010), Swire articulates how the EU’s adequacy-based transfer restrictions can function as a structural lever over global information governance, shaping compliance strategies and the architecture of cross-border services.  \n- **Frameworks for intelligence surveillance law and constitutional privacy:** “**The system of foreign intelligence surveillance law**” (2003), together with “**Katz Is Dead—Long Live Katz**” (2003), offers a systematic account of U.S. foreign intelligence surveillance authorities and oversight while updating Fourth Amendment analysis for modern communications and networked monitoring.  \n- **Critical analysis of mandated portability and its welfare effects:** In “**Why the right to data portability likely reduces consumer welfare: antitrust and privacy critique**” (2012), Swire evaluates the EU’s emerging portability right through antitrust and privacy lenses, emphasizing how mandated transfers can alter incentives, increase security risk, and potentially undermine *consumer welfare* despite pro-competition aims.\n\n---\n\n## Academic Background\nSwire’s publication record reflects a long-running academic trajectory spanning **public law, economic regulation, and information policy**, with early work engaging institutional design and regulatory competition (e.g., “The race to laxity and the race to undesirability…,” 1996) and financial/legal governance (e.g., “Bank insolvency law now that it matters again,” 1992; and “Financial privacy and the theory of high-tech government surveillance,” 1999). Over time, his scholarship increasingly concentrated on **privacy**, **cybersecurity**, and the governance of digital systems, including influential contributions on encryption policy and disclosure models for security (“A model for when disclosure helps security…,” 2004; “A theory of disclosure for security and competitive reasons…,” 2005). His affiliations and collaborations—evident in multi-domain work connecting law with software and compliance engineering (2011–2015) and in high-profile policy-facing outputs such as “Liberty and security in a changing world” (2013/2014)—suggest sustained engagement with both academic and governmental policy communities. Now at Georgia Tech as the **Huang Professor of Law and Ethics**, Swire’s career is marked by interdisciplinary reach and continuing influence on debates over <u>data protection</u>, <u>lawful access</u>, and <u>technology regulation</u>."}, "8mHLyRoAAAAJ": {"keywords": "Cryptography, Information Security, Provable Security, Public-Key Encryption, Deterministic Encryption, Searchable Encryption, Order-Preserving Encryption, Identity-Based Encryption, Threshold Signatures, Secure Transport Protocols (TLS/QUIC)", "summary": "## Overview\nAlexandra Boldyreva is a Professor in the School of Cybersecurity and Privacy at the Georgia Institute of Technology, where she leads an active research program at the intersection of **cryptography** and **information security**. Her work is best known for developing *provable-security foundations* for practical cryptographic primitives and protocols, with sustained emphasis on <u>rigorous security definitions</u> and <u>efficient constructions</u> that remain meaningful under real deployment constraints. Across a body of scholarship exceeding **10,800 citations**, Boldyreva has shaped how the community formalizes and builds systems for secure data handling, authenticated communication, and privacy-preserving computation—especially in settings where functionality requirements (e.g., search, range queries, revocation, multi-user use) force cryptography beyond its classical, randomized, single-user idealizations.\n\n---\n\n## Research Areas\nBoldyreva’s research spans foundational and applied cryptography, with a recurring theme of reconciling *useful functionality* with <u>formal security guarantees</u>. A major thread is **property-preserving and structured encryption**, initiated in her influential work on **order-preserving symmetric encryption**—“Order-preserving symmetric encryption” (2009)—which formalized OPE and clarified the security tradeoffs required to support efficient range queries on encrypted databases; she later deepened the theoretical picture in “Order-preserving encryption revisited: Improved security analysis and alternative solutions” (2011), addressing what security is achievable when ciphertext order must reflect plaintext order. Complementing this, she advanced **deterministic and searchable encryption** for database settings, notably in “Deterministic and efficiently searchable encryption” (2007) and “On notions of security for deterministic encryption, and efficient constructions without random oracles” (2008), where she helped articulate “as-strong-as-possible” privacy notions for deterministic public-key encryption and provided constructions enabling efficient exact-match search. Another core area is **public-key security in realistic environments**, including multi-user and multi-recipient settings: “Public-key encryption in a multi-user setting: Security proofs and improvements” (2000) and subsequent work on randomness reuse in multi-recipient encryption (e.g., “Randomness re-use in multi-recipient encryption schemeas” (2002); “Multi-recipient encryption schemes: Security notions and randomness re-use” (2003)) systematically identified overlooked attacks and refined the security models needed for correct, deployable proofs. She has also contributed substantially to **signature systems and delegation**, from early constructions in “Threshold signatures, multisignatures and blind signatures based on the gap-Diffie-Hellman-group signature scheme” (2002) to later formalization of delegation in “Secure proxy signature schemes for delegation of signing rights” (2012). Finally, her portfolio includes **protocol security analyses** of widely deployed standards and Internet infrastructure—e.g., “How secure and quick is QUIC? Provable security and performance analyses” (2015), “Provable-security analysis of authenticated encryption in Kerberos” (2011), and “Provable security analysis of FIDO2” (2021)—demonstrating a consistent commitment to bridging *theory* and <u>real-world protocol assurance</u>.\n\n---\n\n## Notable Works\n- **Foundations of order-preserving encryption for encrypted databases:** In “**Order-preserving symmetric encryption**” (2009) and the follow-up “**Order-preserving encryption revisited: Improved security analysis and alternative solutions**” (2011), Boldyreva initiated and then substantially refined the formal study of OPE, clarifying what can (and cannot) be achieved when encryption must preserve order to enable efficient range queries.\n- **Deterministic and searchable public-key encryption with strong privacy definitions:** Through “**Deterministic and efficiently searchable encryption**” (2007) and “**On notions of security for deterministic encryption, and efficient constructions without random oracles**” (2008), she helped establish precise security notions for deterministic encryption and delivered constructions enabling efficient search while making explicit the inherent leakage constraints.\n- **Multiparty signatures and threshold robustness in Gap Diffie–Hellman groups:** Her highly cited “**Threshold signatures, multisignatures and blind signatures based on the gap-Diffie-Hellman-group signature scheme**” (2002) provided robust proactive threshold signatures alongside multisignature and blind-signature constructions in GDH groups, influencing both the design space and the formal treatment of distributed signing.\n\n---\n\n## Academic Background\nBoldyreva’s publication record, beginning with influential early-2000s work on **multi-user public-key security** (2000) and **key-privacy/anonymity in encryption** (“Key-privacy in public-key encryption,” 2001), reflects a career rooted in the modern provable-security tradition and closely aligned with the leading venues of the field (e.g., CRYPTO/Eurocrypt-style contributions and standards-facing analyses). Her trajectory shows sustained engagement with both theory and practice: foundational modeling work on the **random oracle methodology** and its limits (e.g., “An uninstantiable random-oracle-model scheme for a hybrid-encryption problem,” 2004; OAEP analyses across 2005–2010) sits alongside later, systems-relevant security studies of **QUIC**, **Kerberos**, and **FIDO2**. As a senior faculty member at **Georgia Tech’s School of Cybersecurity and Privacy**, she is affiliated with one of the world’s leading cybersecurity research ecosystems, and her extensive citation impact and long-running contributions to core cryptographic primitives indicate sustained scholarly recognition consistent with major field-level influence and leadership in the academic cryptography community."}, "8n-iVhwAAAAJ": {"keywords": "Computational biology, Algorithms, Single-cell RNA-seq, Single-cell multi-omics integration, scATAC-seq, Spatial transcriptomics, Batch effect correction, Gene regulatory network inference, RNA velocity and trajectory inference, Lineage tracing and lineage reconstruction", "summary": "## Overview\nXiuwei Zhang is a researcher in **Computational Biology** and **Algorithms** at the Georgia Institute of Technology, where they contribute to the development of *statistically principled* and *machine-learning–enabled* methods for analyzing high-dimensional single-cell data. Their work centers on <u>single-cell and spatial omics</u>, with an emphasis on disentangling **biological variation** from **technical artifacts**, building scalable **data integration** frameworks across modalities (e.g., scRNA-seq and scATAC-seq), and creating realistic **simulators** that provide *ground truth* for benchmarking. Across a body of work exceeding **2,463 citations**, Zhang has helped shape how the field quantifies noise, evaluates algorithms, and infers dynamics and regulation from single-cell measurements.\n\n---\n\n## Research Areas\nZhang’s research spans core methodological problems in modern single-cell genomics, particularly where experimental noise, batch effects, and modality mismatch complicate inference. Early influential contributions addressed <u>technical noise modeling</u> in scRNA-seq through quantitative statistical frameworks designed to separate true cell-to-cell variability from measurement artifacts, as exemplified by “**Accounting for technical noise in single-cell RNA-seq experiments**” (2013). Building on this foundation, Zhang has advanced <u>simulation-based evaluation</u> of computational pipelines: “**Simulating multiple faceted variability in single cell RNA sequencing**” (2019) introduced **SymSim**, explicitly modeling processes that generate observed single-cell transcriptomes to enable controlled benchmarking; later work extended simulation to richer settings including multimodal and spatial contexts (e.g., “**scMultiSim**,” 2025; “**VeloSim**,” 2021; “**TedSim**,” 2022). A second major thread is <u>multi-omics integration</u>, developing algorithms that align heterogeneous datasets and learn cross-modality relationships without relying on low-fidelity handcrafted mappings, notably “**scDART**” (2022) for unmatched scRNA-seq/scATAC-seq integration and “**scMoMaT**” (2023) for mosaic integration and biomarker detection. Complementing integration, Zhang’s work addresses <u>biological dynamics and lineage</u>—including trajectory inference using RNA velocity (“**Inference of high-resolution trajectories in single-cell RNA-seq data by using RNA velocity**,” 2021) and lineage reconstruction from paired barcodes and expression (e.g., “**LinRace**,” 2023). A further line of research targets <u>regulatory and intercellular mechanisms</u>, including gene regulatory network recovery (“**GRNUlar**,” 2022; “**CeSpGRN**,” 2022) and the coupling of cell–cell interactions with regulatory network refinement in spatial transcriptomics (“**CLARIFY**,” 2023). Interwoven with these computational contributions are collaborative single-cell studies in immunology and disease, such as steroid biosynthesis in T helper cells (2014) and cytotoxic lymphocyte involvement in IgG4-related disease (2021), reflecting sustained engagement with *biologically grounded* applications.\n\n---\n\n## Notable Works\n- **Noise-aware single-cell quantification:** Developed a widely cited statistical approach to distinguish biological variability from measurement artifacts in scRNA-seq in “**Accounting for technical noise in single-cell RNA-seq experiments**” (2013), helping establish <u>noise modeling</u> as a prerequisite for reliable downstream inference.  \n- **Realistic simulation for benchmarking and method development:** Introduced **SymSim** in “**Simulating multiple faceted variability in single cell RNA sequencing**” (2019), an in silico platform that models multiple sources of variability in single-cell experiments and enabled rigorous evaluation of emerging analysis methods.  \n- **Cross-modality integration without predefined gene activity mappings:** Proposed “**scDART: integrating unmatched scRNA-seq and scATAC-seq data and learning cross-modality relationship simultaneously**” (2022), addressing a central limitation of prior integration approaches by jointly aligning modalities while learning the underlying cross-modality relationship.\n\n---\n\n## Academic Background\nBased on publication history, Zhang’s academic trajectory reflects sustained training and leadership in **computational genomics** and **algorithmic biology**, with early work emphasizing phylogenetic and evolutionary perspectives on biological networks (e.g., regulatory network refinement and phylogenetic transfer of information in papers from 2008–2012) before pivoting to the single-cell revolution with a landmark contribution in 2013 on scRNA-seq technical noise. Their affiliation with the **Georgia Institute of Technology** situates them in a highly interdisciplinary research environment spanning computing, statistics, and biomedical science, consistent with a portfolio that combines methodological innovation (simulation, integration, deep learning for GRNs) with collaborative biological discovery in immunology and disease. The breadth of venues implied by their cross-domain output—ranging from foundational computational method papers (e.g., **SymSim**, **scDART**, **scMoMaT**) to applied single-cell studies of T cell states and thymic epithelial development—suggests extensive collaborations across experimental and computational labs. With **2,463 citations** and multiple highly cited works, Zhang’s record indicates strong scholarly impact and field recognition, particularly in <u>single-cell data modeling</u>, <u>benchmarking via simulation</u>, and <u>multi-omics integration</u>."}, "90whi3wAAAAJ": {"keywords": "Robotics, Control and Optimization, Machine Learning, Safe Autonomy, Human-Robot Interaction, Learning from Demonstrations, Constraint Learning, Formal Methods and Temporal Logic (LTL), Safe Motion Planning and Reachability Analysis, Conformal Prediction and Statistical Safety Guarantees", "summary": "## Overview\nGlen Chou is an Assistant Professor at Georgia Tech, where he leads a research program at the intersection of **robotics**, **control and optimization**, and **machine learning**, with an emphasis on *trustworthy autonomy*. His work centers on <u>safe autonomy</u>: designing learning-enabled robotic systems that can plan and act under uncertainty while providing **formal or statistical safety guarantees**. Across a body of work with 673 citations, Chou’s research consistently integrates *model-based control theory* with *data-driven learning* to produce methods that are not only effective in practice but also grounded in rigorous notions of safety, reachability, and robustness—often in settings involving <u>unknown dynamics</u>, <u>learned perception</u>, and <u>human-robot interaction</u> through demonstrations and language.\n\n---\n\n## Research Areas\nChou’s research spans several tightly connected areas in safe, learning-enabled robotics. A major thrust is **constraint learning from demonstrations**, where he extends learning-from-demonstration beyond reward or policy inference to recover *unknown safety constraints* shared across tasks. In “Learning Constraints from Demonstrations” (2018) and “Learning Parametric Constraints in High Dimensions from Demonstrations” (2019), he develops methods that leverage system dynamics, task costs, and sampling (e.g., hit-and-run) to distinguish safe expert behavior from unsafe alternatives; later work generalizes this to more realistic settings, including cost uncertainty (“Learning Constraints from Locally-Optimal Demonstrations under Cost Function Uncertainty,” 2020), uncertainty-aware planning (“Uncertainty-Aware Constraint Learning for Adaptive Safe Motion Planning from Demonstrations,” 2020), and nonparametric representations (“Gaussian Process Constraint Learning for Scalable Chance-Constrained Motion Planning From Demonstrations,” 2022). A second core area is **safe motion planning and control under learned dynamics**, where he provides probabilistic guarantees on safety and reachability when the dynamics model is learned from data, as in “Planning With Learned Dynamics: Probabilistic Guarantees on Safety and Reachability via Lipschitz Constants” (2021) and “Model Error Propagation via Learned Contraction Metrics…” (2021), extending to output-feedback settings that incorporate high-dimensional sensing (“Safe Output Feedback Motion Planning from Images via Learned Perception Modules and Contraction Theory,” 2022). Complementing these contributions are works in **formal methods and reachability**, including early foundations in approximate solutions to Hamilton–Jacobi–Bellman PDEs (“Using neural networks to compute approximate and guaranteed feasible Hamilton-Jacobi-Bellman PDE solutions,” 2016) and safety-driven falsification for autonomous driving (“Using control synthesis to generate corner cases…,” 2018). Finally, Chou has advanced **human-robot communication and task specification**, notably by learning mappings from natural language to temporal-logic task specifications (“Data-Efficient Learning of Natural Language to Linear Temporal Logic Translators for Robot Task Specification,” 2023) and by inferring temporal logic structure directly from demonstrations (“Explaining Multi-stage Tasks by Learning Temporal Logic Formulas from Suboptimal Demonstrations,” 2020; “Learning temporal logic formulas from suboptimal demonstrations…,” 2022).\n\n---\n\n## Notable Works\n- **Constraint inference as a first-class object in learning from demonstration:** In “Learning Constraints from Demonstrations” (2018), Chou introduces a framework for recovering unknown constraints shared across tasks from safe demonstrations, using system dynamics and sampling-based generation of unsafe alternatives to make constraint recovery identifiable and practically useful for safe planning.\n- **Safety-guaranteed planning with learned models:** In “Planning With Learned Dynamics: Probabilistic Guarantees on Safety and Reachability via Lipschitz Constants” (2021), he develops a feedback motion-planning method for unknown dynamics that quantifies where a learned model can be trusted, yielding probabilistic guarantees on <u>safety</u>, <u>reachability</u>, and goal stability.\n- **Bridging human intent and formal robot specifications:** In “Data-Efficient Learning of Natural Language to Linear Temporal Logic Translators for Robot Task Specification” (2023), Chou advances data-efficient translation from natural language commands to <u>linear temporal logic</u> (LTL), enabling robots to interpret user intent in a formal language suitable for verification and synthesis.\n\n---\n\n## Academic Background\nAs an Assistant Professor at Georgia Tech, Glen Chou has established a research agenda that reflects deep training in **control theory**, **optimization**, and **formal methods**, coupled with a sustained focus on **machine learning for robotics**. His publication trajectory suggests an early foundation in reachability and optimal control—evident in work on neural-network approximations to Hamilton–Jacobi–Bellman PDE solutions (2016) and multi-agent safety analysis (“A hybrid framework for multi-vehicle collision avoidance,” 2017)—followed by a sustained and expanding program on learning-enabled safety. The progression from constraint learning in structured settings (2018–2019) to robust variants handling uncertainty in costs, observations, and stochasticity (2020–2023) indicates a consistent emphasis on translating theory into deployable methods for real robotic systems. His collaborations across topics such as autonomous driving falsification, perception-in-the-loop safety, and language-to-logic task specification reflect broad affiliations within the robotics and autonomy community, with scholarly impact evidenced by 673 citations and multiple highly cited papers that have helped shape modern approaches to <u>safe autonomy</u> through the principled integration of learning, control, and formal guarantees."}, "9ZIsgScAAAAJ": {"keywords": "online education, learning at scale, student experience, distance learning, computer science, digital proctoring and academic integrity, online graduate computer science programs, enrollment motivations and demographics, student retention and persistence, peer advising and student communities", "summary": "## Overview\nAlex Duncan is **Senior Director of OMSCS** at the **Georgia Institute of Technology**, where he provides academic and strategic leadership for one of the world’s most visible experiments in *affordable, online graduate computer science education*. Working at the intersection of program operations and education research, Duncan’s scholarship centers on <u>learning at scale</u>: understanding how large online degree programs can support **academic integrity**, **student success**, and a more *human-centered* **student experience** while maintaining rigor and access.\n\n---\n\n## Research Areas\nDuncan’s research spans the design, governance, and lived realities of large online CS degrees, with particular emphasis on evidence-driven policy and student-centered supports. A recurring theme is <u>academic integrity in online assessment</u>, most prominently in his 2022 study, **“On the necessity (or lack thereof) of digital proctoring: Drawbacks, perceptions, and alternatives,”** which synthesizes institutional needs with student perceptions and practical alternatives to surveillance-oriented proctoring. He also investigates who enrolls in at-scale programs and why, analyzing demographic variation in motivations in **“Enrollment motivations in an online graduate CS program: Trends & gender-and age-based differences”** (2020), and extending this student-centered lens to preparation pathways in **“Ready or not, here I computer science”** (2023). Complementing learner-focused work, Duncan examines <u>faculty and instructional spillovers</u> between online and campus teaching in **“Teaching at Scale and Back Again”** (2023), and contributes meta-research on the field itself through reviews and trend analyses of Learning@Scale publications (e.g., **“The l@ st eight years”** (2023) and **“Ten years, ten trends”** (2024)). His portfolio further reflects applied research leadership in data stewardship and compliance for education studies, including pragmatic guidance on <u>GDPR constraints</u> in **“With or without EU”** (2021), alongside community-oriented work on peer advising ecosystems and student life initiatives that promote belonging and engagement at scale (2019; 2023).\n\n---\n\n## Notable Works\n- **Digital assessment integrity without overreliance on surveillance:** In **“On the necessity (or lack thereof) of digital proctoring: Drawbacks, perceptions, and alternatives”** (2022), Duncan advances a nuanced account of proctoring trade-offs, foregrounding student perceptions and institutional integrity goals while detailing alternative approaches to uphold rigor in online environments.\n- **Who enrolls in at-scale CS degrees—and why:** In **“Enrollment motivations in an online graduate CS program: Trends & gender-and age-based differences”** (2020), he documents patterns in enrollment motivations and highlights how motivations vary by demographic factors, informing recruitment, messaging, and student support strategies in large online graduate programs.\n- **A decade-level empirical view of affordable degrees at scale:** In **“Ten years, ten trends: The first decade of an affordable at-scale degree”** (2024), Duncan characterizes longitudinal program trends using publicly available data, offering a field-facing framework for interpreting growth, evolution, and operational realities in <u>at-scale online degrees</u>.\n\n---\n\n## Academic Background\nBased at Georgia Tech in a senior leadership role within OMSCS, Duncan’s academic profile reflects a hybrid trajectory typical of research-active education leaders in large online programs: sustained administrative stewardship paired with a publication record in **learning at scale**, online program analytics, and education policy. His works indicate deep institutional engagement with OMSCS as a living laboratory for studying online graduate CS education, including admissions and retention dynamics (2025), student preparation and research opportunities (2023–2025), and community infrastructure such as peer advising and student life initiatives (2019; 2023). His collaborations and meta-analyses of the Learning@Scale community (2023–2024) suggest active affiliation with the broader international research network shaping scalable digital pedagogy. With **129 total citations**, his most-cited contributions cluster around high-impact, practice-relevant questions—particularly digital proctoring and enrollment motivations—underscoring a scholarly agenda oriented toward rigorous, ethical, and scalable improvements to online CS education."}, "A_j_baAAAAAJ": {"keywords": "Computer Architecture, Computer Security, Side Channels, Software Engineering, Electromagnetic Side-Channel Analysis, Secure Processor Design, Memory Encryption and Integrity Verification, Dynamic Taint Analysis, Hardware Trojan Detection, Rollback Recovery and Checkpointing", "summary": "## Overview\nMilos Prvulovic is a Professor of Computer Science at Georgia Tech, where he leads a research program at the intersection of **computer architecture** and **computer security**, with a sustained emphasis on *bridging principled microarchitectural design and real-world attack/defense needs*. His work is widely recognized for developing **practical, measurable, and deployable** mechanisms that improve system **reliability**, **performance**, and **trustworthiness**, particularly in the presence of sophisticated adversaries and complex software stacks. Across his publications, Prvulovic has helped define how <u>hardware-supported security</u> and <u>side-channel-aware design</u> can be evaluated, engineered, and integrated into modern systems—from secure processor memory hierarchies to electromagnetic (EM) leakage analysis and anomaly detection.\n\n---\n\n## Research Areas\nPrvulovic’s research spans several tightly connected themes in modern computing systems. In **architectural simulation and evaluation**, he co-developed the widely used *SESC simulator* (“**SESC simulator**,” 2005), enabling detailed exploration of microarchitectural and multiprocessor design choices and helping standardize experimental methodology in architecture research. In **secure architecture**, he has made foundational contributions to memory protection mechanisms that combine confidentiality and integrity with practical performance: “**Improving cost, performance, and security of memory encryption and authentication**” (2006) and “**Using address independent seed encryption and bonsai merkle trees to make secure processors os-and performance-friendly**” (2007) advance counter-mode encryption, integrity verification, and OS-friendly secure memory designs that reduce overheads and complexity. In **reliability and debugging for parallel systems**, he introduced rollback and checkpointing techniques such as “**ReVive: Cost-effective architectural support for rollback recovery in shared-memory multiprocessors**” (2002), and later leveraged speculation/rollback ideas for developer productivity in “**ReEnact: Using thread-level speculation mechanisms to debug data races in multithreaded codes**” (2003), alongside low-overhead race detection approaches such as “**CORD**” (2006). A major later thread of his work focuses on **electromagnetic side channels and external observation of computation**: he helped experimentally establish practical EM leakage in commodity systems (“**Experimental demonstration of electromagnetic information leakage from modern processor-memory systems**,” 2014), proposed quantitative metrics for leakage (“**A practical methodology for measuring the side-channel signal available to the attacker for instruction-level events**,” 2014), and developed *observer-effect-free* methods for profiling and security monitoring via EM emanations (“**Spectral profiling**,” 2016; “**Zero-overhead profiling via em emanations**,” 2016). This EM-centric direction extends to both attack and defense, including cryptanalytic demonstrations such as “**{One&Done}: A {Single-Decryption}{EM-Based} Attack on {OpenSSL’s}{Constant-Time} Blinded {RSA}**” (2018) and system-level defenses for embedded/IoT anomaly and malware detection (“**IDEA**,” 2019; “**Syndrome**,” 2018; “**REMOTE**,” 2019), as well as hardware assurance via novel physical channels for detecting hardware Trojans (“**Creating a backscattering side channel to enable detection of dormant hardware trojans**,” 2019).\n\n---\n\n## Notable Works\n- **SESC simulator (2005)** — A highly cited contribution that helped shape experimental practice in architecture by providing a flexible, detailed platform for evaluating microarchitectural and multiprocessor ideas at scale (“**SESC simulator**,” 2005).\n- **Secure processor memory encryption and integrity with OS/performance awareness (2006–2007)** — A sequence of influential papers that reduced the cost and improved the practicality of secure memory systems, notably through combined encryption/authentication and OS-friendly integrity structures (“**Improving cost, performance, and security of memory encryption and authentication**,” 2006; “**Using address independent seed encryption and bonsai merkle trees to make secure processors os-and performance-friendly**,” 2007).\n- **Electromagnetic side-channel measurement, exploitation, and mitigation-oriented analytics (2014–2018)** — A body of work that moved EM side channels from qualitative concern to measurable, actionable engineering, including experimental demonstrations of leakage, the SAVAT metric for instruction-level leakage, and a landmark single-decryption EM attack on a constant-time RSA implementation (“**Experimental demonstration of electromagnetic information leakage from modern processor-memory systems**,” 2014; “**A practical methodology for measuring the side-channel signal available to the attacker for instruction-level events**,” 2014; “**{One&Done}**,” 2018).\n\n---\n\n## Academic Background\nAs a long-standing faculty member at the Georgia Institute of Technology, Prvulovic has built an internationally visible research portfolio in **computer architecture**, **security**, and **software/system reliability**, reflected in a sustained record of highly cited publications (total citations: **6365**) and recurring impact across multiple subfields. His early and influential work on speculative and multiprocessor execution—spanning scalability (“Removing architectural bottlenecks to the scalability of speculative parallelization,” 2001), checkpointing/rollback (“ReVive,” 2002), and out-of-order resource management (“Cherry,” 2002)—helped establish him as a leading voice in practical architectural mechanisms for performance and dependability. Over time, his agenda expanded toward security and developer-facing system correctness, including hardware-assisted debugging and monitoring (“ReEnact,” 2003; “Memtracker,” 2007) and dynamic tainting accelerators (“FlexiTaint,” 2008). In the past decade, his group has been closely associated with the emergence of **electromagnetic side-channel analysis** as both an attack surface and a tool for *external, non-intrusive* profiling and intrusion detection, producing a coherent line of work from leakage measurement and modeling (“SAVAT,” 2014; “EMSim,” 2020) to embedded/IoT security monitoring (“IDEA,” 2019; “REMOTE,” 2019) and hardware assurance (“backscattering side channel,” 2019). Collectively, these contributions indicate deep engagement with the computer architecture and security research communities and sustained leadership in advancing <u>measurement-driven secure and reliable system design</u>."}, "AaVPa5kAAAAJ": {"keywords": "scientific machine learning, uncertainty quantification, stochastic optimization, Bayesian inference, reduced basis methods, model order reduction, PDE-constrained optimization under uncertainty, Bayesian inverse problems, Bayesian optimal experimental design, neural operators", "summary": "## Overview\nPeng Chen is a faculty researcher at the Georgia Institute of Technology, where he leads and collaborates on efforts at the intersection of **scientific machine learning**, **uncertainty quantification**, and **Bayesian inference** for complex systems governed by partial differential equations (PDEs). His work centers on building *scalable* algorithms for many-query settings—such as inversion, optimal experimental design, and optimization under uncertainty—by combining <u>model order reduction</u>, <u>gradient/Hessian-informed dimension reduction</u>, and <u>derivative-aware neural surrogates</u>. Across a body of work exceeding **2,235 citations**, Chen has advanced computational methodology that makes high-dimensional stochastic modeling feasible in practice, particularly when each forward evaluation requires expensive PDE solves.\n\n---\n\n## Research Areas\nChen’s research spans foundational and applied computational science, with a consistent emphasis on combating the curse of dimensionality in stochastic PDE problems. A major theme is <u>reduced basis (RB) methods</u> for UQ and parametric PDEs, including systematic treatments and reviews of RB for UQ in “**Reduced basis methods for uncertainty quantification**” (2017) and algorithmic refinements such as “**A weighted reduced basis method for elliptic partial differential equations with random input data**” (2013), which introduces weighting strategies to better reflect probabilistic structure in random inputs. He has also contributed comparative and hybrid approaches, as in “**Comparison between reduced basis and stochastic collocation methods for elliptic problems**” (2014), clarifying tradeoffs between projection-based reduction and non-intrusive stochastic discretizations for elliptic PDEs.\n\nA second core area is <u>Bayesian inverse problems</u> and scalable posterior computation. Chen’s work develops geometry-aware methods that exploit intrinsic low-dimensional structure in posteriors, exemplified by “**Projected Stein variational Newton**” (2019) and “**Projected Stein Variational Gradient Descent**” (2020), which project particle-based variational inference onto informative subspaces to enable high-dimensional Bayesian inference. Complementing sampling, he has advanced integration and quadrature for Bayesian computation, including “**Hessian-based adaptive sparse quadrature for infinite-dimensional Bayesian inverse problems**” (2017) and “**Sparse quadrature for high-dimensional integration with Gaussian measure**” (2018), both leveraging curvature or measure structure to allocate computational effort efficiently.\n\nA third thrust is <u>optimization under uncertainty</u> (OUU) and <u>Bayesian optimal experimental design</u> (OED) for PDE-governed systems. His contributions include scalable risk-aware control formulations (“**Taylor approximation and variance reduction for PDE-constrained optimal control under uncertainty**,” 2019) and large-scale OED frameworks that target expected information gain and sensor placement (“**A fast and scalable computational framework for large-scale high-dimensional Bayesian optimal experimental design**,” 2023; and related offline–online decompositions in 2023). More recently, Chen has integrated scientific ML into these pipelines with *derivative-informed* surrogates, including “**Derivative-informed projected neural networks for high-dimensional parametric maps governed by PDEs**” (2022) and “**Derivative-informed neural operator**” (2024), which learn parameter-to-output/operator maps while explicitly incorporating derivative information to improve sample efficiency and generalization in high-dimensional settings. His methods have been demonstrated across application domains, from cardiovascular hemodynamics UQ (“**Simulation-based uncertainty quantification of human arterial network hemodynamics**,” 2013) to robust design problems such as “**Optimal design of acoustic metamaterial cloaks under uncertainty**” (2021).\n\n---\n\n## Notable Works\n- **Reduced basis UQ for stochastic PDEs:** Established and synthesized RB methodology for uncertainty-aware PDE modeling in “**Reduced basis methods for uncertainty quantification**” (2017), building on weighted RB formulations for random-input elliptic problems in “**A weighted reduced basis method for elliptic partial differential equations with random input data**” (2013).  \n- **Projected Stein methods for scalable Bayesian inference:** Introduced dimension-reduced particle variational inference in “**Projected Stein variational Newton**” (2019) and expanded the framework in “**Projected Stein Variational Gradient Descent**” (2020), enabling high-dimensional Bayesian inference by exploiting intrinsic posterior geometry.  \n- **Derivative-informed scientific ML surrogates for many-query PDE tasks:** Developed derivative-enhanced learning architectures for parametric PDE maps in “**Derivative-informed projected neural networks for high-dimensional parametric maps governed by PDEs**” (2022) and generalized the approach to operator learning in “**Derivative-informed neural operator**” (2024), supporting fast UQ, inversion, and OED/OUU workflows.\n\n---\n\n## Academic Background\nBased on his publication record and recurring emphasis on reduced basis methods, sparse grids, and PDE-constrained optimization, Chen’s academic formation is strongly rooted in **numerical analysis**, **scientific computing**, and **computational mathematics** for stochastic PDEs. Early work includes a thesis-length contribution on computational methods (e.g., “**The lattice Boltzmann method for fluid dynamics: Theory and applications**,” 2011) and a later dissertation-style consolidation focused on reduction and UQ (“**Model order reduction techniques for uncertainty quantification problems**,” 2014), indicating sustained training in rigorous algorithmic development and analysis. His subsequent career trajectory shows increasing leadership in bridging classical model reduction with modern probabilistic computation—spanning RB-accelerated Bayesian inversion (2015–2016), Hessian-informed quadrature and sampling (2017–2019), and projected variational inference methods (2019–2022)—before expanding into **scientific machine learning** and **neural operator** methodologies (2022–2025). Now at the Georgia Institute of Technology, Chen is affiliated with a research ecosystem that emphasizes high-performance and data-driven scientific computing, and his citation impact (over **2,235 citations**) reflects broad adoption of his contributions across UQ, Bayesian inversion, and PDE-governed design and optimization."}, "B2Qy_xAAAAAJ": {"keywords": "VR/AR, Immersive Analytics, Data Visualization, Human-Data Interaction, HCI, Metaverse, Geospatial Visualization, Flow Map Visualization, Network Visualization, Situated AR Visualization", "summary": "## Overview\nYalong Yang is an Assistant Professor at **Georgia Tech**, where he leads research within the *immersive visualization and interaction* community associated with the <u>Information Visualization & Interaction (IVI) Lab</u> (https://ivi.cc.gatech.edu/). His scholarship centers on **VR/AR** and **Immersive Analytics**, developing *human-centered* methods that connect **data visualization** with **human-data interaction** in spatial computing environments. With over **2,197 citations**, Yang’s work advances the design and empirical evaluation of <u>immersive visual interfaces</u> that help people explore complex data—particularly **geospatial flows**, **networks**, and *situated* real-world tasks—through interactive, embodied, and collaborative experiences.\n\n---\n\n## Research Areas\nYang’s research spans **Immersive Analytics**, **VR/AR visualization**, and **HCI**, with a strong emphasis on *evaluation-driven* design. A major thread of his work investigates how immersive environments reshape the visual analysis of geographically embedded data, including dense origin–destination and many-to-many flow patterns (e.g., “**Origin-destination flow maps in immersive environments**,” 2018; “**Many-to-many geographically-embedded flow visualisation: An evaluation**,” 2016) and the rendering of maps and globes for global-scale understanding in VR (e.g., “**Maps and globes in virtual reality**,” 2018). He also develops novel interaction techniques that bridge 2D and 3D representations, exemplified by “**Tilt map**” (2020), which supports interactive transitions among choropleth, prism, and bar-chart encodings in immersive settings. Beyond geospatial visualization, Yang contributes to foundational questions about immersive sensemaking—such as navigation, viewpoint, and embodied interaction—through controlled studies (e.g., “**Embodied navigation in immersive abstract data visualization**,” 2020; “**Is Embodied Interaction Beneficial? A Study on Navigating Network Visualizations**,” 2023) and cognitive considerations for scalable visual analysis (e.g., “**Scalability of network visualisation from a cognitive load perspective**,” 2020). His applied AR research extends immersive analytics into *situated* domains including sports training and viewing, studying real-time, co-located visualization and embedded information access (e.g., “**Towards an Understanding of Situated AR Visualization for Basketball Free-Throw Training**,” 2021; “**The Quest for Omnioculars**,” 2022). More recently, his agenda broadens toward <u>hybrid and collaborative immersive systems</u> (e.g., distributed asymmetric collaboration, hybrid UI with tablets/headsets), and emerging paradigms that connect immersive environments with larger socio-technical visions such as the **metaverse** (e.g., “**Metaverse: Perspectives from graphics, interactions and visualization**,” 2022).\n\n---\n\n## Notable Works\n- **Framing immersive worlds for visualization research:** In “**Metaverse: Perspectives from graphics, interactions and visualization**” (2022), Yang co-articulates a unifying framework and research perspective for how graphics, interaction, and visualization can support the *visual construction and exploration* of metaverse-like environments, helping shape an emerging interdisciplinary agenda.\n- **Defining the research frontier of Immersive Analytics:** Through “**Grand Challenges in Immersive Analytics**” (2021), Yang helps synthesize the field’s open problems—spanning interaction, perception, evaluation methods, and systems—providing a widely cited roadmap that anchors subsequent work on <u>immersive data analysis</u>.\n- **Empirical foundations for immersive geospatial flow visualization:** In “**Origin-destination flow maps in immersive environments**” (2018) and related evaluation work on dense flows (e.g., “**Many-to-many geographically-embedded flow visualisation: An evaluation**,” 2016), Yang advances evidence-based design guidance for representing and interacting with complex movement data in VR/AR contexts.\n\n---\n\n## Academic Background\nYang’s publication trajectory indicates a sustained research program in **information visualization** and **HCI**, progressing from early contributions on hierarchical visualization (e.g., “Visualizing large hierarchies with drawer trees,” 2014; “Cabinet Tree,” 2015) and empirical evaluation of visualization techniques (e.g., hierarchical quantitative data comparisons in 2019) toward a distinctive specialization in **immersive analytics** and **VR/AR-based** visual interfaces. His influential, highly cited works from 2016–2022 on geospatial flows, immersive mapping, and field-level synthesis (including the “Grand Challenges” and metaverse perspective papers) suggest deep engagement with the international visualization community through collaborative, multi-author efforts and agenda-setting scholarship. As an Assistant Professor at **Georgia Tech**, he is affiliated with a leading research ecosystem in computing and interactive systems, and his recent papers on hybrid immersive interfaces, distributed asymmetric collaboration, immersive computational notebooks, and AI-assisted learning analytics (2024–2025) reflect an expanding portfolio that bridges *foundational evaluation* with *systems-oriented* research and interdisciplinary applications."}, "BHZT068AAAAJ": {"keywords": "Online Algorithms, Economics and Computation, Stochastic Optimization, Learning Theory, Prophet Inequalities, Secretary Problems, Stochastic Probing, Algorithmic Mechanism Design, Matroid Optimization, Submodular Optimization", "summary": "## Overview\nSahil Singla is an Assistant Professor in the School of Computer Science at Georgia Tech, where he leads research at the intersection of **online algorithms**, **economics and computation**, and **stochastic optimization**. His work develops *provably efficient* methods for decision-making under uncertainty, with a particular emphasis on <u>random-order and stochastic input models</u>, <u>prophet inequalities</u>, and <u>adaptivity in sequential optimization</u>. Across theory and applications, Singla’s research aims to characterize fundamental limits and design algorithms and mechanisms that remain robust when information is partial, costly, or revealed over time.\n\n---\n\n## Research Areas\nSingla’s research spans several tightly connected themes in theoretical computer science. A central line of work studies <u>prophet inequalities and stopping theory</u> in combinatorial settings, extending classical single-choice results to rich feasibility constraints and valuation classes. In “**Combinatorial prophet inequalities**” (2017), he develops a framework for prophet inequalities with combinatorial objectives—e.g., (non-monotone) submodular maximization under matroid constraints—yielding constant-competitive guarantees, and thereby linking online selection to core problems in submodular optimization. This program continues through settings motivated by mechanism design, including “**Prophet secretary for combinatorial auctions and matroids**” (2018), which fuses the secretary model’s random arrival with prophet-style benchmarks for auctions and matroid feasibility, and “**Optimal Online Contention Resolution Schemes via Ex-Ante Prophet Inequalities**” (2018), which connects ex-ante relaxations to online rounding via OCRSs—tools that have become foundational for stochastic combinatorial optimization with commitment constraints.\n\nA second major thrust addresses <u>stochastic probing and the value of adaptivity</u>, where information acquisition is constrained and outcomes are random. In “**Algorithms and adaptivity gaps for stochastic probing**” (2016) and its follow-up “**Adaptivity gaps for stochastic probing: Submodular and XOS functions**” (2017), Singla studies how much is lost when algorithms must be non-adaptive, establishing approximation guarantees and quantifying adaptivity gaps for broad objective classes such as submodular and XOS functions. Complementing this, his work on <u>information costs in optimization</u> formalizes tradeoffs between exploration and decision quality: “**The Price of Information in Combinatorial Optimization**” (2018) and “**The Markovian Price of Information**” (2019) model settings where learning exact costs/rewards incurs explicit payments, yielding algorithmic principles for when and what to reveal.\n\nSingla has also contributed to <u>online learning and discrepancy/vector balancing</u>, including models with vector-valued costs (“**Online learning with vector costs and bandits with knapsacks**,” 2020) and online discrepancy minimization under stochastic arrivals (“**Online discrepancy minimization for stochastic arrivals**,” 2021), as well as broader expository work on the <u>random-order model</u> (“**Random-Order Models**,” 2020). His publication record additionally reflects breadth: from exact performance analysis in systems-oriented probability models (“**Exact analysis of TTL cache networks**,” 2014) to algorithmic mechanism design (truthful mechanisms for submodular/subadditive combinatorial auctions, 2019–2021) and related advances in matroid algorithms (“**Faster matroid intersection**,” 2019).\n\n---\n\n## Notable Works\n- **Combinatorial prophet inequalities** (2017): Introduced a unifying framework for prophet inequalities with combinatorial objectives, including constant-competitive algorithms for submodular maximization under matroid constraints and extensions to broader valuation classes—helping establish <u>prophet inequalities</u> as a core tool in stochastic combinatorial optimization.\n- **Prophet secretary for combinatorial auctions and matroids** (2018): Developed prophet-secretary guarantees for multi-item settings central to mechanism design, integrating random arrival with prophet-style benchmarks under matroid and auction constraints, and advancing <u>online mechanisms</u> for combinatorial allocation.\n- **Algorithms and adaptivity gaps for stochastic probing** (2016) (and **Adaptivity gaps for stochastic probing: Submodular and XOS functions**, 2017): Quantified the power of adaptivity in stochastic probing, providing approximation algorithms and rigorous <u>adaptivity gap</u> bounds for objectives including submodular and XOS functions under probing/feasibility constraints.\n\n---\n\n## Academic Background\nAt Georgia Tech, Singla has built a research agenda grounded in **theoretical computer science** and strongly connected to communities in *algorithms*, *learning theory*, and *economic design*. His publication trajectory suggests an evolution from early work combining probabilistic modeling and systems/optimization perspectives—such as the widely cited “Exact analysis of TTL cache networks” (2014) and earlier energy-systems studies on pricing and storage adoption (2012–2014)—toward a sustained focus on <u>stochastic and online optimization</u> with deep ties to **mechanism design** and **matroid/submodular** methods. The breadth and impact of this program are reflected in his substantial citation record (1927 citations) and repeated contributions to foundational problem classes—prophet/secretary models, stochastic probing, information acquisition, and random-order online algorithms—indicating active engagement with leading venues and collaborations across algorithms, economics and computation, and learning-oriented optimization."}, "BXEDBDUAAAAJ": {"keywords": "computer science education, learning sciences, motivation, culture and technology, participatory design, broadening participation in computing, computing identity, digital games for learning, assets-based design, intersectionality in HCI", "summary": "## Overview\nBetsy DiSalvo is an **Associate Professor** in the **School of Interactive Computing** at **Georgia Tech**, where she leads research at the intersection of **computer science education**, the **learning sciences**, and **human-computer interaction (HCI)**. Her scholarship centers on how *motivation, identity, and culture* shape participation in computing, and how designers and educators can build learning environments that broaden participation without reducing learners to deficits. Across her work, she advances <u>participatory design</u>, <u>assets-based design</u>, and <u>intersectional</u> approaches as key concepts for creating equitable learning technologies and pathways into computing.\n\n---\n\n## Research Areas\nDiSalvo’s research spans (1) **participatory design and the learning sciences**, (2) **motivation and identity in computing**, and (3) **equity-oriented technology design for families and communities**. In learning-sciences–grounded design scholarship, she articulates methods for <u>participatory design of learning environments</u> and argues for participatory design as both a design practice and a learning process (e.g., *“Participatory design for learning”* (2017) and *“Designing for democracy in education: Participatory design and the learning sciences”* (2014), alongside *“Participatory design through a learning science lens”* (2016)). In computing education, she examines how **gamification** and game-related practices influence engagement—critiquing simplistic motivational add-ons while mapping how platforms operationalize game elements (e.g., *“Khan academy gamifies computer science”* (2014) and *“Pink boxes and chocolate-dipped broccoli…”* (2015)). A major thread of her work investigates how **culture, race, gender, and peer norms** shape computing identity and participation, particularly for young Black men and other historically marginalized learners (e.g., *“Learning in context: Digital games and young black men”* (2008), *“African American men constructing computing identity”* (2011), and *“Saving face while geeking out…”* (2014)). More recently, she has advanced <u>intersectionality</u> and <u>assets-based design</u> as analytic and design orientations for HCI and CSCW, providing conceptual tools for moving from needs-based framings to strengths-based interventions (e.g., *“Designing for intersections”* (2018), *“Culture in action: Unpacking capacities to inform assets-based design”* (2020), and *“From needs to strengths: Operationalizing an assets-based design of technology”* (2020)). Complementing these contributions, her work also addresses the information and engagement infrastructures surrounding learning—such as parent information seeking and parent-school technologies—often emphasizing how inequities in access and communication shape educational opportunity (e.g., *“Design guidelines for parent-school technologies…”* (2017) and *“Information seeking practices of parents…”* (2016)).\n\n---\n\n## Notable Works\n- **Advancing participatory design as a learning-sciences practice**: In *“Participatory design for learning”* (2017), DiSalvo synthesizes design practices tailored to learning contexts, positioning <u>participatory design</u> as central to creating learning environments that are both effective and responsive to stakeholders.\n- **Interrogating motivation and gamification in CS learning platforms**: In *“Khan academy gamifies computer science”* (2014), she analyzes how gamification is implemented in a widely used CS learning ecosystem, connecting specific mechanics (e.g., points/badges) to motivational theory and design implications for computing education.\n- **Operationalizing intersectionality for technology design**: In *“Designing for intersections”* (2018), she develops an actionable lens for <u>intersectionality</u> in HCI, offering a way to analyze and design technologies that account for overlapping structures of identity and power rather than treating demographic categories in isolation.\n\n---\n\n## Academic Background\nAt Georgia Tech, DiSalvo has built a research portfolio spanning **HCI**, **CS education**, and the **learning sciences**, with a publication record that reflects sustained engagement in premier community venues aligned with these areas (e.g., learning-sciences–oriented participatory design, CSCW/HCI equity scholarship, and computing education research). Her trajectory shows long-term commitment to **broadening participation in computing**, including work connected to large-scale pipeline efforts such as *“Georgia Computes! improving the computing education pipeline”* (2009), which aligns with the **NSF Broadening Participation in Computing** ecosystem and its community partnerships. Across her career, she has combined empirical qualitative methods (including attention to mediated qualitative data collection, as in *“Qualitative data collection technologies…”* (2012)) with theory-driven design frameworks, contributing to both methodological rigor and equity-centered design practice. With **2,860 citations**, her work has become influential in shaping how researchers and practitioners think about culture, identity, and power in the design of computing learning experiences and sociotechnical systems."}, "BfOdG-oAAAAJ": {"keywords": "Natural Language Processing, Social Media, Machine Learning, Text Simplification, Paraphrase Generation, Noisy User-Generated Text, Named Entity Recognition, Multilingual NLP, Natural Language Generation Evaluation, Privacy and Bias in Large Language Models", "summary": "## Overview\nWei “Coco” Xu is a researcher at the Georgia Institute of Technology, where she leads and collaborates with teams at the intersection of **Natural Language Processing** and **Machine Learning** with a sustained emphasis on *real-world language use* in **social media** and other noisy, high-variance domains. Her scholarship centers on building <u>data-driven, evaluation-grounded text generation and understanding systems</u>, particularly for **text simplification**, **paraphrasing**, and **responsible language technologies** (e.g., bias and privacy in large language models). With **6,319 citations**, Xu’s work is widely recognized for combining methodological rigor with resources—datasets, shared tasks, and benchmarks—that shape community practice.\n\n---\n\n## Research Areas\nXu’s research spans three tightly connected threads: (1) **text simplification and controllable rewriting**, (2) **paraphrase and sentence-pair modeling**, and (3) **robust NLP for user-generated and multilingual text**, increasingly extended to *responsible* and *trustworthy* LLM use. In text simplification, she has advanced both modeling and problem framing—optimizing statistical MT approaches for simplification in “**Optimizing statistical machine translation for text simplification**” (2016) and challenging field assumptions about dataset over-reliance in “**Problems in current text simplification research: New data can help**” (2015). She later broadened the technical toolkit with alignment and supervision improvements (“**Neural CRF model for sentence alignment in text simplification**,” 2020), controllability mechanisms (“**Controllable text simplification with explicit paraphrasing**,” 2021), and modern evaluation (“**LENS: A learnable evaluation metric for text simplification**,” 2023), alongside multilingual and domain-specific simplification efforts (e.g., “**Revisiting non-English text simplification: A unified multilingual benchmark**,” 2023; “**Multilingual simplification of medical texts**,” 2023). In paraphrasing and semantic similarity, her contributions include style-conditioned rewriting (“**Paraphrasing for style**,” 2012), large-scale paraphrase acquisition from social platforms (“**A Continuously Growing Dataset of Sentential Paraphrases**,” 2017), and systematic comparisons of neural sentence-pair architectures (“**Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering**,” 2018). Complementing these, Xu has helped define best practices for **noisy text processing** through community evaluations and shared tasks (e.g., WNUT and SemEval Twitter paraphrase tasks), and more recently has examined <u>cultural bias</u> and <u>privacy risks</u> in modern foundation models (“**Having beer after prayer? measuring cultural bias in large language models**,” 2024; “**Reducing privacy risks in online self-disclosures with language models**,” 2024), connecting core NLP research to societal deployment constraints.\n\n---\n\n## Notable Works\n- **Reframing and advancing text simplification as a data- and evaluation-driven discipline**: Xu’s influential pair of works—“**Optimizing statistical machine translation for text simplification**” (2016) and “**Problems in current text simplification research: New data can help**” (2015)—both improved practical simplification systems and reshaped how the community thinks about datasets, assumptions, and generalization beyond Simple Wikipedia.\n- **Building foundational resources and community infrastructure for noisy social media NLP**: Through shared-task leadership and datasets such as “**Shared tasks of the 2015 workshop on noisy user-generated text: Twitter lexical normalization and named entity recognition**” (2015) and “**Semeval-2015 task 1: Paraphrase and semantic similarity in twitter (pit)**” (2015), Xu helped standardize evaluation for normalization, NER, and paraphrase/semantic similarity under real-world noise conditions.\n- **Pioneering modern measurement of cultural bias in large language models**: In “**Having beer after prayer? measuring cultural bias in large language models**” (2024), Xu and collaborators developed a research agenda and methodology for assessing how multilingual and monolingual LMs encode culturally situated assumptions—extending fairness/bias analysis beyond predominantly Western-centric benchmarks.\n\n---\n\n## Academic Background\nBased at the Georgia Institute of Technology, Xu’s publication trajectory indicates a long-standing engagement with core NLP problems—summarization and information extraction in earlier work (e.g., “**Extractive summarization using inter-and intra-event relevance**,” 2006), followed by substantial contributions to **information extraction** and **distant supervision** (e.g., “**Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction**,” 2013; “**Infusion of Labeled Data into Distant Supervision for Relation Extraction**,” 2014). Her subsequent research emphasis consolidated around sentence-level meaning and rewriting—paraphrasing, style transfer, and simplification—paired with a strong record of *field-building service* through benchmark creation and shared-task organization (WNUT 2015/2016; WNUT 2020 wet-lab IE; GEM benchmark for NLG evaluation, 2021). The breadth of venues implied by these works—spanning shared tasks, benchmarks, and high-impact empirical studies—reflects sustained affiliations with the broader computational linguistics community and a pattern of recognition that often accompanies widely adopted datasets, evaluation frameworks, and highly cited methodological contributions."}, "BlL6XAsAAAAJ": {"keywords": "Cyber Conflict, Political Psychology, Public Opinion, Experimental Methods, Cyberattacks and Political Trust, Cyberterrorism, Threat Perception, Survey Experiments, Cybersecurity Policy Attitudes, Military Escalation and Retaliation", "summary": "## Overview\nRyan Shandler is a researcher at the Georgia Institute of Technology whose work sits at the intersection of **cyber conflict**, **political psychology**, and **public opinion**. Drawing heavily on *experimental methods*—especially survey and cross-national survey experiments—Shandler examines how citizens interpret, emotionally process, and politically respond to cyber incidents. Across this agenda, a central throughline is the <u>societal and democratic consequences of cyber operations</u>, with particular attention to how cyberattacks can reshape **trust in government**, public tolerance for *intrusive security policies*, and support for escalation in international crises.\n\n---\n\n## Research Areas\nShandler’s research develops a micro-foundational account of mass politics under cyber threat, emphasizing how information environments and psychological mechanisms translate cyber events into political attitudes. In “**The hidden threat of cyber-attacks–undermining public confidence in government**” (2023), he advances and tests the argument that the most consequential harms of cyberattacks often arise not from physical destruction but from <u>erosion of political trust</u>, using survey data gathered in the immediate aftermath of a ransomware attack. Complementing this trust-centered line of inquiry, “**Cyberattacks, cyber threats, and attitudes toward cybersecurity policies**” (2021) investigates how exposure to cyber incidents influences support for intrusive cybersecurity measures, highlighting the mediating role of perceived threat. A parallel strand addresses escalation dynamics and retaliation preferences: “**Cyber terrorism and public support for retaliation–a multi-country survey experiment**” (2022) and “**A fragile public preference for cyber strikes**” (2021) use randomized designs across multiple countries to show how framing, perceived destructiveness, and uncertainty shape public willingness to endorse cyber operations or conventional military responses. Methodologically, Shandler also contributes to cumulative inference in this area through “**Cyberattacks, Psychological Distress, and Military Escalation: An Internal Meta-Analysis**” (2023), consolidating evidence on affective responses—especially distress—and their implications for escalation. Beyond crisis and conflict, his work extends to the political and normative stakes of digital dependence, including internet access as a rights-bearing social necessity (“**A Reality of Vulnerability and Dependence: Internet Access as a Human Right**,” 2019; “**Can You Engage in Political Activity Without Internet Access?**,” 2019), and to conceptual clarification in a rapidly expanding field (“**Public Opinion and Cyberterrorism**,” 2023).\n\n---\n\n## Notable Works\n- **Identifying political trust as a primary societal vulnerability to cyberattacks**: In “*The hidden threat of cyber-attacks–undermining public confidence in government*” (2023), Shandler foregrounds <u>institutional confidence</u> as a central outcome of cyber incidents and empirically tests this claim with post-attack survey evidence, helping reorient cyber-threat assessment toward democratic and social resilience.\n- **Explaining support for intrusive cybersecurity policy through threat perception**: “*Cyberattacks, cyber threats, and attitudes toward cybersecurity policies*” (2021) maps how exposure to cyberattacks can translate into public backing for more invasive security measures, emphasizing perceived cyber threat as a key psychological channel linking events to policy preferences.\n- **Advancing cross-national experimental evidence on retaliation and escalation after cyber terrorism**: In “*Cyber terrorism and public support for retaliation–a multi-country survey experiment*” (2022), Shandler uses controlled, randomized exposure to news depictions of cyber and conventional terrorism to isolate the psychological mechanisms that increase support for retaliatory action, strengthening comparative inference about escalation pressures in cyber crises.\n\n---\n\n## Academic Background\nBased at the Georgia Institute of Technology, Shandler’s publication record and citation impact (755 total citations) indicate a rapidly consolidating scholarly profile in international security and political psychology, with a distinctive emphasis on *experimental* and *quasi-real-time* public opinion measurement around cyber incidents. His work shows sustained engagement with cross-national research designs (e.g., studies spanning the United States, United Kingdom, Israel, and multi-country samples) and with field-defining conceptual issues in cyberterrorism and cyber conflict (e.g., “Public Opinion and Cyberterrorism,” 2023; “Introduction: Cyber-conflict–Moving from speculation to investigation,” 2024). The breadth of venues and formats—from empirical journal-style articles and internal meta-analysis (2023) to policy-facing and public scholarship on internet access and digital vulnerability (2019–2020)—suggests active affiliation with interdisciplinary communities spanning security studies, political behavior, and technology policy. Collectively, this trajectory reflects a scholar positioned within, and contributing to, the maturation of cyber-conflict research from speculative debate toward cumulative, methodologically rigorous investigation grounded in public attitudes and psychological mechanisms."}, "C-NftTgAAAAJ": {"keywords": "Data Science, Machine Learning, AI, Databases, Network Science, Information Diffusion, Epidemic Modeling, Graph Mining, Network Embeddings, Influence Maximization", "summary": "## Overview\nB. Aditya Prakash is a Professor at the **Georgia Institute of Technology**, where he leads research at the intersection of **Data Science**, **Machine Learning**, **AI**, and **Databases** with a sustained emphasis on *networked and temporal data*. His scholarship centers on building principled, scalable methods for understanding and shaping complex systems—especially those governed by diffusion and contagion dynamics—by combining algorithmic foundations with modern learning paradigms. Across highly cited contributions in information diffusion, epidemic modeling, and graph mining, his work advances <u>predictive and prescriptive analytics on large graphs</u>, with particular attention to *real-world decision-making* in public health, online platforms, and cyber-infrastructure.\n\n---\n\n## Research Areas\nPrakash’s research spans (i) **diffusion and cascade modeling** in online media and social systems, exemplified by analyses of temporal popularity trajectories in “**Rise and fall patterns of information diffusion: model and implications**” (2012) and learning-based cascade forecasting in “**DeepDiffuse: Predicting the 'who' and 'when' in cascades**” (2018); (ii) **network epidemiology and control**, including foundational work on epidemic thresholds and stability such as “**Threshold conditions for arbitrary cascade models on arbitrary networks**” (2012) and time-varying contact processes in “**Virus propagation on time-varying networks: Theory and immunization algorithms**” (2010), alongside algorithmic interventions such as “**On the vulnerability of large graphs**” (2010), “**Node immunization on large graphs: Theory and algorithms**” (2015), and spectral-control approaches in “**Approximation algorithms for reducing the spectral radius to control epidemic spread**” (2015); and (iii) **operational forecasting and evaluation for public health**, including real-time COVID-19 forecasting and rigorous assessment of probabilistic and ensemble systems in “**Ensemble forecasts of coronavirus disease 2019 (COVID-19) in the US**” (2020), “**DeepCOVID: An operational deep learning-driven framework for explainable real-time covid-19 forecasting**” (2021), and “**Evaluation of individual and ensemble probabilistic forecasts of COVID-19 mortality in the United States**” (2022). Complementing these themes are contributions to **graph representation learning** (e.g., “**Sub2Vec: Feature Learning for Subgraphs**,” 2018), **graph manipulation and optimization** (e.g., “**Gelling, and melting, large graphs by edge manipulation**,” 2012), and emerging directions in **foundation-model and physics-informed learning** (e.g., “**PINNsFormer**,” 2024; “**LstPrompt**,” 2024), reflecting a broader agenda of <u>scalable learning and optimization for structured data</u>.\n\n---\n\n## Notable Works\n- **Characterizing temporal diffusion dynamics in online media:** In “**Rise and fall patterns of information diffusion: model and implications**” (2012), Prakash helped formalize how information popularity grows and decays, linking observable diffusion curves to mechanisms and implications for social media and news propagation.\n- **Algorithmic control of contagion and robustness in large networks:** Through works such as “**On the vulnerability of large graphs**” (2010) and “**Node immunization on large graphs: Theory and algorithms**” (2015), he advanced scalable strategies for selecting nodes to immunize/monitor, grounding intervention design in graph structure and spectral properties.\n- **Operational, evaluated epidemic forecasting at national scale:** In “**Evaluation of individual and ensemble probabilistic forecasts of COVID-19 mortality in the United States**” (2022) (and related ensemble work in 2020), he contributed to rigorous evaluation and synthesis of probabilistic forecasts, strengthening the evidence base for *ensemble* approaches in public-facing decision support.\n\n---\n\n## Academic Background\nAs a faculty member at **Georgia Tech**, Prakash has built an internationally visible research program in graph-centric data science, reflected in a strong and sustained publication record across network science, machine learning, and applied forecasting, and evidenced by **7,911 citations**. His early highly cited contributions (circa 2009–2012) on internet measurement and privacy-preserving mining (e.g., “**BGP-lens**,” 2009; “**FRAPP**,” 2009) and on epidemic thresholds and network vulnerability (“**Got the flu (or mumps)? check the eigenvalue!**,” 2010; “**On the vulnerability of large graphs**,” 2010) indicate deep roots in scalable algorithms for large, real-world systems. Over time, his work broadened into *learning-driven* methods for graphs and time series (e.g., “**Sub2Vec**,” 2018; “**DeepDiffuse**,” 2018) and into high-impact public health collaborations during COVID-19, including ensemble forecasting and evaluation efforts (“**Ensemble forecasts…**,” 2020; “**Evaluation…**,” 2022). These trajectories, together with repeated contributions to community-facing forecasting benchmarks and surveys (e.g., reviews on epidemic modeling and graph reduction in 2024), suggest sustained leadership and cross-disciplinary affiliation spanning **computer science**, **public health analytics**, and **networked systems**, with an emphasis on translating methodological advances into <u>deployable, decision-relevant tools</u>."}, "C2k6mYwAAAAJ": {"keywords": "Manufacturing Generative AI, Digital Twins for Manufacturing, Materials Informatics, Crystal Plasticity Modeling, Crystallographic Texture Evolution, Deformation Twinning, Microstructure-Sensitive Design, Process-Structure-Property Linkages, Microstructure Informatics (2-Point Statistics), Nanoindentation Stress–Strain Characterization", "summary": "## Overview\nSurya R. Kalidindi is a senior materials and manufacturing researcher at the Georgia Institute of Technology (Atlanta, GA), where he leads efforts in the Multiscale Materials and Manufacturing group (Multiscale.Tech) focused on building **data- and physics-integrated** frameworks for accelerated materials and process innovation. His work centers on *bridging experiments, mechanistic modeling, and machine learning* to create <u>process–structure–property (PSP) linkages</u> that enable **digital twins for manufacturing**, **materials informatics**, and emerging directions in **manufacturing generative AI**. Across foundational contributions in crystal plasticity and texture evolution and more recent advances in microstructure informatics and deep learning, Kalidindi’s research has shaped modern approaches to <u>microstructure-sensitive design</u> and ICME-driven decision-making at scale.\n\n---\n\n## Research Areas\nKalidindi’s research spans multiscale mechanics, computational materials science, and materials data science, unified by the goal of extracting actionable structure–property knowledge from complex, heterogeneous materials systems. Early, highly cited work established rigorous **polycrystalline plasticity** and **texture evolution** modeling for FCC metals, notably through Taylor-type formulations and robust numerical integration strategies in “Crystallographic texture evolution in bulk deformation processing of FCC metals” (1992) and “Polycrystalline plasticity and the evolution of crystallographic texture in FCC metals” (1992). A major theme in his mechanics portfolio is <u>deformation twinning</u> as a microstructural mechanism governing anisotropy and strain hardening: he advanced constitutive and computational treatments in “Incorporation of deformation twinning in crystal plasticity models” (1998) and connected twinning to measurable mechanical response in alloys such as magnesium and titanium, including “Deformation twinning in AZ31: Influence on strain hardening and texture evolution” (2010) and “Strain hardening of titanium: role of deformation twinning” (2003). In parallel, his group contributed influential experimental–mechanistic studies of MAX phases (e.g., Ti\\$_3\\$SiC\\$_2\\$), including unusual reversible deformation behavior (“Fully reversible, dislocation-based compressive deformation of Ti3SiC2 to 1 GPa,” 2003) and indentation-driven characterization methods (“Spherical nanoindentation stress–strain curves,” 2015). Over the last decade, Kalidindi has been a key architect of **microstructure informatics** and **materials data science**, developing statistical representations (e.g., 2-point statistics), reconstruction methods (“Microstructure reconstructions from 2-point statistics using phase-recovery algorithms,” 2008), and scalable learning-based surrogates, including deep learning for structure–property mining (“Material structure-property linkages using three-dimensional convolutional neural networks,” 2018) and simulation-informed analytics (“Deep learning approaches for mining structure-property linkages in high contrast composites from simulation datasets,” 2018). These contributions align closely with ICME systems thinking (“Key computational modeling issues in integrated computational materials engineering,” 2013) and have helped set the stage for contemporary <u>digital twin</u> workflows in manufacturing and data-centric materials design.\n\n---\n\n## Notable Works\n- **Crystal plasticity and texture evolution in FCC metals:** Developed and validated Taylor-type polycrystal frameworks and robust integration schemes enabling quantitative prediction of texture evolution and stress–strain response in forming-relevant deformation paths, as established in “Crystallographic texture evolution in bulk deformation processing of FCC metals” (1992) and complemented by “Polycrystalline plasticity and the evolution of crystallographic texture in FCC metals” (1992).  \n- **Constitutive modeling of deformation twinning and its mechanical consequences:** Introduced a widely adopted constitutive and computational framework to incorporate twinning crystallography into polycrystal plasticity (“Incorporation of deformation twinning in crystal plasticity models,” 1998) and connected twinning to anisotropic hardening and texture changes in key structural alloys (“Deformation twinning in AZ31: Influence on strain hardening and texture evolution,” 2010; “Strain hardening of titanium: role of deformation twinning,” 2003).  \n- **Foundations of microstructure informatics and data-driven PSP linkages:** Advanced statistical microstructure representations and reconstruction (“Microstructure reconstructions from 2-point statistics using phase-recovery algorithms,” 2008) and helped catalyze modern materials data science through integrative perspectives and machine-learning pipelines (“Materials data science: current status and future outlook,” 2015; “Material structure-property linkages using three-dimensional convolutional neural networks,” 2018).\n\n---\n\n## Academic Background\nBased at the Georgia Institute of Technology, Kalidindi has built an internationally recognized research program spanning mechanics, materials processing, and data-centric manufacturing science, reflected in a sustained, high-impact publication record and a large citation footprint (over 33,000 citations). The chronology and thematic evolution of his work—from seminal 1990s contributions in polycrystal plasticity and deformation texture, through 2000s advances in twinning-aware constitutive modeling and experimental mechanics (including nanoindentation and MAX-phase deformation), to 2010s leadership in ICME, microstructure-sensitive design, and materials informatics—suggest a career characterized by continuous expansion from mechanistic fundamentals to scalable, computationally enabled engineering practice. His widely cited syntheses, including “Key computational modeling issues in integrated computational materials engineering” (2013) and “A review of the application of machine learning and data mining approaches in continuum materials mechanics” (2019), indicate influential roles in shaping community research directions and interdisciplinary collaborations linking materials science, mechanical engineering, and data science, consistent with Georgia Tech’s strengths in advanced manufacturing and multiscale modeling."}, "C8LDYK0AAAAJ": {"keywords": "Data, Data Management, Data Systems, Fast Data Analytics, Streaming Time Series Analysis, Attention Prioritization, Approximate Query Processing, Similarity Search (LSH/ANN), Vector Databases and Hybrid Queries, ML Pipeline Optimization and Automated Preprocessing", "summary": "## Overview\nKexin Rong is a researcher in the School of Computer Science at the Georgia Institute of Technology, where they contribute to the broader data management and data systems community with an emphasis on **scalable analytics** and *human-centered* system design. Their work centers on building systems that help users extract value from high-volume, high-velocity data by <u>prioritizing attention</u>, enabling efficient detection of salient structure (e.g., anomalies, trends, or rare events) in streaming and large-scale datasets. Across projects spanning fast data engines, time-series analytics, and modern AI-native data platforms, Rong’s research advances **data systems** that combine algorithmic efficiency with practical usability constraints, reflecting a consistent focus on <u>making large-scale analytics both computationally and cognitively tractable</u>.\n\n---\n\n## Research Areas\nRong’s research spans core topics in **data management**, **data systems**, and data-centric machine learning, with a distinctive throughline around <u>attention prioritization</u> and scalable similarity-driven analytics. In fast data settings, their work on *MacroBase* (“Macrobase: Prioritizing attention in fast data,” 2017; and the related principles paper “Prioritizing attention in fast data: Principles and promise,” 2017) frames analytics as an interface between massive telemetry streams and limited human bandwidth, emphasizing modular pipelines that surface the most actionable patterns. Complementing this, “ASAP: prioritizing attention via time series smoothing” (2017) addresses the visualization and monitoring problem by smoothing streaming telemetry so that long-range structure is visible despite noise—an approach aligned with operational analytics in modern platforms. Rong has also contributed to scalable, similarity-based discovery in scientific data: “Locality-sensitive hashing for earthquake detection” (2018) and “Unsupervised large-scale search for similar earthquake signals” (2019) apply **locality-sensitive hashing (LSH)** and large-scale search to continuous seismic waveforms, demonstrating how data systems techniques enable *data-driven science* at scale. Beyond streaming and scientific search, their work engages algorithmic foundations for efficient computation in high dimensions (“Rehashing kernel evaluation in high dimensions,” 2019), and extends into data-centric ML workflows, such as automated preprocessing pipeline optimization (“Diffprep: Differentiable data preprocessing pipeline search for learning over tabular data,” 2023). More recent papers indicate an expanding systems scope toward AI-native infrastructure and multimodal data management, including explorations of cloud/AI co-design (“Computing in the era of large generative models: From cloud-native to AI-native,” 2024) and interactive video analytics interfaces (“SketchQL: Video moment querying with a visual query interface,” 2024), reinforcing a sustained interest in <u>systems that make complex data queryable and actionable</u>.\n\n---\n\n## Notable Works\n- **MacroBase—an engine for attention-first fast data analytics**: In “**Macrobase: Prioritizing attention in fast data**” (2017), Rong and collaborators introduce a data analytics engine designed to surface the most important patterns in high-volume streams, operationalizing <u>attention prioritization</u> as a systems goal and influencing how practitioners think about anomaly explanation and actionable summarization in telemetry-heavy environments.\n- **Attention-aware time-series visualization via smoothing**: “**ASAP: prioritizing attention via time series smoothing**” (2017) advances practical time-series monitoring by smoothing streaming telemetry to reveal large-scale trends that raw plots often obscure, connecting data systems concerns (streaming, latency, usability) with *human interpretability* in observability workflows.\n- **Scaling similarity search for scientific discovery in seismology**: In “**Locality-sensitive hashing for earthquake detection: A case study of scaling data-driven science**” (2018) (alongside the related “Unsupervised large-scale search for similar earthquake signals,” 2019), Rong applies LSH-based retrieval to continuous seismic time series, showing how <u>approximate similarity search</u> can unlock efficient detection of recurring earthquakes in years of waveform data.\n\n---\n\n## Academic Background\nRong is affiliated with the School of Computer Science at **Georgia Institute of Technology**, with a publication record that reflects sustained engagement with leading topics in **data systems** and adjacent areas such as scalable machine learning infrastructure and scientific data analytics. The arc of their work—from early contributions on fast data and time-series attention mechanisms (2017) through high-dimensional acceleration methods (2019) and into differentiable preprocessing and AI-native systems directions (2023–2024)—suggests an academic trajectory grounded in systems research with strong algorithmic foundations and practical, end-to-end evaluation. Their collaborations and venues implied by the paper portfolio indicate active participation in the data management and systems research community, including work that bridges academia and industry-relevant problem settings (e.g., telemetry analytics, cluster management, checkpoint compression, and AI-relational systems demonstrations). With **587 citations** and highly cited contributions such as *MacroBase*, Rong’s impact is especially visible in the emerging research program around <u>human-in-the-loop, attention-centric analytics</u> for modern, high-scale data platforms."}, "CCGI7x4AAAAJ": {"keywords": "high performance computing, parallel computing, sparse matrix-vector multiplication (SpMV), sparse matrix kernels, autotuning, GPU computing (GPGPU), performance modeling, sparse tensor computations, memory hierarchy optimization (cache blocking/prefetching), energy roofline model", "summary": "## Overview\nRichard Vuduc is a faculty researcher at the **Georgia Institute of Technology**, where he leads a research program at the intersection of **high-performance computing (HPC)** and **parallel computing**. His work centers on making performance *predictable* and *portable* across rapidly evolving architectures by combining <u>performance modeling</u> with <u>autotuning</u> and careful data-structure design. Across multicore CPUs, GPUs, and heterogeneous systems, Vuduc’s scholarship emphasizes **sparse and irregular computations**—especially sparse linear algebra and sparse tensor kernels—where memory behavior, locality, and communication costs dominate runtime and energy.\n\n---\n\n## Research Areas\nVuduc’s research spans systems, algorithms, and software frameworks for optimizing computational kernels that are both ubiquitous and notoriously architecture-sensitive. A major theme is <u>sparse matrix-vector multiplication (SpMV)</u>, explored through multicore-focused optimization in “Optimization of sparse matrix-vector multiplication on emerging multicore platforms” (2007) and through principled limits-and-bounds reasoning in “Performance optimizations and bounds for sparse matrix-vector multiply” (2002). He has advanced <u>automatic performance tuning</u> as a methodology and as deployable software: the “OSKI: A library of automatically tuned sparse matrix kernels” (2005) and the broader “Sparsity: Optimization framework for sparse matrix kernels” (2004) articulate how data layouts (e.g., blocking and register tiling) can be systematically selected rather than hand-crafted, while “POET: Parameterized optimizations for empirical tuning” (2007) and “Statistical models for empirical search-based performance tuning” (2004) formalize empirical and model-based search. As accelerators became central, Vuduc extended model-driven tuning to GPUs in “Model-driven autotuning of sparse matrix-vector multiply on GPUs” (2010), complemented by diagnostic work such as “A performance analysis framework for identifying potential benefits in GPGPU applications” (2012) and architectural investigations of latency tolerance and memory behavior (e.g., “When prefetching works, when it doesn’t, and why,” 2012; “Many-thread aware prefetching mechanisms for GPGPU applications,” 2010). In later work, his focus broadened from sparse matrices to <u>sparse tensors</u> and data analytics kernels, contributing storage formats and repositories that support reproducible research and scalable computation (e.g., “FROSTT” 2017; “HiCOO” 2018), as well as GPU/CPU optimization of tensor primitives (e.g., “Optimizing sparse tensor times matrix,” 2016; “Load-balanced sparse mttkrp on gpus,” 2019). He has also contributed to performance/energy reasoning via “A roofline model of energy” (2013) and to communication-centric analysis for exascale (e.g., “On the communication complexity of 3D FFTs,” 2012), reflecting a sustained interest in <u>time–energy–communication tradeoffs</u> for modern HPC.\n\n---\n\n## Notable Works\n- **Architecture-aware SpMV optimization for multicore systems** — In “Optimization of sparse matrix-vector multiplication on emerging multicore platforms” (2007), Vuduc and collaborators provided influential analyses and techniques for improving SpMV on multicore processors, addressing the kernel’s *memory-bound* nature and irregular access patterns that limit achievable performance.\n- **Foundational autotuning software for sparse kernels (OSKI)** — “OSKI: A library of automatically tuned sparse matrix kernels” (2005) introduced a practical, library-oriented approach to <u>automatic tuning</u> of sparse primitives (including SpMV and triangular solve), demonstrating how performance portability can be achieved through systematic exploration of data structures and kernel variants.\n- **Model-driven autotuning for GPU SpMV** — “Model-driven autotuning of sparse matrix-vector multiply on GPUs” (2010) advanced a performance-model-guided framework for selecting among GPU SpMV implementations, helping establish a principled bridge between analytic modeling and empirical tuning on accelerator-based systems.\n\n---\n\n## Academic Background\nBased on his publication trajectory and early dissertation-related work on “Automatic performance tuning of sparse matrix kernels” (2003), Vuduc’s academic formation is strongly rooted in the HPC tradition of combining numerical kernel insight with systems-level performance engineering. His longstanding association with sparse kernel autotuning and the OSKI/Sparsity line of work aligns with the broader ecosystem of performance-tuned scientific computing libraries and suggests deep engagement with major HPC research communities (e.g., those centered on scalable linear algebra, performance modeling, and heterogeneous computing). At Georgia Tech, he has sustained a high-impact, highly cited research program (over **10,000 citations**) that bridges fundamental performance theory (bounds, communication complexity, roofline-style models) with practical software artifacts and evaluation methodologies (autotuning frameworks, performance diagnosis tools, and open repositories such as FROSTT). His collaborations across multicore, GPU, and distributed-memory contexts—ranging from sparse direct solvers and fast multipole methods to tensor decompositions and, more recently, high-level modeling for transformer/LLM systems (“Calculon,” 2023)—reflect an academic profile characterized by cross-cutting affiliations within **parallel computing**, **scientific computing**, and performance-centric **computer systems** research."}, "CeEO6SIAAAAJ": {"keywords": "Data-Centric AI, Trustworthy AI, Uncertainty Quantification, Large Language Models (LLMs), Spatiotemporal Modeling, Traffic Prediction, Human Mobility & Trajectory Mining, Weak Supervision & Distant Supervision, Retrieval-Augmented Generation (RAG), Graph Neural Networks (GNNs)", "summary": "## Overview\nChao Zhang is an Assistant Professor in the **College of Computing** at **Georgia Tech**, where he leads a research program at the intersection of **Data-Centric AI** and **Trustworthy AI** with a sustained emphasis on *learning from imperfect data* and deploying models in *high-stakes, real-world settings*. His work advances <u>spatiotemporal modeling</u> for urban and mobility systems, <u>uncertainty quantification</u> for reliable deep learning, and <u>LLM-centered</u> methods that improve robustness through tools, retrieval, and data generation. With **17,840** total citations, Zhang’s research has helped define modern approaches to traffic forecasting, mobility prediction, weak supervision, and the emerging paradigm of *tool-augmented and calibrated language models*.\n\n---\n\n## Research Areas\nZhang’s scholarship spans several tightly connected research areas. In **spatiotemporal forecasting**, he helped establish graph-based deep learning as a practical foundation for intelligent transportation through *T-GCN: A temporal graph convolutional network for traffic prediction* (2019), which couples temporal dynamics with graph convolution to model road-network dependencies for real-time forecasting. In **human mobility modeling**, his work addresses sequential regularities and contextual signals in location traces, exemplified by *Deepmove: Predicting human mobility with attentional recurrent networks* (2018), and earlier contributions on semantic trajectories and pattern mining (e.g., *SERM* (2017) and *Splitter* (2014)), which treat mobility as a structured sequence problem with rich metadata. A second major thread is **data-centric and weakly supervised NLP**, where Zhang develops methods to learn high-quality text models with limited or noisy labels, including distant supervision for open-domain NER (*BOND* (2020)), label-name-driven and weakly supervised text classification (*Text classification using label names only* (2020); *Weakly-supervised neural text classification* (2018)), and programmatic weak supervision synthesis and denoising (*A survey on programmatic weak supervision* (2022); *Denoising multi-source weak supervision for neural text classification* (2020)). In **trustworthy deep learning**, he has advanced *practical uncertainty estimation* and reliability, including *SDE-Net: Equipping deep neural networks with uncertainty estimates* (2020) and calibration-aware fine-tuning (*Calibrated language model fine-tuning for in- and out-of-distribution data* (2020)). More recently, Zhang’s work engages **large language models and agents**, spanning multimodal grounding (*SALMONN: Towards generic hearing abilities for large language models* (2023)), tool use and external reasoning (*ToolQA* (2023); *Toolchain*: (2023)), and retrieval-augmented generation and evaluation-oriented designs (*RankRAG* (2024); *Large language models for generative information extraction: A survey* (2024)), with a consistent focus on reducing hallucination, improving robustness, and aligning model behavior with real-world constraints.\n\n---\n\n## Notable Works\n- **Spatiotemporal graph learning for traffic systems:** *“T-GCN: A temporal graph convolutional network for traffic prediction”* (2019) introduced a widely adopted framework that integrates **graph convolution** with **temporal modeling** to capture networked dependencies in traffic forecasting at scale.  \n- **Attention-based mobility prediction:** *“Deepmove: Predicting human mobility with attentional recurrent networks”* (2018) advanced **human mobility modeling** by capturing high-order sequential patterns and time-dependent behaviors via attentional recurrent architectures for next-location prediction and trajectory understanding.  \n- **LLM grounding and tool augmentation for reliable reasoning:** *“SALMONN: Towards generic hearing abilities for large language models”* (2023) expanded LLM capability into general audio understanding, while *“ToolQA: A dataset for LLM question answering with external tools”* (2023) provided a benchmark and methodology for **tool-augmented QA** to mitigate hallucination and improve numerical and procedural reliability.\n\n---\n\n## Academic Background\nZhang’s publication trajectory indicates a sustained, research-intensive career spanning early work in **geo-social computing** and **spatiotemporal data mining** (e.g., event detection and clustering in geo-tagged social streams such as *STREAMCUBE* (2015) and *GeoBurst* (2016)), followed by influential contributions to **mobility analytics**, **trajectory representation learning**, and **graph-based forecasting** that culminated in highly cited systems like *T-GCN* (2019). His subsequent body of work reflects a broadening into **data-centric NLP**—including weak supervision, distant supervision, and taxonomy/topic mining—and then into **trustworthy and calibrated deep learning**, as seen in uncertainty estimation (*SDE-Net* (2020)) and OOD-aware calibration (*Calibrated language model fine-tuning* (2020)). As an Assistant Professor at **Georgia Tech’s College of Computing**, he is affiliated with the institute’s broader AI and data science ecosystem and has contributed to community-shaping resources, including surveys (e.g., *Large language models for generative information extraction* (2024); *A survey on programmatic weak supervision* (2022)) and datasets for tool-using LLMs (*ToolQA* (2023)). The scale and breadth of his citation record, together with cross-domain collaborations (e.g., biomedical and materials text mining), suggest a research profile recognized across **machine learning**, **NLP**, and **spatiotemporal intelligence** communities."}, "CpVsPWEAAAAJ": {"keywords": "Astrophysics, black hole accretion, radiative feedback, intermediate-mass black holes, supermassive black holes, radiation hydrodynamics simulations, AGN feedback, galaxy clusters (cool-core clusters), seed black hole growth (early universe), dynamical friction", "summary": "## Overview\nKwangHo Park is an astrophysicist at the Georgia Institute of Technology whose work centers on the physics of **black hole accretion** and **feedback** in gaseous environments, spanning scales from seed and intermediate-mass black holes to cluster cores. Working at the interface of theory and computation, Park is best known for radiation-(magneto)hydrodynamic modeling that quantifies how *ionizing radiation, radiation pressure, and gas thermodynamics* self-regulate inflow. A unifying theme across his research is the emergence of <u>feedback-regulated growth</u>—how **radiative** and **kinetic feedback** shape accretion rates, luminosities, and the surrounding medium, with implications for early-universe black hole formation and the coevolution of black holes and galaxies.\n\n---\n\n## Research Areas\nPark’s research program develops and applies high-resolution numerical simulations (e.g., ZEUS-MP and Enzo radiation-hydrodynamics) to determine how black holes accrete when their own emission alters the inflowing gas. His early, highly cited series on radiation-regulated accretion onto intermediate-mass black holes establishes a parametric foundation for quasi-spherical inflow, explicitly modeling the structure and evolution of the ionized region and the resulting time-dependent accretion behavior (“**Accretion onto intermediate-mass black holes regulated by radiative feedback. I**,” 2011). Building on this, he derives scaling relations for growth rates and *duty cycles* while incorporating additional physics such as angular momentum and radiation pressure (“**Accretion onto black holes from large scales regulated by radiative feedback. II**,” 2012), and extends the framework to black holes moving supersonically through ambient gas, explaining conditions for enhanced luminosity and altered wake morphology (“**…III. Enhanced luminosity… moving at supersonic speeds**,” 2013). Beyond accretion regulation, Park has investigated the dynamical consequences of feedback, including how radiative heating reshapes gaseous wakes and modifies orbital evolution via <u>gaseous dynamical friction</u> (“**Gaseous dynamical friction in presence of black hole radiative feedback**,” 2017). He has also explored environments that can boost or transform black hole fueling—such as bulge-embedded reservoirs (“**Bulge-driven fueling of seed black holes**,” 2016), dusty gas that couples strongly to radiation and enables observational diagnostics of Eddington ratios (“**Dusty Gas Accretion… and Infrared Diagnosis of the Eddington Ratio**,” 2017), and turbulent, radiation-driven inflows (“**Radiation-driven turbulent accretion onto massive black holes**,” 2017). More recently, his work has expanded to galaxy-cluster contexts, using 3D radiation-hydrodynamic simulations to study how combined **radiative** and **kinetic** AGN feedback shapes cool-core intracluster media (“**The interplay of kinetic and radiative feedback in galaxy clusters**,” 2019), linking microphysical accretion regulation to macrophysical thermal balance.\n\n---\n\n## Notable Works\n- **Radiation-regulated accretion framework for IMBH growth:** Developed a foundational parametric characterization of quasi-spherical accretion regulated by radiative feedback using radiation-hydrodynamic simulations (“*Accretion onto intermediate-mass black holes regulated by radiative feedback. I. Parametric study for spherically symmetric accretion*,” 2011), establishing how heating and ionization structure control inflow and variability.\n- **Scaling relations for growth rates and duty cycles under feedback:** Quantified how radiation pressure and angular momentum modify accretion from large (galactic) scales and derived broadly applicable growth/duty-cycle scalings (“*Accretion onto black holes from large scales regulated by radiative feedback. II. Growth rate and duty cycle*,” 2012), widely used for interpreting early black hole growth constraints.\n- **Feedback-modified orbital evolution via gaseous dynamical friction:** Demonstrated, with radiative hydrodynamic simulations, that black hole radiative feedback can significantly alter the density wake and reduce/reshape dynamical friction forces (“*Gaseous dynamical friction in presence of black hole radiative feedback*,” 2017), impacting models of massive black hole pairing after mergers.\n\n---\n\n## Academic Background\nBased at the Georgia Institute of Technology, Park’s publication trajectory indicates advanced training in computational and theoretical astrophysics with a strong emphasis on radiation-hydrodynamics and black hole accretion physics. His early dissertation work—circulating as thesis-form publications on “radiation-regulated accretion onto black holes from large scales” (2012) and related materials—maps directly onto his subsequent, influential journal series (2011–2013), suggesting doctoral-level specialization in feedback-regulated accretion and numerical method development (e.g., coupling ZEUS-MP with radiative transfer). Over time, his collaborations and topic expansion—from IMBH/seed growth and instabilities near ionization fronts to dusty-gas accretion, dynamical friction, and 3D cluster-scale AGN feedback—reflect sustained affiliations within computational astrophysics networks and a research profile recognized by substantial community uptake (total citations: 649). His body of work positions him as a key contributor to the modern theoretical understanding of <u>how black holes grow under self-generated feedback</u>, bridging early-universe seeding problems with galaxy and cluster evolution."}, "D9jCEIkAAAAJ": {"keywords": "Geographic Information Science, Urban Planning, Social Networks, Information Visualization, Spatial Social Networks, Human Mobility and Trajectory Analysis, Origin-Destination Flow Analysis, Spatial Network Analysis, Points of Interest (POI) Analytics, Geospatial Privacy and Synthetic Trajectory Generation", "summary": "## Overview\nClio Andris is an Associate Professor at Georgia Tech and leads the *Friendly Cities* research group (friendlycities.gatech.edu/), where she develops computational approaches at the intersection of **Geographic Information Science (GIScience)**, **urban planning**, and **information visualization**. Her scholarship centers on <u>spatial social networks</u> and <u>human mobility</u>, using large-scale digital traces (e.g., call detail records, trajectories, and online platforms) to reveal how *social interaction*, *movement*, and *place-based infrastructure* co-produce urban and regional structure. Across this agenda, Andris emphasizes **network thinking** and *interpretable visual analytics* to translate complex spatial data into actionable insights for cities, public policy, and societal wellbeing.\n\n---\n\n## Research Areas\nAndris’s research spans several tightly connected areas of GIScience and computational social science. A defining theme is the measurement and modeling of <u>social connectivity in geographic space</u>, exemplified by her work on delineating regions from interaction networks in “Redrawing the map of Great Britain from a network of human interactions” (2010), which challenges administratively defined boundaries by deriving fine-grained regions from large-scale human transactions. She also advances the methodological foundations of <u>spatial social network analysis</u>, including conceptual and systems-level integration of social networks with GIS (“Integrating Social Networks into GISystems,” 2016) and the development of metrics that jointly account for Euclidean embedding and network topology (“Metrics for characterizing network structure and node importance in Spatial Social Networks,” 2019; “Spatial social networks in geographic information science,” 2021). A second major thread is <u>urban mobility analytics</u>, where she analyzes trajectory and origin–destination data to uncover behavioral regularities and operational strategies in transportation systems, including taxi-driver decision-making and mobility “intelligence” (“Uncovering cabdrivers’ behavior patterns from their digital traces,” 2010; “Revealing taxi driver's mobility intelligence through his trace,” 2010) and broader pattern discovery in OD matrices (“Discovering spatial patterns in origin‐destination mobility data,” 2012). A third area connects <u>cities, governance, and polarization</u>, applying network methods to political collaboration and migration flows—e.g., quantifying cross-party cooperation and identifying “super-cooperators” in “The rise of partisanship and super-cooperators in the US House of Representatives” (2015), and linking migration networks to partisan sorting (“Migration and political polarization in the US,” 2019). More recently, she has expanded into <u>place data and urban digital infrastructure</u> (e.g., POI quality and use in “Points of Interest (POI): A commentary…,” 2022), <u>geoprivacy</u> (e.g., synthetic trajectory generation in “trajGANs,” 2018), and <u>public-facing crisis visualization</u> and risk communication during COVID-19 (“Real-time, interactive website for US-county-level COVID-19 event risk assessment,” 2020; “Visualization design practices in a crisis…,” 2022).\n\n---\n\n## Notable Works\n- **Interaction-based regionalization and relational geographies:** “**Redrawing the map of Great Britain from a network of human interactions**” (2010) introduced a fine-grained framework for deriving regions from large-scale interaction networks, demonstrating how *everyday connectivity* can yield alternative, empirically grounded boundary systems.\n- **Computational measurement of political collaboration and polarization:** “**The rise of partisanship and super-cooperators in the US House of Representatives**” (2015) provided network-based evidence on legislative collaboration patterns, identifying cross-party “super-cooperators” and quantifying structural shifts in partisanship.\n- **Urban mobility behavior from digital traces:** “**Uncovering cabdrivers’ behavior patterns from their digital traces**” (2010) developed a methodology to infer high-level operational behaviors from continuous trajectory traces, helping establish data-driven approaches for understanding decision-making in urban transportation systems.\n\n---\n\n## Academic Background\nAs an Associate Professor at the Georgia Institute of Technology, Andris is affiliated with an interdisciplinary research environment that bridges **computing**, **design/visualization**, and **urban and policy** scholarship—an orientation reflected in her sustained publication record across GIScience, network science, and urban analytics. Her early, highly cited contributions (2010–2012) on interaction networks and mobility traces indicate formative training in computational methods for spatial data and complex networks, later extending into integrated GISystems perspectives (“Integrating Social Networks into GISystems,” 2016) and field-shaping syntheses on spatial social networks (2021–2022). Her work’s impact—evidenced by approximately **2,709 citations**—also suggests strong standing within the GIScience and computational urbanism communities, alongside ongoing collaborations spanning transportation, political science, public health communication, and place-based data quality. The trajectory of her publications further reflects a pattern of leadership in *translational research*, including public-facing tools and visualization studies during the COVID-19 pandemic, consistent with Georgia Tech’s emphasis on societally engaged, interdisciplinary scholarship."}, "DDDRx9EAAAAJ": {"keywords": "Startup engineering, Automated planning, Planning graphs, Graphplan, Computational complexity, Circuit complexity, Polynomial-time hierarchy, Computational learning theory, Cryptographic primitives, Permutation group algorithms", "summary": "## Overview\nMerrick Furst is a **Distinguished Professor** at **Georgia Tech**, where he has led influential work spanning theoretical computer science and applied computing, while also championing *innovation-driven education* and **startup engineering**. His scholarship is anchored in the design and analysis of efficient algorithms and complexity-theoretic limits, with enduring impact on <u>automated planning</u>, <u>circuit complexity</u>, and <u>computational learning theory</u>. Across decades of highly cited contributions (total citations: **7934**), Furst has combined **foundational theory** with *systems and curricular initiatives* that translate computing research into real-world practice.\n\n---\n\n## Research Areas\nFurst’s research portfolio is notably broad but thematically unified by an emphasis on <u>computational efficiency</u>—what can be computed, learned, or planned within realistic resource bounds. In **AI planning**, his co-authored work on planning graph methods culminated in the widely adopted Graphplan paradigm, formalized in “*Fast planning through planning graph analysis*” (1997), which introduced compact planning graphs and systematic mutex reasoning to derive shortest partial-order plans or certify infeasibility. In **computational complexity**, his early and highly influential paper “*Parity, circuits, and the polynomial-time hierarchy*” (1984) established strong lower bounds for constant-depth circuits computing parity and developed reduction notions that connected circuit lower bounds to the structure of the polynomial-time hierarchy, with extensions to majority, multiplication, and transitive closure. In **learning theory and cryptography**, “*Cryptographic primitives based on hard learning problems*” (1993) linked intractability in Valiant-style learning models to the construction of cryptographic primitives, helping crystallize the interplay between learnability, hardness assumptions, and security. Complementing these core areas are contributions to **algorithmic group theory** (e.g., “*Polynomial-time algorithms for permutation groups*,” 1980), **Fourier-analytic learning** (e.g., “*Weakly learning DNF and characterizing statistical query learning using Fourier analysis*,” 1994), and applied systems/innovation work such as “*Parallel web sites*” (2001) and entrepreneurship-oriented educational initiatives (e.g., “*Threads™*,” 2007; “*The InVenture Prize*,” 2010).\n\n---\n\n## Notable Works\n- **Planning graphs and Graphplan for efficient automated planning:** In “*Fast planning through planning graph analysis*” (1997), Furst and collaborators introduced the <u>planning graph</u> representation and the Graphplan algorithm, enabling fast plan extraction with mutex-based constraint propagation and guarantees of shortest partial-order plans when solutions exist.  \n- **Circuit lower bounds shaping modern complexity theory:** “*Parity, circuits, and the polynomial-time hierarchy*” (1984) provided super-polynomial size lower bounds for fixed-depth circuits computing parity and developed constant-depth reduction techniques that connected lower bounds for multiple functions to broader questions about the <u>polynomial-time hierarchy</u>.  \n- **Hardness of learning as a basis for cryptography:** In “*Cryptographic primitives based on hard learning problems*” (1993), Furst helped formalize how <u>hard learning problems</u> can yield cryptographic constructions, strengthening the conceptual bridge between computational learning theory and modern cryptography.\n\n---\n\n## Academic Background\nAcross a publication record beginning in the late 1970s and extending through influential work in the 2000s and 2010s, Furst’s career reflects sustained leadership in both **theory** and **institutional innovation**. His early contributions to algorithms and discrete mathematics—such as polynomial-time methods for **permutation groups** (“*Polynomial-time algorithms for permutation groups*,” 1980) and complexity-focused studies of computation and communication (“*Multi-party protocols*,” 1983)—situate him within the core traditions of theoretical computer science. The subsequent arc of his scholarship includes landmark results in **circuit complexity** (1984), **pseudorandomness/space complexity** (mid-1980s), **learning theory** (early-to-mid 1990s), and the highly cited planning breakthrough of Graphplan (1997), indicating strong cross-field influence and long-term citation impact. As a senior academic at **Georgia Tech**, his affiliation is also marked by visible engagement in <u>computing education reform</u> and *entrepreneurial ecosystems*, including curriculum restructuring proposals (“*Threads™*,” 2007) and invention-to-venture programming (“*The InVenture Prize*,” 2010), aligning naturally with his stated keyword focus on **startup engineering** and the translation of computing expertise into scalable innovation."}, "DKiiFh4AAAAJ": {"keywords": "Machine Learning, Multiagent Systems, Computer Vision, Robotics, Agent-Based Modeling, Collective Animal Behavior, Social Network Inference, RFID and Position Tracking, Probabilistic Graphical Models, Swarm Robotics and Foraging Coordination", "summary": "## Overview\nBrian Hrolenok is an Assistant Professor (teaching) in the Department of Computer Science at George Mason University, where he contributes to curriculum and instruction while advancing research at the intersection of **Machine Learning**, **Multiagent Systems**, and *data-driven modeling* for biological and robotic domains. His scholarship centers on <u>learning and validating executable models of collective behavior</u> from sensor and tracking data, with recurring emphasis on **agent-based simulation**, **probabilistic inference**, and *coordination mechanisms* that enable groups—whether robots or animals—to exhibit robust, interpretable collective dynamics.\n\n---\n\n## Research Areas\nHrolenok’s research spans several tightly connected areas. In **multiagent coordination** and *collective robotics*, his early work proposes a practical alternative to pheromone-like communication by using sparse, movable markers in “**Collaborative foraging using beacons**” (2010), addressing how agents can coordinate in shared environments under limited communication. In **learning models of collective animal behavior**, he develops methods for constructing *executable* agent-based models directly from observation, notably in “**Learning Executable Models of Multiagent Behavior from Live Animal Observation**” (2013) and related work on evaluating learned schooling dynamics (e.g., “Assessing learned models of fish schooling behavior,” 2014; “Constructing and Evaluating Executable Models of Collective Behavior,” 2018). He also advances **probabilistic modeling of stochastic multiagent behavior**, arguing for sampling-based approaches to better reproduce variability in “**Sampling beats fixed estimate predictors for cloning stochastic behavior in multiagent systems**” (2017). Complementing these themes, he applies **computer vision and sensing-driven inference** to behavioral ecology and animal management, using tracking data (including RFID and continuous 3D tracking) to infer <u>social structure and dominance relationships</u> in rhesus macaques (2014; 2018; 2022), demonstrating how automated observation can replace or augment costly manual ethological annotation.\n\n---\n\n## Notable Works\n- **“Collaborative foraging using beacons” (2010)** — Introduced a sparse, physical proxy for pheromone communication via movable beacons, providing a pragmatic framework for coordination in multiagent foraging when direct communication is constrained.  \n- **“Learning Executable Models of Multiagent Behavior from Live Animal Observation” (2013)** — Advanced the agenda of deriving *executable* agent-based simulations from observation data, bridging predictive graphical models and simulation-ready behavioral rules for collective animal behavior.  \n- **“Sampling beats fixed estimate predictors for cloning stochastic behavior in multiagent systems” (2017)** — Demonstrated that sampling-based prediction better captures stochasticity in collective systems (e.g., schooling), reframing fixed-estimate predictors within a probabilistic perspective and emphasizing faithful reproduction of variability.\n\n---\n\n## Academic Background\nBased on his publication trajectory and affiliations, Hrolenok’s academic development reflects sustained engagement with **multiagent simulation** and *computational modeling* communities, including contributions related to the MASON simulation ecosystem (e.g., “Future MASON Directions: Community Recommendations,” 2013), suggesting active participation in community-driven software and methodology efforts. His research record shows a progression from foundational multiagent coordination (2010) to data-driven, sensor-enabled inference and model learning for biological collectives (2013–2022), indicating interdisciplinary collaboration with researchers in ethology, animal behavior, and applied sensing (e.g., RFID-based and continuous 3D tracking studies of rhesus macaques in 2014, 2018, and 2022). As a teaching-focused faculty member at George Mason University, he integrates these themes—**machine learning**, **robotics**, **computer vision**, and **multiagent systems**—into academic training while maintaining an applied research agenda evidenced by a focused set of publications and an established citation footprint (66 total citations)."}, "E2Idm8UAAAAJ": {"keywords": "Design, Product, Education, Engineering, Human-Computer Interaction (HCI), Disability Studies, Accessible Computing, Assistive Technology, Rehabilitation Technology, Peer Review in Engineering Design Education", "summary": "## Overview\nMahender Mandala is a researcher at the **Georgia Institute of Technology**, working at the intersection of **engineering design**, **inclusive computing**, and *human-centered* technology development. His scholarship centers on <u>accessibility and disability-oriented innovation</u>, spanning both the sociotechnical foundations of **HCI** and the practical design of educational and rehabilitative technologies. Across his publications, Mandala advances *equitable participation* in STEM and technology use by foregrounding the lived realities of disability, integrating <u>design education</u> with **assistive technology** and evidence-driven evaluation.\n\n---\n\n## Research Areas\nMandala’s research portfolio integrates three mutually reinforcing areas. First, he contributes to critical and disability-centered **HCI** through conceptual work that frames the field as “sociotechnical articulation,” most prominently in “**Articulations toward a crip HCI**” (2021), which develops <u>crip theory–informed</u> perspectives on how technologies, norms, and infrastructures shape participation and agency. Second, he investigates **accessible computing education**, including K–12 pathways for learners with disabilities—e.g., “**Accessible computer science for k-12 students with hearing impairments**” (2020) and “**Accessible block-based programming for k-12 students who are blind or low vision**” (2021)—addressing barriers in mainstream instructional tools (such as block-based programming environments) and proposing design principles for inclusive learning experiences. Third, he engages applied health and rehabilitation technology research, examining adoption and outcomes in clinical and community contexts; “**Perceptions of stroke survivors regarding factors affecting adoption of technology and exergames for rehabilitation**” (2023) analyzes determinants of uptake for VR/AR-enabled exergames and task-specific training, while related work on mobility measurement (“**Lower AM-PAC 6-Clicks basic mobility score predicts discharge…**,” 2022) reflects an interest in <u>evidence-based decision support</u> and patient-centered outcomes. Complementing these themes, Mandala has also studied scalable feedback mechanisms in engineering design education—e.g., collaborative peer review effects on feedback quality (2018)—linking instructional design to the development of robust design competencies.\n\n---\n\n## Notable Works\n- **“Articulations toward a crip HCI” (2021)** — A highly cited conceptual contribution that reframes HCI through <u>crip-informed</u> sociotechnical analysis, emphasizing how “articulation work” and infrastructural arrangements shape disability, access, and participation.\n- **“Perceptions of stroke survivors regarding factors affecting adoption of technology and exergames for rehabilitation” (2023)** — An empirical study detailing user-identified facilitators and barriers to rehabilitation technology adoption, with specific attention to exergames and VR/AR-based interfaces for motor training and repetitive practice.\n- **“Impact of collaborative team peer review on the quality of feedback in engineering design projects” (2018)** — A design-education study demonstrating how **collaborative** peer review structures influence the quality and characteristics of feedback in engineering design courses, addressing the instructional challenges of scaling rich critique in large classes.\n\n---\n\n## Academic Background\nMandala’s publication trajectory suggests an academic formation grounded in **engineering design** and **rehabilitation/assistive technology**, with sustained affiliations in interdisciplinary environments that bridge computing, education, and health. Early work (2011–2013) on dynamic seating systems and pressure-relief evaluation (e.g., studies of inflation/deflation patterns and interface pressures) indicates training in applied biomechanics or rehabilitation engineering methods, followed by programmatic contributions to design education across the academic pipeline—most explicitly in “**An innovative approach to design education… Technology Innovations for People with Disabilities**” (2013). His later scholarship expands into scalable pedagogies for engineering design critique (2016–2018) and into inclusive K–12 computing interventions (2020–2021), reflecting a growing emphasis on <u>accessibility</u> and broadening participation. His involvement in service-learning and research experiences for veterans and teachers (2014) further indicates sustained engagement with workforce development and community-facing STEM programs. With **139 citations** and a set of cross-domain collaborations spanning HCI, education research, and rehabilitation-focused studies, Mandala’s academic profile is characterized by *interdisciplinary* practice and a consistent commitment to designing technologies and learning environments that better serve people with disabilities."}, "E6h84HUAAAAJ": {"keywords": "Cardiac dynamics, Complex systems, Nonlinear dynamics, Mathematical biology, Cardiac electrophysiology modeling, Spiral and scroll wave dynamics, Cardiac arrhythmias and fibrillation, Alternans and restitution dynamics, Low-energy defibrillation and control of electrical turbulence, Machine learning for chaotic time-series prediction (reservoir computing)", "summary": "## Overview\nElizabeth M. Cherry is a researcher at the **Georgia Institute of Technology** and a leading contributor to computational cardiology through her work with the *Virtual Heart* effort (thevirtualheart.org). Her scholarship centers on **cardiac dynamics** viewed through the lens of **complex systems** and **nonlinear dynamics**, with a sustained emphasis on mechanistic, multiscale modeling of electrical activity in the heart. Across studies of arrhythmogenesis, wave propagation, and control strategies, Cherry’s work advances <u>mathematical models of cardiac electrophysiology</u> that connect cellular ion-channel behavior to tissue- and organ-level phenomena, aiming to clarify how *spatiotemporal complexity* emerges in health and disease.\n\n---\n\n## Research Areas\nCherry’s research spans foundational and applied problems in **mathematical biology** and cardiac electrophysiology, particularly the dynamical mechanisms that generate and sustain arrhythmias. A central theme is the study of <u>reentry and spiral/scroll-wave dynamics</u>, including the identification of distinct routes to fibrillation-like states in “**Multiple mechanisms of spiral wave breakup in a model of cardiac electrical activity**” (2002), which helped formalize how heterogeneous instabilities and nonlinear wave interactions can fragment organized reentrant activity. She has also developed and analyzed reduced yet tissue-realistic cellular descriptions, exemplified by the “**Minimal model for human ventricular action potentials in tissue**” (2008), designed to reproduce key propagation and restitution properties across ventricular cell types. Complementing mechanistic modeling, Cherry has contributed to the community’s understanding of model credibility and interoperability via reviews and benchmarks—most notably “**Models of cardiac tissue electrophysiology: progress, challenges and open questions**” (2011) and “**Verification of cardiac tissue electrophysiology simulators using an N-version benchmark**” (2011)—which situate tissue models within the broader <u>Cardiac Physiome</u> agenda and articulate open problems in multiscale integration, numerical reliability, and reproducibility. Her portfolio also extends to intervention and prediction: low-energy defibrillation concepts (“**Low-energy control of electrical turbulence in the heart**,” 2011) and data-driven forecasting of nonlinear time series using recurrent networks and reservoir computing (“**Prediction of chaotic time series using recurrent neural networks and reservoir computing techniques**,” 2022; related arrhythmic prediction studies in 2021–2022), reflecting an interest in bridging biophysical modeling with modern machine-learning approaches for complex dynamical signals.\n\n---\n\n## Notable Works\n- **Conceptual synthesis and agenda-setting for tissue electrophysiology modeling:** “**Models of cardiac tissue electrophysiology: progress, challenges and open questions**” (2011) articulated the state of the field within the <u>Cardiac Physiome Project</u>, highlighting multiscale challenges and establishing a roadmap for model development, validation, and application.\n- **Mechanistic explanation of arrhythmogenic wave fragmentation:** “**Multiple mechanisms of spiral wave breakup in a model of cardiac electrical activity**” (2002) provided a dynamical-systems account of how reentrant waves destabilize and transition toward fibrillation-like electrical turbulence.\n- **Reduced human-ventricular modeling for tissue-level realism:** “**Minimal model for human ventricular action potentials in tissue**” (2008) introduced a compact yet physiologically grounded framework to study wave propagation and stability across ventricular phenotypes, supporting computational investigations of reentry and arrhythmia susceptibility.\n\n---\n\n## Academic Background\nBased at the **Georgia Institute of Technology**, Cherry has built an influential career at the intersection of applied mathematics, nonlinear dynamics, and cardiac electrophysiology, evidenced by a sustained, highly cited publication record (over **7,000 citations**) spanning early methodological advances in efficient simulation of excitable media (e.g., space–time adaptive approaches in 2000–2003) through widely used modeling and review contributions in the 2000s and 2010s. Her repeated engagement with international modeling initiatives and standards—particularly through work aligned with the *Cardiac Physiome* vision (e.g., 2011 reviews on cardiac cell and tissue modeling, and verification benchmarks for simulators)—suggests a strong affiliation with interdisciplinary consortia that connect engineering, physiology, and computation. The trajectory of her publications, from mechanistic studies of alternans, restitution, and spiral/scroll waves to translationally motivated efforts in low-energy arrhythmia control, real-time/GPU-enabled simulation, and more recent machine-learning forecasting of chaotic cardiac signals, reflects a research program shaped by both theoretical rigor and practical relevance, with contributions that have become touchstones for communities in **computational cardiology** and **complex systems**."}, "EMYV8cEAAAAJ": {"keywords": "Ubiquitous Computing, Battery-Free Systems, Wearables, Intermittent Computing, Energy Harvesting, Batteryless Internet of Things (IoT), Intermittent Operating Systems, Non-Volatile Memory (NVM) Systems, Ultra-Low-Power Machine Learning (TinyML), Mobile Health (mHealth) Sensing", "summary": "## Overview\nJosiah Hester is an Associate Professor in **Interactive Computing** and **Computer Science** at Georgia Tech, where he leads research at the intersection of *ubiquitous computing* and ultra-low-power embedded systems. His work centers on enabling <u>battery-free</u>, <u>intermittently powered</u> devices—tiny computers that harvest energy from their environments and must compute reliably despite frequent power loss. Across systems, tools, and applications, Hester’s research advances **intermittent computing**, **energy-harvesting architectures**, and *wearable platforms* that push sensing and inference closer to the real world while reducing maintenance, cost, and environmental impact.\n\n---\n\n## Research Areas\nHester’s research spans the full stack of batteryless and energy-constrained computing, from foundational runtime/OS support to end-user applications in health and sustainability. A core theme is making <u>intermittent execution</u> practical and predictable: his work on timely progress under power failures (“**Timely execution on intermittently powered batteryless sensors**,” 2017) and OS support for reactive, failure-resilient sensing (“**InK: Reactive Kernel for Tiny Batteryless Sensors**,” 2018) addresses the fundamental tension between volatile energy supply and the need for correct, responsive behavior. Complementing these systems contributions are toolchains and prototyping frameworks that broaden who can build such devices and how quickly they can be evaluated, including rapid development workflows (“**Flicker: Rapid Prototyping for the Batteryless Internet-of-Things**,” 2017) and experimentation infrastructure to make energy-harvesting behavior measurable and repeatable (“**Ekho: Realistic and repeatable experimentation for tiny energy-harvesting sensors**,” 2014; “**Realistic Simulation for Tiny Batteryless Sensors**,” 2016). He also investigates time as a first-class systems resource in battery-free sensing, developing approaches to <u>reliable timekeeping</u> and clocks that survive intermittent power (“**Persistent Clocks for Batteryless Sensing Devices**,” 2016; “**Reliable timekeeping for intermittent computing**,” 2020). Beyond core infrastructure, Hester’s group demonstrates how these ideas translate into compelling devices and societal domains: battery-free interactive systems (“**Battery-free game boy**,” 2020), health-oriented wearables for behavior sensing (“**Necksense**,” 2020; “**FaceBit**,” 2021), and broader visions for sustainable IoT (“**The Future of Sensing is Batteryless, Intermittent, and Awesome**,” 2017; “**The internet of batteryless things**,” 2024). More recently, his work also connects energy-constrained embedded intelligence to practical deployment via low-power ML workflows (“**hls4ml**,” 2021) and explores sustainability-facing hardware and networks (e.g., “**Greentooth**,” 2024; “**Soil-powered computing**,” 2024), reflecting a growing emphasis on computing that is not only efficient, but environmentally accountable.\n\n---\n\n## Notable Works\n- **Batteryless and intermittent sensing as a computing paradigm:** Hester helped crystallize the research agenda for <u>battery-free</u> ubiquitous computing through the influential vision paper “**The Future of Sensing is Batteryless, Intermittent, and Awesome**” (2017), articulating why batteries impede “smart dust”-scale deployments and motivating systems that embrace intermittent power as a design constraint rather than an exception.\n- **Systems support for correctness and responsiveness under power loss:** In “**Timely execution on intermittently powered batteryless sensors**” (2017) and “**InK: Reactive Kernel for Tiny Batteryless Sensors**” (2018), he advanced runtime and OS mechanisms that enable dependable progress and reactive behavior despite frequent outages—work that underpins practical, long-lived sensing in unpredictable energy environments.\n- **Democratizing ultra-low-power intelligence and prototyping:** Through “**Flicker: Rapid Prototyping for the Batteryless Internet-of-Things**” (2017) and the low-power ML workflow “**hls4ml**” (2021), Hester contributed widely used approaches that reduce the barrier to building energy-harvesting devices and deploying efficient near-sensor inference, bridging embedded systems practice with emerging low-power machine learning needs.\n\n---\n\n## Academic Background\nAs a faculty member in Georgia Tech’s School of Interactive Computing and School of Computer Science, Hester has built an academic profile grounded in **ubiquitous computing**, **embedded/low-power systems**, and **wearable sensing**, with research impact reflected in a citation record exceeding **3,300**. His publication trajectory shows sustained leadership in intermittent and battery-free computing—from early work on energy-harvesting experimentation and evaluation (“**Ekho**,” 2014) and energy storage architectures (“**Tragedy of the coulombs**,” 2015) to mature OS/runtime contributions (“**InK**,” 2018) and expanding application domains in health wearables and sustainable infrastructures (“**Necksense**,” 2020; “**The internet of batteryless things**,” 2024). The breadth of venues implied by his portfolio—spanning systems, ubiquitous computing, and human-centered wearables—suggests active engagement with major HCI/ubiquitous computing and embedded systems research communities, alongside interdisciplinary collaborations in digital health and sustainability. Collectively, this record positions Hester as a leading scholar advancing <u>maintenance-free computing</u> and *energy-harvesting devices* that can operate for years to decades without batteries, enabling scalable and environmentally responsible sensing and interaction."}, "EWU0STsAAAAJ": {"keywords": "Computer Architecture, Memory Systems, Hardware Security, Quantum Computing, Phase-Change Memory (PCM), Cache Replacement and Partitioning, DRAM Reliability and Refresh, Rowhammer Mitigation, Secure Memory and Integrity Trees, NISQ Quantum Error Mitigation and QEC Decoding", "summary": "## Overview\nMoinuddin Qureshi is a Professor at the Georgia Institute of Technology, where he leads a research program at the intersection of **computer architecture** and *systems reliability and security*. His work centers on designing **high-performance memory hierarchies** and **robust shared resources** for modern processors, with particular emphasis on <u>memory systems</u>, <u>hardware security</u>, and emerging paradigms such as *NISQ-era* <u>quantum computing</u>. Across a body of highly cited contributions, Qureshi is known for translating deep architectural insights into practical mechanisms that improve performance, energy efficiency, lifetime, and trustworthiness of computing platforms.\n\n---\n\n## Research Areas\nQureshi’s research spans several tightly connected areas in architecture. In **memory systems**, he has been a leading contributor to making emerging non-volatile memories practical, including phase-change memory (PCM) designs that address cost and power limits of DRAM—exemplified by “Scalable high performance main memory system using phase-change memory technology” (2009) and follow-on work on endurance and performance such as “Enhancing lifetime and security of PCM-based main memory with start-gap wear leveling” (2009) and latency-management techniques like “Improving read performance of phase change memories via write cancellation and write pausing” (2010). In **cache and shared-resource management**, his papers introduced runtime policies that improve multicore performance under contention and thrashing, including “Utility-based cache partitioning” (2006), “Adaptive insertion policies for high performance caching” (2007), and MLP-sensitive designs such as “A case for MLP-aware cache replacement” (2006). In **hardware security and reliability**, he has advanced defenses against microarchitectural and memory-based attacks, notably conflict-based cache attacks (“CEASER: Mitigating conflict-based cache attacks via encrypted-address and remapping,” 2018; and “{MIRAGE},” 2021) and DRAM vulnerability mitigation (“Architectural support for mitigating row hammering in DRAM memories,” 2014; “Hydra,” 2022). More recently, his group has contributed to **quantum computing systems**, developing variability-aware scheduling and reliability techniques for near-term machines—e.g., “Not all qubits are created equal” (2019)—as well as work toward scalable fault tolerance such as “Afs: Accurate, fast, and scalable error-decoding for fault-tolerant quantum computers” (2022).\n\n---\n\n## Notable Works\n- **Utility-driven shared-cache management for multicore systems**: Introduced a practical, runtime approach to partition shared caches with low overhead and strong performance isolation in “Utility-based cache partitioning: A low-overhead, high-performance, runtime mechanism to partition shared caches” (2006), helping define a policy framework for coordinated cache allocation under multiprogramming.\n- **Architectural foundations for PCM-based main memory**: Helped establish PCM as a scalable main-memory alternative by addressing system-level cost/power constraints in “Scalable high performance main memory system using phase-change memory technology” (2009), complemented by endurance and robustness mechanisms in “Enhancing lifetime and security of PCM-based main memory with start-gap wear leveling” (2009).\n- **Variability-aware optimization for NISQ quantum computers**: Advanced the systems view of near-term quantum computing by arguing for policies that explicitly account for heterogeneous qubit quality in “Not all qubits are created equal: A case for variability-aware policies for NISQ-era quantum computers” (2019), influencing how compilers/schedulers and runtimes map programs onto imperfect hardware.\n\n---\n\n## Academic Background\nAs a long-standing faculty member at Georgia Tech, Qureshi has built an internationally visible research portfolio in **computer architecture**, reflected in high-impact publications across major architecture and systems venues and a sustained focus on mechanisms that are both analytically grounded and implementable. His early, widely adopted work on <u>cache management</u> and multicore resource allocation (e.g., utility-based partitioning and adaptive insertion policies) established a foundation for later contributions in <u>emerging memories</u>—particularly PCM—where he addressed endurance, latency, and security constraints that shape deployability. In parallel, his record shows a consistent engagement with industry-relevant challenges in <u>hardware security</u> (e.g., cache side channels and RowHammer) and, more recently, with the architectural co-design needs of <u>quantum computing</u> from NISQ reliability to fault-tolerant decoding. With **17,518 citations**, his work evidences broad adoption and strong scholarly influence, and his collaborations across memory, security, and quantum systems suggest active affiliation with cross-disciplinary communities spanning architecture, dependable computing, and secure systems."}, "EYo_WkEAAAAJ": {"keywords": "Robotics, Artificial Intelligence, Learning from Demonstration (LfD), Imitation Learning, Interactive Robot Learning, Reinforcement Learning from Demonstrations, Human-Robot Interaction (HRI), Sim2Real Transfer, Embodied AI (Navigation and Rearrangement), Explainable AI (XAI) for Robotics", "summary": "## Overview\nSonia Chernova is an Associate Professor at the Georgia Institute of Technology, where she leads a research program at the intersection of **Robotics** and **Artificial Intelligence**, with a primary emphasis on *human-centered robot learning*. Her work advances <u>Learning from Demonstration (LfD)</u> and interactive learning systems that enable robots to acquire skills from people, while also addressing the practical realities of deploying embodied agents outside the lab—spanning <u>sim-to-real transfer</u>, <u>embodied AI evaluation</u>, and *explainability* in human–robot interaction. Across highly cited contributions, Chernova has helped shape how the field conceptualizes the design space of demonstration-driven learning and how robots can collaborate with humans through adaptive autonomy and transparent communication.\n\n---\n\n## Research Areas\nChernova’s research centers on enabling robots to learn robustly from people and to operate effectively in real environments. A core thread is <u>robot learning from demonstration</u>, including taxonomies and design choices for LfD pipelines—covering demonstrator characteristics, problem representations, policy derivation, and evaluation—synthesized in her widely cited survey work (“A survey of robot learning from demonstration,” 2009) and later updates (“Recent Advances in Robot Learning from Demonstration,” 2020; “Robot learning from human teachers,” 2022). She has also developed interactive learning methods that tightly couple human guidance with robot autonomy, notably **Confidence-Based Autonomy** for interactive policy learning (“Interactive policy learning through confidence-based autonomy,” 2009) and probabilistic policy representations that manage uncertainty in demonstrations (“Confidence-based policy learning from demonstration using gaussian mixture models,” 2007). Another major area integrates LfD with reinforcement learning to reduce sample complexity and improve learning efficiency, including reward shaping and leveraging demonstrations of varying quality (“Reinforcement Learning from Demonstration through Shaping,” 2015; “Integrating reinforcement learning with human demonstrations of varying ability,” 2011; “Learning from demonstration for shaping through inverse reinforcement learning,” 2016). Complementing learning methods, her embodied AI research investigates whether simulation benchmarks predict real-world outcomes and how to measure—and reduce—the <u>sim2real gap</u> (“Sim2real predictivity,” 2020; “Are we making real progress in simulated environments?,” 2019; “Bi-directional domain adaptation for sim2real transfer,” 2021; “Rethinking sim2real,” 2023). More recent work extends into <u>semantic manipulation and rearrangement</u> (“Rearrangement: A challenge for embodied ai,” 2020; “StructDiffusion,” 2022) and <u>explainable AI</u> for improving user assistance, fault recovery, and understanding in human-facing systems (“Explainable ai for robot failures,” 2021; “State2explanation,” 2023), reflecting a sustained focus on *deployable* and *human-compatible* robot intelligence.\n\n---\n\n## Notable Works\n- **Foundational synthesis of Learning from Demonstration (LfD):** Chernova’s “**A survey of robot learning from demonstration**” (2009) systematized the LfD field by articulating key design dimensions—demonstrator type, problem space, policy derivation, and performance assessment—providing a shared conceptual framework that has guided subsequent algorithmic and empirical work.\n- **Interactive autonomy for data-efficient policy learning:** In “**Interactive policy learning through confidence-based autonomy**” (2009), she introduced Confidence-Based Autonomy (CBA), demonstrating how robots can *actively* request demonstrations when uncertain and execute learned behavior when confident—an influential paradigm for mixed-initiative teaching and safe, incremental skill acquisition.\n- **Rigor in embodied AI evaluation and sim-to-real transfer:** With “**Sim2real predictivity: Does evaluation in simulation predict real-world performance?**” (2020), she helped establish methodology for interrogating whether simulation-based progress reliably transfers to physical robots, motivating more principled evaluation protocols and follow-on work on measuring and reducing the sim2real gap.\n\n---\n\n## Academic Background\nAs an Associate Professor at Georgia Tech with a research portfolio exceeding **13,000 citations**, Chernova is widely recognized as a leading scholar in robot learning and human–robot interaction. Her publication trajectory indicates early contributions to legged robotics and optimization (e.g., “An evolutionary approach to gait learning for four-legged robots,” 2004) followed by sustained, field-defining work in <u>Learning from Demonstration</u> and interactive learning (2007–2012), including probabilistic policy representations and human-in-the-loop autonomy. Her later work reflects expanding leadership across embodied AI and deployment-oriented research, including community-shaping benchmark and evaluation efforts in sim2real and rearrangement tasks (2019–2023), as well as interdisciplinary engagement with **Explainable AI** and smart environments (“Explainable activity recognition for smart home systems,” 2023; “Bootstrapping human activity recognition systems for smart homes from scratch,” 2022). The breadth of venues and long-running themes—interactive teaching, uncertainty-aware learning, and evaluation grounded in real-world constraints—suggest deep collaborations across robotics, machine learning, and HRI communities, consistent with the research and educational mission of Georgia Tech’s computing and robotics ecosystem."}, "EtoOcLMAAAAJ": {"keywords": "Applied Mathematics, Plasma Physics, High Performance Computing, Scientific Machine Learning, Magnetohydrodynamics (MHD), Weighted Essentially Non-Oscillatory (WENO) Schemes, Positivity-Preserving High-Order Methods, Grad-Shafranov Equilibrium Solvers, Structure-Preserving Neural Networks (Symplectic/Physics-Informed), Stiff Dynamical Systems and Neural ODEs", "summary": "## Overview\nQi Tang is a researcher in **Computational Science and Engineering** at the Georgia Institute of Technology, working at the intersection of **applied mathematics**, **plasma physics**, and **high-performance computing**. Their scholarship centers on designing *reliable and scalable* computational tools for multiscale physical systems, with particular emphasis on <u>structure-preserving numerical methods</u> and <u>scientific machine learning</u> for challenging dynamical regimes such as shocks, chaos, and stiffness. Across a body of work totaling **820 citations**, Tang has contributed both to foundational discretization techniques for magnetized plasma models and to modern learning-based surrogates that embed physical constraints for long-time prediction.\n\n---\n\n## Research Areas\nTang’s research spans high-order methods for hyperbolic and magnetofluid systems, stability theory for coupled multiphysics algorithms, and physics-aware learning of dynamical systems. In computational magnetohydrodynamics, they developed high-order **finite-difference WENO** formulations with <u>constrained transport</u> to maintain divergence-free magnetic fields, as demonstrated in “**Finite difference weighted essentially non-oscillatory schemes with constrained transport for ideal magnetohydrodynamics**” (2014) and its extensions to *positivity preservation* (“**Positivity-preserving finite difference Weighted ENO schemes with constrained transport for ideal magnetohydrodynamic equations**,” 2015) and alternative flux formulations on curvilinear grids (“**A high-order finite difference WENO scheme for ideal magnetohydrodynamics on curvilinear meshes**,” 2018). Complementing these, Tang advanced *maximum-principle-preserving* and positivity-preserving flux-limiting strategies on complex geometries (“**High order parametrized maximum-principle-preserving and positivity-preserving WENO schemes on unstructured meshes**,” 2015) and designed efficient single-stage, single-step high-order methods for MHD and compressible flow (“**A high-order positivity-preserving single-stage single-step method for the ideal magnetohydrodynamic equations**,” 2016; “**An explicit high-order single-stage single-step positivity-preserving finite difference WENO method for the compressible Euler equations**,” 2016). In parallel, Tang has pursued scalable solvers and discretizations for elliptic and equilibrium problems central to fusion, including adaptive DPG methods for the <u>Grad–Shafranov equation</u> (“**An adaptive discontinuous Petrov-Galerkin method for the Grad-Shafranov equation**,” 2020) and parallel algorithms for free-boundary equilibria (“**A parallel cut-cell algorithm for the free-boundary Grad-Shafranov problem**,” 2021). More recently, Tang’s scientific ML contributions emphasize <u>long-time stable learning</u> and <u>structure preservation</u>: “**Stabilized neural ordinary differential equations for long-time forecasting of dynamical systems**” (2023) addresses high-wavenumber fidelity and robustness for shocks/chaos, while “**Fast neural Poincaré maps for toroidal magnetic fields**” (2020) targets rapid surrogate construction for magnetic field-line tracing. This trajectory extends to symplectic and Hamiltonian learning (“**Approximation of nearly-periodic symplectic maps via structure-preserving neural networks**,” 2023; “**Symplectic neural surrogate models for beam dynamics**,” 2024) and to ML-accelerated modeling of stiff plasma chemistry and kinetics (“**Latent space dynamics learning for stiff collisional-radiative models**,” 2024).\n\n---\n\n## Notable Works\n- **Long-time stable scientific ML for dynamical systems:** In “*Stabilized neural ordinary differential equations for long-time forecasting of dynamical systems*” (2023), Tang introduced stabilization strategies for neural ODE modeling that explicitly confront *high-wavenumber errors* and degradation in the presence of shocks or chaotic dynamics, enabling more reliable long-horizon forecasts.  \n- **High-order, divergence-preserving MHD discretizations:** Through “*Finite difference weighted essentially non-oscillatory schemes with constrained transport for ideal magnetohydrodynamics*” (2014) and “*Positivity-preserving finite difference Weighted ENO schemes with constrained transport for ideal magnetohydrodynamic equations*” (2015), Tang helped establish robust WENO–CT frameworks that jointly address <u>shock capturing</u>, <u>∇·B control</u>, and *physical admissibility* (positivity) in ideal MHD.  \n- **Fast surrogates for magnetic confinement analysis:** In “*Fast neural Poincaré maps for toroidal magnetic fields*” (2020), Tang developed neural approaches to accelerate Poincaré map evaluation—reducing reliance on repeated field-line integration—thereby supporting rapid assessment of confinement-relevant magnetic topology in fusion devices.  \n\n---\n\n## Academic Background\nBased on Tang’s publication arc—from early, highly cited contributions (2013–2016) on **WENO**, **DG**, and positivity/maximum-principle preserving schemes for conservation laws and MHD, to later work (2017–2018) on stable partitioned algorithms for <u>fluid–structure interaction</u>, and then to fusion-centric equilibrium/kinetic modeling and **scientific machine learning** (2020–2025)—their academic training is strongly rooted in **applied and computational mathematics** with sustained engagement in computational plasma physics. Their affiliation with Georgia Tech’s **Computational Science and Engineering** community aligns with a research profile that couples mathematical analysis, algorithm design, and *HPC-ready* implementations for large-scale simulation and surrogate modeling. The breadth of topics—ideal MHD discretization, Grad–Shafranov solvers, implicit scalable algorithms for drift-kinetic/Fokker–Planck–Boltzmann models, and structure-preserving neural architectures—indicates active collaboration across applied math, fusion energy science, and computational engineering networks, with impact reflected in a citation record of **820** and multiple widely used methodological contributions in both classical numerical PDEs and modern physics-informed learning."}, "F0HhkJIAAAAJ": {"keywords": "Electrical and Computer Engineering, Computing Diversity, Context-aware communication, Ubiquitous computing in the home, Availability and interruption management, Domestic communication systems (family intercom), Situation-aware interpersonal communication, Collaborative augmented reality, Deictic referencing and virtual pointers, Broadening participation and retention in computing education", "summary": "## Overview\nKristine Nagel is a researcher affiliated with the Georgia Institute of Technology, working at the intersection of **Electrical and Computer Engineering** and *computing education*. Her scholarship centers on designing and evaluating <u>human-centered computing systems</u> and <u>learning interventions</u> that improve everyday communication and broaden participation in computing. Across her publication record—spanning domestic context-aware communication, collaborative augmented reality, and diversity-focused educational programs—Nagel’s work emphasizes **evidence-based design** and *practical, deployable approaches* that connect technical innovation with human needs.\n\n---\n\n## Research Areas\nNagel’s research spans two closely connected domains: (1) <u>context-aware interpersonal communication</u> and (2) <u>computing participation and persistence</u>. In ubiquitous and domestic computing, she investigated how people manage interruptions and negotiate availability outside the workplace, developing models and design implications for home settings in “**Predictors of availability in home life context-mediated communication**” (2004) and extending these ideas into system concepts such as “**Building a better intercom: Context-mediated communication within the home**” (2000) and “**Using availability indicators to enhance context-aware family communication applications**” (2006). This line of work reframed “availability” as a socially situated, home-specific construct, including routine- and activity-sensitive reasoning about when interruptions are welcome (e.g., “**Between Dinner and Children’s Bedtime: Predicting and Justifying Routines in the Home**,” 2005; “**Designing home availability services**,” 2007). In parallel, Nagel contributed to <u>collaborative augmented reality</u> by analyzing how collaborators successfully reference objects across mixed physical/virtual scenes, mapping the design space of referencing in “**Understanding the design space of referencing in collaborative augmented reality environments**” (2007) and evaluating interaction techniques such as “**virtual pointers**” in “**Studies on the effectiveness of virtual pointers in collaborative augmented reality**” (2008). More recently, her work foregrounds <u>computing diversity and student success</u>, designing interventions that build confidence, community, and persistence—especially for underrepresented students—through service-learning and bridge programs, including “**Technology focused service-learning course to increase confidence and persistence in computing**” (2018), “**STARS Computing Corps: Enhancing Engagement of Underrepresented Students and Building Community in Computing**” (2016), and experience reports on programming boot camps (2015–2017). Complementing these interventions are pedagogical innovations that support engagement and inquiry (e.g., “**Scenario-based inquiry for engagement in general education computing**,” 2017) and structured collaborative learning environments such as “**The design discussion area**” (1999) and “**SMILE**” (1999), which scaffold reflection and peer-to-peer discourse during design-centered learning.\n\n---\n\n## Notable Works\n- **Availability modeling and design for domestic context-mediated communication** — In “*Predictors of availability in home life context-mediated communication*” (2004), Nagel advanced a research program on <u>home availability</u>, identifying factors that shape interruption tolerance and translating them into design guidance for signaling and negotiating contact among close ties.\n- **Scaffolded collaborative learning for design and problem-solving** — Through “*The design discussion area: A collaborative learning tool in support of learning from problem-solving and design activities*” (1999), she introduced structured supports that help learners produce well-formed design narratives and engage in reflective discussion, strengthening <u>learning-from-design</u> pedagogy.\n- **Referencing techniques for effective collaboration in augmented reality** — In “*Understanding the design space of referencing in collaborative augmented reality environments*” (2007) and the evaluation study “*Studies on the effectiveness of virtual pointers in collaborative augmented reality*” (2008), Nagel clarified how reference cues function in mixed environments and assessed interaction mechanisms that improve shared understanding in <u>collaborative AR</u>.\n\n---\n\n## Academic Background\nAffiliated with the Georgia Institute of Technology, Nagel’s academic trajectory reflects sustained engagement with both **human-centered computing research** and *computing education for broadening participation*. Her early, highly cited contributions (1999–2008) indicate deep involvement in research communities focused on collaborative learning technologies, ubiquitous computing, and augmented reality—work characterized by empirical studies, design frameworks, and evaluative experimentation. A later phase of her scholarship (2012 onward) shows a strong institutional and community-building orientation, including course-embedded research initiatives (e.g., “**Transforming programming-intensive courses with course-embedded research**,” 2012), outreach logistics for scalable summer computing camps (2015–2017), and national participation efforts through the **STARS Computing Corps** (2016). With **411 total citations**, her record suggests both foundational impact in domestic context-aware communication and sustained applied influence on <u>student persistence and inclusion</u> in computing, aligning her affiliations and collaborations with major U.S. computing education and human–computer interaction venues and initiatives."}, "FDu4ciwAAAAJ": {"keywords": "Machine Learning, Game Theory, Sequential Decision Making, Online Learning, Regret Minimization, Multi-Armed Bandits, Online Convex Optimization, Minimax Optimization, Generative Adversarial Networks, Mechanism Design", "summary": "## Overview\nJacob Abernethy is a **Research Scientist at Google** and an **Associate Professor of Computing at Georgia Tech**, where he leads and collaborates with groups working at the intersection of **machine learning**, **game theory**, and **sequential decision making**. His scholarship is centered on *the theory and practice of learning in strategic and adversarial environments*, with particular emphasis on <u>regret minimization</u>, <u>online optimization</u>, and <u>min–max dynamics</u> as unifying principles for modern learning systems. Across highly cited contributions—from bandit linear optimization to the training dynamics of generative models—Abernethy has advanced a view of learning algorithms as **interactive processes** whose stability, incentives, and convergence can be analyzed through *games* and *optimization*.\n\n---\n\n## Research Areas\nAbernethy’s research spans foundational theory for **online learning** and **adversarial decision-making**, as well as applications that connect learning to markets, mechanisms, and modern deep learning practice. A recurring theme is the design of algorithms with provable guarantees under partial information, exemplified by bandit and online linear optimization results such as “**Competing in the dark: An efficient algorithm for bandit linear optimization**” (2009) and related work on efficient methods and high-probability guarantees. In online game formulations, he has developed minimax perspectives and equilibrium-oriented analyses, including “**Optimal strategies and minimax lower bounds for online convex games**” (2008) and the conceptual bridge “**Blackwell approachability and no-regret learning are equivalent**” (2011), which ties classical game-theoretic approachability to modern no-regret dynamics. He has also contributed to the theory of **min–max optimization** and stability in nonconvex settings, prominently reframing GAN training through online learning in “**On convergence and stability of gans**” (2017) and extending convergence thinking to last-iterate behavior in later min–max work. Complementing these core areas are contributions to **collaborative filtering and matrix methods** (e.g., spectral regularization approaches in “**A new approach to collaborative filtering: Operator estimation with spectral regularization**,” 2009), and to **market design and learning-incentives interfaces**, including convex-optimization-based market making (“**Efficient market making via convex optimization, and a connection to online learning**,” 2013) and pricing/experimentation in bandit-driven revenue optimization (“**Dynamic online pricing with incomplete information using multiarmed bandit experiments**,” 2019).\n\n---\n\n## Notable Works\n- **Reframing GAN training as regret minimization and analyzing stability**: In “**On convergence and stability of gans**” (2017), Abernethy and collaborators proposed analyzing GAN dynamics through the lens of **regret minimization** and game dynamics, providing a principled account of convergence pathologies and stability phenomena in adversarial training.\n- **Optimal, efficient algorithms for bandit linear optimization**: “**Competing in the dark: An efficient algorithm for bandit linear optimization**” (2009) introduced an efficient approach achieving optimal-order regret for bandit online linear optimization, helping establish a modern foundation for **partial-information online decision-making**.\n- **A unifying operator-theoretic view of collaborative filtering via spectral regularization**: In “**A new approach to collaborative filtering: Operator estimation with spectral regularization**” (2009), Abernethy and coauthors presented a general framework that subsumes low-rank matrix completion methods, clarifying how **spectral regularization** can structure learning of user–item operators.\n\n---\n\n## Academic Background\nAbernethy’s academic trajectory reflects a sustained focus on rigorous foundations for learning and decision-making, with an early publication record spanning online learning, convex games, and collaborative filtering (including influential 2006–2009 work on low-rank methods, online convex games, and bandit optimization). He holds a faculty appointment in **Computing at Georgia Tech**, where his research agenda aligns naturally with academic communities in **theoretical machine learning**, **optimization**, and **algorithmic game theory**, and he concurrently contributes to industrial research as a **Research Scientist at Google**, bridging theory with large-scale learning systems. His impact is evidenced by substantial scholarly uptake (over **5,500 citations**), with widely cited papers that have become reference points in both the online learning literature (e.g., minimax regret and bandit optimization) and the modern study of adversarial training dynamics (e.g., GAN stability). His cross-cutting affiliations—spanning academia and industry—are consistent with a research program that connects <u>theory</u> (minimax analysis, approachability, regret) to <u>practice</u> (market mechanisms, pricing experiments, and robust learning dynamics)."}, "FzBDS4gAAAAJ": {"keywords": "Computer Architecture, Compilers, Computer Performance Evaluation, Superscalar and Wide-Issue Microarchitecture, Instruction Fetch and Branch Prediction, Profile-Driven Optimization and Hardware Performance Counters, Instruction Scheduling for ILP (VLIW/Clustered Architectures), Cache and Memory Hierarchy Design (Leakage-Aware/Low-Power Caches), Architectural Simulation and Benchmark Characterization (EEMBC/Trace Sampling), Multicore Systems and Power/Energy Modeling", "summary": "## Overview\nTom Conte is the **Associate Dean for Research** in the **College of Computing** and a **Professor of Computer Science and Electrical & Computer Engineering** at Georgia Tech, where he has led and shaped research at the intersection of *high-performance computing systems* and *practical, measurable performance*. His scholarship centers on **computer architecture**, **compilers**, and **computer performance evaluation**, with an emphasis on <u>architectural mechanisms</u> and <u>measurement-driven design methodology</u> that connect hardware innovations to real workload behavior. With **6,295 citations**, Conte’s body of work reflects sustained influence across processor microarchitecture, profiling and simulation infrastructure, and system-level design tradeoffs.\n\n---\n\n## Research Areas\nConte’s research spans three tightly coupled areas—architecture, compilation, and evaluation—unified by a consistent focus on *feeding the machine efficiently* and *measuring the right things at the right fidelity*. In **microarchitecture**, he has contributed foundational ideas in instruction delivery and front-end design for wide-issue processors, including high-bandwidth instruction fetch and mechanisms tailored to increasing issue rates (e.g., **“Optimization of instruction fetch mechanisms for high issue rates”**, 1995) as well as branch-cost mitigation in pipelined designs (**“Comparing software and hardware schemes for reducing the cost of branches”**, 1989). In **compiler–architecture co-design**, his work addresses instruction scheduling and clustered microarchitectures, notably the coupling of assignment and scheduling decisions for partitioned register files (**“Unified assign and schedule: A new approach to scheduling for clustered register file microarchitectures”**, 1998) and region-based scheduling strategies for highly parallel processors (e.g., “Treegion scheduling for wide issue processors,” 1998). In **performance evaluation and methodology**, Conte has advanced practical approaches to profiling and simulation—reducing measurement overhead and improving representativeness—through trace sampling techniques (**“Reducing state loss for effective trace sampling of superscalar processors”**, 1996) and broader arguments for systematic, benchmark-aware design (“Performance analysis and its impact on design,” 2002; “Benchmark characterization,” 1991). His later work also reflects engagement with emerging system contexts, including multicore simulation frameworks (**“Manifold: A parallel simulation framework for multicore systems”**, 2014), energy and memory technology tradeoffs (e.g., “Energy efficient phase change memory based main memory…,” 2011), and even networking-oriented robustness (**“Robust multipath routing”**, 2013), underscoring a methodological throughline: *architectural decisions should be justified by rigorous, workload-relevant evidence*.\n\n---\n\n## Notable Works\n- **Instruction fetch and front-end scaling for wide-issue processors** — Conte’s work on sustaining high issue rates through improved fetch mechanisms, exemplified by **“Optimization of instruction fetch mechanisms for high issue rates”** (1995), helped articulate how front-end bandwidth, prediction, and fetch organization constrain realized ILP in superscalar designs.\n- **Trace sampling methodology for realistic superscalar evaluation** — In **“Reducing state loss for effective trace sampling of superscalar processors”** (1996), he advanced evaluation techniques that reduce distortion and overhead in sampled simulation, strengthening the empirical foundation for comparing microarchitectural alternatives.\n- **Compiler-aware scheduling for clustered register-file microarchitectures** — Through **“Unified assign and schedule: A new approach to scheduling for clustered register file microarchitectures”** (1998), he contributed a structured approach to jointly handling instruction placement and scheduling—key for clustered designs seeking shorter cycle times without sacrificing performance.\n\n---\n\n## Academic Background\nAs a senior faculty member at **Georgia Tech** with joint appointment across **CS and ECE**, Conte’s career reflects a long-standing commitment to bridging *architectural innovation* with *compiler and evaluation techniques* that make such innovation usable and verifiable. The trajectory of his publication record—from early work on pipelining and branch handling (late 1980s), through the 1990s emphasis on instruction fetch, VLIW/superscalar evaluation, profiling, and scheduling, to 2000s research on power-aware caches and systematic performance analysis—suggests deep involvement in the core debates that shaped modern processor design. His later leadership role as **Associate Dean for Research** indicates substantial institutional service and research stewardship, consistent with a scholar whose influence extends beyond individual contributions to the broader research ecosystem, including benchmarking practice (e.g., **“A benchmark characterization of the EEMBC benchmark suite”**, 2009) and multicore evaluation infrastructure (e.g., **“Manifold…”**, 2014)."}, "G6kA1CMAAAAJ": {"keywords": "Numerical Algorithms, Data Analysis, Visual Analytics, Parallel Computing, Nonnegative Matrix Factorization (NMF), Sparse Matrix Factorization, Graph Clustering, Block Coordinate Descent Optimization, Generalized Singular Value Decomposition (GSVD), Interactive Topic Modeling", "summary": "## Overview\nHaesun Park is a **Regents’ Professor of Computational Science and Engineering** at the **Georgia Institute of Technology**, where she leads research at the intersection of *numerical linear algebra* and *data-driven computing* for modern, large-scale scientific and information problems. Her work centers on **scalable numerical algorithms** for <u>matrix and tensor factorizations</u>, with an emphasis on *interpretability* and *computational efficiency* in **data analysis**, **visual analytics**, and **parallel computing**. With **21,022 citations**, Park’s scholarship has helped shape how <u>nonnegative matrix factorization (NMF)</u> and related decomposition methods are understood, optimized, and deployed across domains such as clustering, text mining, and bioinformatics.\n\n---\n\n## Research Areas\nPark’s research spans foundational algorithm design and application-driven data analytics, anchored by influential contributions to **nonnegative matrix factorization** and its role in <u>clustering and representation learning</u>. In “**Orthogonal nonnegative matrix t-factorizations for clustering**” (2006), she advanced the theory and practice of *three-factor* NMF variants, clarifying when additional factor structure yields new modeling capabilities beyond standard two-factor formulations. Her work on sparsity—particularly “**Sparse non-negative matrix factorizations via alternating non-negativity-constrained least squares for microarray data analysis**” (2007) and “**Sparse nonnegative matrix factorization for clustering**” (2008)—connects NMF objectives to K-means-like formulations and emphasizes *sparse, parts-based representations* for high-dimensional biological and text data. A recurring theme is algorithmic rigor: “**Nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method**” (2008) and “**Fast nonnegative matrix factorization: An active-set-like method and comparisons**” (2011) develop efficient optimization strategies for nonconvex NMF problems, while “**Algorithms for nonnegative matrix and tensor factorizations: A unified view based on block coordinate descent framework**” (2014) synthesizes the field through the lens of <u>block coordinate descent</u>. Beyond NMF, Park has contributed to **dimension reduction** and **discriminant analysis** using tools such as the <u>generalized singular value decomposition (GSVD)</u> (e.g., “Generalizing discriminant analysis using the generalized singular value decomposition,” 2004), and to **visual analytics** and interactive machine learning for text exploration (e.g., “Utopian: User-driven topic modeling based on interactive nonnegative matrix factorization,” 2013; and systems work on sensemaking in Jigsaw, 2012/2013), reflecting a sustained interest in *human-in-the-loop* analytical workflows.\n\n---\n\n## Notable Works\n- **Structured NMF for clustering via multi-factor models:** In “**Orthogonal nonnegative matrix t-factorizations for clustering**” (2006), Park and collaborators provided a systematic treatment of *three-factor* NMF, highlighting how <u>constraints and orthogonality</u> can fundamentally change clustering behavior and interpretability relative to two-factor NMF.\n- **Sparse NMF as a principled clustering and bioinformatics tool:** Through “**Sparse non-negative matrix factorizations via alternating non-negativity-constrained least squares for microarray data analysis**” (2007) and “**Sparse nonnegative matrix factorization for clustering**” (2008), Park helped establish sparse NMF as a practical framework for <u>high-dimensional data</u>, linking NMF objectives to K-means-style formulations and enabling more *interpretable* factor patterns in microarray and other nonnegative datasets.\n- **Algorithmic foundations and unification of NMF/NTF optimization:** Park’s “**Fast nonnegative matrix factorization: An active-set-like method and comparisons**” (2011) and the survey “**Algorithms for nonnegative matrix and tensor factorizations: A unified view based on block coordinate descent framework**” (2014) advanced <u>efficient solvers</u> and offered a coherent optimization perspective that has guided subsequent work in scalable matrix/tensor factorization.\n\n---\n\n## Academic Background\nAs a senior faculty member at **Georgia Tech** holding the distinguished title of **Regents’ Professor**, Haesun Park’s academic trajectory reflects sustained leadership in **computational science and engineering** and in the broader community of **numerical algorithms for data analysis**. Her publication record—spanning early contributions to structured least-norm and total least-squares formulations (e.g., “Total least norm formulation and solution for structured problems,” 1996), parallel algorithm considerations (e.g., “On parallel Jacobi orderings,” 1989), and later high-impact advances in NMF, discriminant analysis, and interactive visual analytics—suggests a career that evolved from core numerical linear algebra toward *scalable, application-facing* machine learning and sensemaking systems. The breadth of her collaborations and application domains (including microarray analysis, text mining, graph clustering via symmetric NMF, and interactive topic modeling) indicates strong interdisciplinary affiliations across computing, data science, and applied scientific communities, consistent with her role in computational science and engineering at a major research institution."}, "GmcChLIAAAAJ": {"keywords": "computer graphics, human-computer interaction, information visualization, interactive computer graphics, model-based user interface design, user interface management systems (UIMS), knowledge-based user interface design, virtual reality interfaces, hypermedia navigation visualization, faceted search interfaces", "summary": "## Overview\nJames Foley is a Professor of Computer Science at the Georgia Institute of Technology, where he has shaped the intellectual foundations of **computer graphics** and **human–computer interaction (HCI)** through research, writing, and curricular leadership. His work emphasizes *interactive* systems that connect computational representations to human goals, spanning <u>interactive graphics programming</u>, <u>user interface design</u>, and <u>information visualization</u>. With a widely cited publication record (over **26,000 citations**), Foley is especially recognized for articulating core principles—both conceptual and practical—behind modern graphics pipelines, interaction techniques, and the design of usable interfaces.\n\n---\n\n## Research Areas\nFoley’s research portfolio bridges foundational graphics methods with human-centered interface engineering. In computer graphics, his scholarship systematized the algorithms and systems concepts that underlie interactive image generation, including raster graphics programming, 2D primitive rendering, geometrical transformations, 3D viewing, object hierarchies, and the role of graphics hardware, as synthesized in *Computer graphics: principles and practice* (1996) and earlier in *Fundamentals of interactive computer graphics* (1982). In HCI, he examined the <u>human factors</u> that govern effective interaction techniques—how designers choose among devices, gestures, and dialogue structures to support real work—most explicitly in *The human factors of computer graphics interaction techniques* (1984) and the earlier design-oriented framing of natural man–machine “conversation” in *The art of natural graphic man—Machine conversation* (1974). A major throughline of his later work is *model-based* and *knowledge-based* user interface engineering: the UIDE program of research (e.g., *UIDE—an intelligent user interface design environment*, 1991; *A second generation user interface design environment: The model and the runtime architecture*, 1993; and the retrospective *History, results, and bibliography of the user interface design environment (UIDE)*, 1995) advanced representations that embed application semantics into interface design and runtime behavior, enabling higher-level specification, consistency, and potential automation (including context-sensitive help and evaluation). Complementing these strands, Foley contributed to information visualization for navigation and search in large information spaces, including hypermedia and the web (e.g., *Visualizing the world-wide web with the navigational view builder*, 1995; *Visualizing complex hypermedia networks through multiple hierarchical views*, 1995) and later work on visualization-driven search interfaces such as *Resultmaps: Visualization for search interfaces* (2009). He also engaged applied domains where interactive graphics and interface design converge, including early discussions of immersive and sensor-rich interfaces (*Interfaces for advanced computing*, 1987) and domain-facing overviews such as *Virtual reality in medicine* (1993).\n\n---\n\n## Notable Works\n- **Codifying the modern graphics curriculum and practice** through the landmark textbook *Computer graphics: principles and practice* (1996), which unified algorithms (e.g., raster methods and transformations), system architecture (hardware and viewing pipelines), and interaction topics into a canonical reference for the field.\n- **Establishing a foundational account of interactive graphics** in *Fundamentals of interactive computer graphics* (1982), a highly influential synthesis that helped define interactive graphics as a coherent area spanning programming models, display technologies, and user interaction.\n- **Advancing model-based, knowledge-rich user interface development** via the UIDE research line—especially *UIDE—an intelligent user interface design environment* (1991) and *A second generation user interface design environment: The model and the runtime architecture* (1993)—which argued for higher-level interface specifications grounded in application models to support design-time guidance and robust runtime behavior.\n\n---\n\n## Academic Background\nAs a long-standing academic leader in computing, Foley’s career reflects sustained contributions across scholarship, pedagogy, and community formation in graphics and HCI. His early work in the 1970s and 1980s—ranging from design principles for interactive graphic “conversation” (1974) to standards- and systems-oriented perspectives on interactive graphics programming—positioned him at the emergence of interactive computer graphics as a discipline. The scale and longevity of his influence are evident in the enduring adoption and citation of his textbooks (*Fundamentals of interactive computer graphics*, 1982; *Introduction to computer graphics*, 1994; and *Computer graphics: principles and practice*, 1996), which functioned as de facto curricula for generations of students and practitioners. His publication history also indicates deep engagement with major research communities in **SIGGRAPH**, **HCI**, and **information visualization**, alongside collaborations that connect interface theory to implementation architectures (notably the UIDE body of work in the early-to-mid 1990s). At Georgia Tech, his professorial role aligns with a profile that integrates research with educational innovation—later exemplified by empirically grounded studies of instructional technology and learning design in computing education (e.g., *Evaluating a web lecture intervention in a human–computer interaction course*, 2006)—and with the broader institutional mission of advancing interactive computing through both foundational theory and human-centered systems."}, "HXcs2NIAAAAJ": {"keywords": "Security and Privacy, Machine Learning, Neural Network Verification, Quantitative Formal Methods, Adversarial Robustness, Membership Inference Attacks, Federated Learning Privacy, Graph Neural Networks, Dynamic Binary Taint Analysis, Machine Unlearning and Model Provenance", "summary": "## Overview\nTeodora Baluta is a researcher at the Georgia Institute of Technology, where she works within the broader security-and-ML research ecosystem on rigorous foundations for **Security and Privacy** in **Machine Learning**. Her work centers on *making learning-enabled systems trustworthy* through <u>formal and quantitative reasoning</u>, spanning **verification of deep neural networks**, **privacy attacks and defenses**, and **security analysis of complex software and training pipelines**. With 436 citations, Baluta’s scholarship is characterized by an emphasis on <u>measurable guarantees</u>—bringing together formal methods, systems security, and modern learning practice to evaluate and improve the robustness, privacy, and provenance of deployed models.\n\n---\n\n## Research Areas\nBaluta’s research portfolio integrates formal verification, privacy, and systems-oriented security for ML. A major theme is <u>quantitative verification</u> of neural networks: in “**Quantitative verification of neural networks and its security applications**” (2019), she advances verification beyond purely existential questions toward measuring security-relevant properties, while “**Scalable quantitative verification for deep neural networks**” (2021) tackles the central tension between scalability and formal guarantees for large models. Complementing verification, she studies <u>privacy leakage mechanisms</u> in learning, including “**Membership Inference Attacks and Generalization: A Causal Perspective**” (2022), which reframes membership inference through causal reasoning to clarify when and why generalization gaps translate into privacy risk. Her work also extends to graph learning and privacy-preserving analytics—e.g., “**LPGNet: Link Private Graph Networks for Node Classification**” (2022) and “**Private Hierarchical Clustering in Federated Networks**” (2021)—as well as classic systems security problems such as dynamic analysis of binaries, exemplified by “**One Engine To Serve'em All: Inferring Taint Rules Without Architectural Semantics**” (2019). More recently, she has contributed to emerging concerns in foundation models, including <u>model provenance</u> (“**Model Provenance Testing for Large Language Models**,” 2025) and <u>machine unlearning evaluation</u> in LLMs (“**Unlearning in-vs. out-of-distribution data in LLMs under gradient-based methods**,” 2024), reflecting a sustained focus on accountability and enforceable guarantees in modern ML ecosystems.\n\n---\n\n## Notable Works\n- **Quantitative verification for security-relevant guarantees in neural networks** — Through “*Quantitative verification of neural networks and its security applications*” (2019), Baluta helped formalize and operationalize <u>quantitative</u> properties of neural networks, connecting verification outcomes to practical security applications rather than limiting analysis to yes/no satisfiability.\n- **Scaling formal guarantees to deep models** — In “*Scalable quantitative verification for deep neural networks*” (2021), she addressed the verification bottleneck for modern architectures by developing methods that better reconcile **scalability** with **formal assurance**, advancing trustworthy deployment for deep learning systems.\n- **Architecture-agnostic dynamic taint analysis for real-world binaries** — With “*One Engine To Serve'em All: Inferring Taint Rules Without Architectural Semantics*” (2019), she contributed a systems-security approach to infer taint propagation rules without relying on architectural semantics, improving the practicality of analyzing COTS binaries across architectures.\n\n---\n\n## Academic Background\nAt the Georgia Institute of Technology, Baluta has developed an academic profile at the intersection of **computer security**, **formal methods**, and **machine learning**, as evidenced by a publication trajectory spanning neural network verification (2019–2021), privacy and federated settings (2021–2022), and more recent foundation-model governance topics such as provenance and unlearning (2024–2025). Her early work on “*Modeling the effects of insider threats on cybersecurity of complex systems*” (2017) indicates foundational training in cybersecurity risk modeling and system-level threat analysis, which later evolves into rigorous, ML-centric security questions—e.g., formalizing verification objectives, diagnosing privacy leakage mechanisms, and evaluating accountability tools for LLMs. Across affiliations and collaborations implied by her cross-cutting topics (binary analysis, graph learning, causal privacy analysis, and LLM provenance), her record reflects sustained engagement with the academic security community and a consistent emphasis on <u>rigorous security analysis of machine learning systems</u> as a unifying scholarly agenda."}, "HmBa_6gAAAAJ": {"keywords": "Computer Architecture, System Infrastructure Design, Machine Learning, Domain-Specific Accelerators, FPGA-Based DNN Acceleration, Approximate Computing, Processing-in-Memory (PIM) Architectures, Distributed Deep Learning Training, Large Language Model (LLM) Inference Acceleration, Recommendation System Training Acceleration", "summary": "## Overview\nDivya Mahajan is a researcher at **Georgia Tech**, where she works at the intersection of **computer architecture** and **system infrastructure design** to make *machine learning systems* faster, more efficient, and easier to deploy at scale. Her work centers on <u>domain-specific acceleration</u>—spanning **FPGAs**, heterogeneous accelerator platforms, and emerging <u>processing-in-memory</u> paradigms—while also advancing the software and algorithmic layers needed to translate high-level models into efficient hardware execution. Across a publication record exceeding **2,255 citations**, Mahajan’s research consistently emphasizes *end-to-end co-design*, aligning models, compilers/runtime systems, and accelerator architectures to close the gap between ML innovation and practical system performance.\n\n---\n\n## Research Areas\nMahajan’s research spans three tightly connected areas: (1) **ML acceleration on reconfigurable and specialized hardware**, (2) **approximate computing across the hardware/software stack**, and (3) **systems-level optimization for large-scale and heterogeneous ML execution**. Early influential work on compiling and mapping deep learning to reconfigurable hardware is exemplified by *“From high-level deep neural models to FPGAs”* (2016), which addresses the challenge of taking **high-level DNN descriptions** and producing efficient FPGA implementations—an important step toward democratizing hardware acceleration for non-hardware experts. In parallel, she helped shape the empirical foundations of approximate computing through *“AxBench: A Multiplatform Benchmark Suite for Approximate Computing”* (2017) and language/tooling efforts like *“Axilog: Language support for approximate hardware design”* (2015), focusing on principled ways to trade accuracy for gains in energy, area, and performance. Her work also advances template- and framework-based acceleration for broader statistical ML, notably *“Tabla: A unified template-based framework for accelerating statistical machine learning”* (2016), which targets the diminishing returns of general-purpose platforms for enterprise ML. More recently, Mahajan’s publications extend to modern large-model workloads and distributed systems, including heterogeneous acceleration for batched LLM inference in *“NeuPIMs: NPU-PIM Heterogeneous Acceleration for Batched LLM Inferencing”* (2024), and algorithmic/system techniques for heterogeneous execution such as *“Efficient algorithms for device placement of DNN graph operators”* (2020). Complementing these are systems contributions in recommender training—e.g., *“Accelerating Recommendation System Training by Leveraging Popular Choices”* (2022) and subsequent pipeline work—along with research on federated learning robustness (*“FLuID…”*, 2023) and performance modeling/forecasting for GPU-based deep learning (*“Forecasting GPU Performance for Deep Learning Training and Inference”*, 2025). Collectively, these efforts emphasize <u>cross-layer optimization</u>: from language and benchmarking infrastructure, to accelerator microarchitecture, to deployment-aware scheduling and placement.\n\n---\n\n## Notable Works\n- **End-to-end FPGA mapping for deep learning** — In *“From high-level deep neural models to FPGAs”* (2016), Mahajan contributed to foundational methods that connect **high-level DNN model descriptions** to efficient FPGA realizations, helping establish practical pathways for deploying neural networks on reconfigurable accelerators.\n- **Benchmarking and standardization for approximate computing** — Through *“AxBench: A Multiplatform Benchmark Suite for Approximate Computing”* (2017), she helped define a widely used benchmark suite enabling reproducible evaluation of <u>quality–efficiency tradeoffs</u> across applications and platforms, strengthening methodological rigor in approximate computing research.\n- **Template-based acceleration for statistical machine learning** — In *“Tabla: A unified template-based framework for accelerating statistical machine learning”* (2016), she advanced a framework approach to accelerate compute-intensive statistical ML workloads on programmable hardware, addressing performance limits of general-purpose systems in enterprise settings.\n\n---\n\n## Academic Background\nAt **Georgia Tech**, Mahajan has developed a research portfolio that reflects sustained engagement with leading directions in **computer architecture**, **accelerator design**, and **ML systems**, with recurring themes of <u>hardware–software co-design</u> and infrastructure that supports fair evaluation (e.g., benchmark development). Her publication trajectory—from early work on emerging devices (*“Memristor based adders”*, 2014), to language/toolchain support for approximation (*“Axilog…”*, 2015), to highly cited acceleration frameworks and benchmarking efforts (2016–2017), and onward to heterogeneous/distributed optimization for modern deep learning and LLM workloads (2020–2025)—suggests deep involvement in collaborative, multi-disciplinary research communities spanning architecture, systems, and ML. Her work is also linked to translational and applied research efforts at Georgia Tech, including patents and system-integration studies (e.g., methods for integrating ML/analytics accelerators with relational database systems), indicating active engagement with broader deployment contexts and industry-relevant infrastructure."}, "IQosWycAAAAJ": {"keywords": "Software Engineering, Requirements Engineering, Privacy, Security, Regulatory Compliance, Goal-Oriented Requirements Engineering (GBRAM), Legal Requirements Engineering, Privacy Policy Analysis, Rights-Obligations Extraction, Text Mining for Policy/Regulation Documents", "summary": "## Overview\nAnnie I. Antón is **Chair and Professor** in the **School of Interactive Computing** at the **Georgia Institute of Technology**, where she leads and collaborates within a research program centered on **Software Engineering** and, in particular, *Requirements Engineering* for socio-technical systems. Her scholarship is widely recognized for uniting rigorous requirements methods with the practical realities of **privacy**, **security**, and **regulatory compliance**, advancing the idea that <u>policy- and law-aware requirements</u> must be treated as first-class engineering artifacts. Across a highly cited body of work (≈**9,880 citations**), Antón has helped shape how researchers and practitioners elicit, refine, and validate requirements when systems evolve and when external constraints—especially legal and organizational commitments—must be made explicit and auditable.\n\n---\n\n## Research Areas\nAntón’s research spans foundational and applied requirements engineering, with early influential contributions to **goal-oriented requirements** and inquiry-driven elicitation. In “**Inquiry-based requirements analysis**” (1994), she emphasizes *where and when information needs occur* via the **Inquiry Cycle Model**, framing requirements work as a structured conversation that supports discovery and refinement. Building on this perspective, “**Goal-based requirements analysis**” (1996) and subsequent goal-oriented studies (e.g., “**The use of goals to surface requirements for evolving systems**,” 1998; “**Goal identification and refinement in the specification of software-based information systems**,” 1997) develop systematic approaches for identifying, organizing, and evolving requirements through explicit goal structures and refinements. A second major arc of her work addresses <u>regulatory compliance and legally grounded requirements</u>, notably by extracting and operationalizing rights and obligations from legal texts—e.g., “**Towards regulatory compliance: Extracting rights and obligations to align requirements with regulations**” (2006) and “**Analyzing regulatory rules for privacy and security requirements**” (2008)—and by characterizing sources of error such as ambiguity (“**Identifying and classifying ambiguity for regulatory requirements**,” 2014). In parallel, Antón has advanced privacy engineering through empirical and analytical studies of online privacy notices and user values (e.g., “**Examining Internet privacy policies within the context of user privacy values**,” 2005; “**Financial privacy policies and the need for standardization**,” 2004), as well as methods to translate policy statements into requirements artifacts (e.g., privacy goal taxonomies and policy mining). Collectively, these threads connect requirements modeling, natural language policy analysis, and human-centered privacy concerns into an integrated agenda for engineering systems that are both *usable* and demonstrably compliant.\n\n---\n\n## Notable Works\n- **Goal-oriented requirements foundations:** Antón’s “**Goal-based requirements analysis**” (1996) established a highly influential framework for using goals to identify, organize, justify, and manage software requirements, including attention to **goal evolution** as systems and stakeholder needs change.  \n- **Regulatory compliance through rights/obligations extraction:** In “**Towards regulatory compliance: Extracting rights and obligations to align requirements with regulations**” (2006), she advanced methods for deriving implementable requirements from ambiguous legal texts by explicitly modeling stakeholder **rights** and **obligations** to reduce ad hoc interpretation.  \n- **Privacy and security requirements grounded in regulation:** “**Analyzing regulatory rules for privacy and security requirements**” (2008) connected legal governance of personal data to concrete requirements engineering practices, helping formalize how privacy and security requirements can be aligned with applicable U.S. laws and regulations.\n\n---\n\n## Academic Background\nAntón’s publication trajectory indicates a research career that began with foundational contributions in the mid-1990s to **requirements elicitation and modeling**, including scenario- and inquiry-based analysis, and quickly matured into a sustained, high-impact program on **goal-oriented requirements engineering**—work that culminated in the dissertation-like treatment “**Goal identification and refinement in the specification of software-based information systems**” (1997). Over time, her agenda expanded toward interdisciplinary scholarship at the intersection of software engineering, public policy, and human factors, as reflected in her extensive series of studies on privacy policies, consumer comprehension, and the translation of legal and organizational commitments into verifiable requirements (2000s onward). Her leadership role as **Chair** at Georgia Tech and her long-standing, highly cited contributions suggest significant professional recognition and influence within the requirements engineering and privacy/security communities, supported by collaborations that bridge computing, law/policy analysis, and empirical studies of user and organizational practices."}, "IdWa_JkAAAAJ": {"keywords": "social computing, collaborative computing, online communities, Wikipedia governance, peer production, content moderation, online harassment, internet research ethics, constructionist learning, youth computing education", "summary": "## Overview\nAmy Bruckman is a **Regents’ Professor** in **Interactive Computing** at the **Georgia Institute of Technology**, where she leads research at the intersection of **social computing** and **collaborative computing**. Her scholarship examines how people learn, work, and govern themselves in networked publics—ranging from early text-based virtual worlds to contemporary platforms such as Wikipedia, Reddit, and Twitter—while advancing *methodological and ethical* standards for studying online communities. Across this agenda, she foregrounds <u>community governance</u>, <u>participation trajectories</u>, and <u>responsible research practice</u>, linking empirical analyses of platform life to actionable design and policy insights.\n\n---\n\n## Research Areas\nBruckman’s research spans three tightly connected areas. First, she is a foundational contributor to research on <u>participation and identity in online communities</u>, with early work on identity play and representation in networked settings (e.g., “**Gender swapping on the Internet**,” 1996; “Identity workshops,” 1992) and later analyses of how newcomers become committed contributors in peer production (e.g., “**Becoming Wikipedian**,” 2005; “Why do people write for Wikipedia?,” 2005). Second, she has produced influential studies of <u>governance, norms, and moderation</u> in large-scale sociotechnical systems, documenting the decentralization and institutionalization of authority in Wikipedia (“**Decentralization in Wikipedia governance**,” 2009; “Scaling consensus,” 2008) and empirically characterizing rule systems and enforcement on Reddit and Twitter (“**The Internet’s hidden rules**,” 2018; “**Human-machine collaboration for content regulation**,” 2019; “Does transparency in moderation really matter?,” 2019; “Online harassment and content moderation: The case of blocklists,” 2018; “Evaluating the effectiveness of deplatforming,” 2021; “Quarantined!,” 2022). Third, she has advanced <u>learning sciences and constructionist HCI</u> through seminal design-and-study work on networked learning environments for youth (e.g., “**MOOSE Crossing**,” 1997; “Community support for constructionist learning,” 1998; “Pianos not stereos,” 1996), extending these ideas to authentic writing and publishing in classrooms (“From Wikipedia to the classroom,” 2006; “Constructing text,” 2007). Interwoven throughout is a sustained emphasis on <u>research ethics in online settings</u>, including privacy, consent, and participant expectations (“Studying the amateur artist,” 2002; “Go away,” 2004; “Ethical guidelines for research online,” 2002; “Psychological research online,” 2004).\n\n---\n\n## Notable Works\n- **Ethics and methods for internet research**: Through “**Psychological research online**” (2004) and complementary work such as “**Studying the amateur artist**” (2002) and “**‘Go away’: Participant objections…**” (2004), Bruckman helped define rigorous, human-subjects-centered approaches to online research, clarifying risks of harm, expectations of privacy, and responsible data handling in digital fieldwork.  \n- **Peer production and Wikipedia participation/governance**: In “**Becoming Wikipedian**” (2005), alongside “Why do people write for Wikipedia?” (2005) and “**Decentralization in Wikipedia governance**” (2009), she mapped how contributors’ roles and motivations evolve and how “self-governance” is practically produced—linking individual trajectories to emergent institutional structures.  \n- **Platform moderation, norms, and human–machine governance**: With large-scale and mixed-method studies including “**The Internet’s hidden rules**” (2018), “**Human-machine collaboration for content regulation: The case of reddit automoderator**” (2019), and “Does transparency in moderation really matter?” (2019), she advanced empirical understanding of how norms are articulated, violated, and enforced, and how automated tools reshape moderator labor and user experience.\n\n---\n\n## Academic Background\nBruckman’s publication record traces a long arc from early 1990s research on text-based virtual reality and professional community (e.g., the **MediaMOO** line of work in 1993–1995) through influential late-1990s contributions to **constructionist learning** and youth-oriented HCI (e.g., “MOOSE Crossing,” 1997; “Community support for constructionist learning,” 1998), and into the 2000s–2020s as a leading voice in **social computing**, **peer production**, and the **ethics of online research** (e.g., “Ethical guidelines for research online,” 2002; “Psychological research online,” 2004). Her sustained impact—reflected in a high-citation corpus (19,469 citations) and recurring engagement with major venues and communities spanning HCI, CSCW, and learning sciences—aligns with her senior appointment at Georgia Tech as a **Regents’ Professor**. The breadth of her collaborations and topics (from Wikipedia governance to harassment mitigation and deplatforming) also indicates deep affiliation with interdisciplinary research networks that connect computing to social science, education, and public-interest technology."}, "Ihyz20wAAAAJ": {"keywords": "Human-Robot Interaction, Explainable Artificial Intelligence, Multi-Agent Reinforcement Learning, Human-Robot Teaming, Trust and Accountability in HRI, Mixed-Initiative Systems, Multi-Robot Task Scheduling, Learning from Demonstration, Inverse Reinforcement Learning, Graph Attention Networks for Multi-Agent Communication", "summary": "## Overview\nMatthew Gombolay is an **Associate Professor** at the **Georgia Institute of Technology**, where he leads a research program at the intersection of *human-centered autonomy* and *learning-enabled robotics*. His work focuses on designing, learning, and evaluating methods for **Human–Robot Interaction (HRI)** and **Human–AI teaming** that improve performance while safeguarding human outcomes such as trust, satisfaction, and workload. Across application domains ranging from manufacturing and healthcare to wildfire response and autonomous driving, his lab advances <u>mixed-initiative decision-making</u>, <u>explainable artificial intelligence</u>, and <u>multi-agent coordination</u>—with an emphasis on measurable impacts in real-world sociotechnical settings and a scholarly footprint exceeding **5,600 citations**.\n\n---\n\n## Research Areas\nGombolay’s research spans three tightly coupled areas: (1) **human-robot teaming and human factors**, (2) **explainable and interpretable learning for decision-making**, and (3) **multi-agent reinforcement learning and coordination**. In HRI, he has examined how autonomy allocation and decision authority shape team outcomes, including efficiency and worker satisfaction in mixed human–robot teams (e.g., *“Decision-making authority, team efficiency and human worker satisfaction in mixed human–robot teams,”* 2015), and how robot framing and responsibility cues influence trust and reliance (e.g., *“Effects of anthropomorphism and accountability on trust in human robot interaction,”* 2020; *“Robotic assistance in the coordination of patient care,”* 2018). Complementing these empirical studies, his work develops computational approaches for *mixed-initiative teaming* that explicitly model human factors—situational awareness, workload, and workflow preferences (e.g., *“Computational design of mixed-initiative human–robot teaming that considers human factors,”* 2017; *“Coordination of Human-Robot Teaming with Human Task Preferences,”* 2015). On the learning and autonomy side, he has advanced interpretable reinforcement learning, including differentiable decision trees optimized for online RL (e.g., *“Optimization Methods for Interpretable Differentiable Decision Trees in Reinforcement Learning,”* 2020) and broader evaluations of how XAI affects human-agent interaction both objectively and subjectively (e.g., *“Explainable Artificial Intelligence: Evaluating the Objective and Subjective Impacts of XAI on Human-Agent Interaction,”* 2023; *“The Utility of Explainable AI (XAI) in Ad Hoc Human-Machine Teaming,”* 2021). In multi-agent systems, his group has contributed methods for scalable coordination and communication—especially graph-attention-based approaches to scheduling and teamwork (e.g., *“Multi-Agent Graph-Attention Communication and Teaming,”* 2021; *“Learning scheduling policies for multi-robot coordination with graph attention networks,”* 2020; *“Heterogeneous graph attention networks for scalable multi-robot scheduling,”* 2021). These algorithmic efforts are frequently grounded in high-stakes applications such as wildfire monitoring with UAV teams (e.g., *“Coordinated Control of UAVs for Human-centered Active Sensing of Wildfires,”* 2020; *“Multi-UAV planning for cooperative wildfire coverage…,”* 2022), and are complemented by methodological contributions to research rigor in HRI measurement (e.g., *“Four years in review: Statistical practices of likert scales in human-robot interaction studies,”* 2020; *“Concerning trends in likert scale usage…,”* 2023) and work on bias and societal impacts of AI (e.g., *“Robots enact malignant stereotypes,”* 2022; *“Towards a comprehensive understanding… of societal biases in pre-trained transformers,”* 2021).\n\n---\n\n## Notable Works\n- **Human decision authority in mixed human–robot teams:** In *“Decision-making authority, team efficiency and human worker satisfaction in mixed human–robot teams”* (2015), Gombolay empirically characterized how autonomy and authority allocation affect both operational performance and human satisfaction—helping establish a foundational evidence base for <u>mixed-initiative</u> team design in manufacturing-like settings.\n\n- **Trust, anthropomorphism, and accountability in HRI:** In *“Effects of anthropomorphism and accountability on trust in human robot interaction”* (2020), he clarified how perceived anthropomorphism, the robot’s role in decision support, and accountability cues shape trust and dependence—informing the design of socially situated, decision-support robots and broader human-agent interaction.\n\n- **Graph-attention communication for multi-agent reinforcement learning:** In *“Multi-Agent Graph-Attention Communication and Teaming”* (2021), he introduced learning mechanisms for deciding *when*, *with whom*, and *how* agents communicate, advancing <u>communication-efficient</u> coordination strategies that improve team performance while managing communication overhead.\n\n---\n\n## Academic Background\nAt Georgia Tech, Gombolay has developed an internationally visible research portfolio spanning HRI, multi-agent learning, and explainable/interpretable autonomy, reflected in sustained publication activity from early work on temporospatially constrained multi-robot scheduling (e.g., *“Fast scheduling of multi-robot teams with temporospatial constraints,”* 2013) through contemporary contributions to XAI evaluation, bias in AI, and generative design tools. His scholarly trajectory suggests a consistent emphasis on *deployable autonomy*—pairing algorithmic advances (e.g., graph-attention scheduling and communication; interpretable RL via differentiable trees) with rigorous human-subject experimentation and methodological critiques that strengthen empirical standards in HRI (notably his widely cited analyses of Likert-scale practices). His collaborations and application-driven studies—spanning healthcare coordination, manufacturing teamwork, UAV wildfire sensing, and personalized autonomy—indicate strong interdisciplinary affiliations across robotics, machine learning, and human factors communities, positioning him as a leading contributor to <u>human-centered AI and robotics</u> within the Georgia Tech ecosystem and the broader HRI/AI research landscape."}, "J1tqbNAAAAAJ": {"keywords": "embedded and pervasive systems, parallel and distributed systems, computer architecture, operating systems, fog and edge computing, Internet of Things (IoT), serverless computing at the edge, distributed data stores and geo-replication, complex event processing (CEP), distributed shared memory (DSM) and cache coherence", "summary": "## Overview\nUmakishore Ramachandran is a Professor of Computer Science at the Georgia Institute of Technology, where he leads research at the intersection of **embedded and pervasive systems**, **parallel and distributed systems**, **computer architecture**, and **operating systems**. His work centers on building *practical, scalable system software* that brings computation and data management closer to where data is generated, emphasizing <u>edge/fog computing</u>, <u>stream-oriented and event-driven processing</u>, and <u>distributed systems abstractions</u> for real-time, geo-distributed applications. Across decades of contributions—from early distributed OS and shared memory foundations to contemporary IoT and serverless-at-the-edge platforms—his research has shaped how developers program, deploy, and manage **latency-sensitive** applications in heterogeneous, distributed environments.\n\n---\n\n## Research Areas\nRamachandran’s research portfolio spans foundational and applied systems work, unified by a focus on *abstraction, performance, and programmability* in distributed computing. In **fog and edge computing for IoT**, he introduced programming and deployment models that treat the network edge as a first-class execution substrate, notably in “**Mobile fog: A programming model for large-scale applications on the internet of things**” (2013), which articulates how geo-spatially distributed devices and resources can be orchestrated for large-scale IoT applications. This line continues with systems support for <u>geo-distributed situation awareness</u>, including incremental deployment and migration techniques in “**Incremental deployment and migration of geo-distributed situation awareness applications in the fog**” (2016), where the emphasis is on elastic placement under mobility, burstiness, and latency constraints. Complementing orchestration, he has advanced **edge-native execution paradigms**, such as “**An execution model for serverless functions at the edge**” (2019), which adapts serverless semantics to resource-scarce, distributed edge settings. A second major theme is **distributed data management and state** for edge pipelines: “**Fogstore: Toward a distributed data store for fog computing**” (2017) and “**Fogstore: A geo-distributed key-value store guaranteeing low latency for strongly consistent access**” (2018) explore relevance-aware, low-latency, strongly consistent storage suitable for event-driven fog applications, while broader platforms such as “**{DataFog}: Towards a Holistic Data Management Platform for the {IoT} Age at the Network Edge**” (2018) point toward end-to-end data lifecycles at the edge. A third theme is **distributed complex event processing (CEP) and stream systems** under mobility and failure, developed through work such as “**Migcep: Operator migration for mobility driven distributed complex event processing**” (2013), “**MCEP: A mobility-aware complex event processing system**” (2014), and reliability-oriented designs like “**Rollback-recovery without checkpoints in distributed event processing systems**” (2013). Underpinning these modern systems are earlier foundational contributions to **distributed operating systems** and **distributed shared memory**, including the “**Clouds distributed operating system**” (2002) and the earlier “**Design and implementation of the clouds distributed operating system**” (1990), as well as shared-memory abstractions and coherence work such as “**An implementation of distributed shared memory**” (1991) and “**Coherence of distributed shared memory—Unifying synchronization and data transfer**” (1989). Together, these threads reflect a sustained agenda: designing system architectures that reconcile *programmability* with *distributed reality*—heterogeneity, mobility, partial failure, and wide-area latency.\n\n---\n\n## Notable Works\n- **Mobile Fog programming for IoT at scale** — In “*Mobile fog: A programming model for large-scale applications on the internet of things*” (2013), Ramachandran advanced a high-level model for orchestrating computation across mobile, sensor, and nearby resources, helping establish <u>fog computing</u> as a programmable substrate for large-scale, latency-sensitive IoT applications.\n- **A distributed OS vision and implementation (Clouds)** — Through “*The Clouds distributed operating system*” (2002) and the earlier “*Design and implementation of the clouds distributed operating system*” (1990), he developed an object/thread-based distributed OS architecture that emphasized location transparency and protected invocation—work that influenced later thinking about system structuring in distributed environments.\n- **Mobility-aware distributed event processing and migration** — In “*Migcep: Operator migration for mobility driven distributed complex event processing*” (2013) and “*MCEP: A mobility-aware complex event processing system*” (2014), he introduced mechanisms for adapting CEP operator placement to client mobility, reducing end-to-end latency for real-time, sensor-driven applications.\n\n---\n\n## Academic Background\nAs a long-standing faculty member at the Georgia Institute of Technology, Ramachandran has built an academic career spanning multiple waves of systems research, evidenced by a publication trajectory that begins with **computer architecture**, **multiprocessor caching**, and **distributed shared memory** in the late 1980s and early 1990s (e.g., coherence and DSM implementation work), expands into **distributed operating systems** via the Clouds project (1990s–early 2000s), and later transitions into **ubiquitous/pervasive computing**, **sensor networks**, and **middleware interoperability** (e.g., uMiddle and RFID reliability frameworks). In the 2010s and beyond, his work has been strongly identified with <u>fog/edge computing</u>, <u>geo-distributed situation awareness</u>, and <u>serverless and data platforms at the edge</u>, including emulation infrastructure (“*Emufog*,” 2017) and edge control-plane research (“*Oneedge*,” 2021). With a substantial citation record (6,423 total citations) and highly cited contributions across decades, his scholarly impact reflects sustained leadership and cross-community influence, bridging foundational systems principles with emerging application domains in IoT and edge intelligence."}, "J5D4kcoAAAAJ": {"keywords": "Robot Learning, Computer Vision, 3D Object Reconstruction, Scene Graph Generation, 6D Object Pose Estimation, RGB-D Sensor Fusion, Imitation Learning from Human Demonstrations, Long-Horizon Task Planning, Task and Motion Planning (TAMP), Large Language Models for Robotics", "summary": "## Overview\nDanfei Xu is an Assistant Professor in the School of Interactive Computing, where she leads research at the intersection of **robot learning** and **computer vision** with an emphasis on *scalable, generalizable autonomy*. Her work develops learning- and model-based methods that connect perception to action for real-world robots, spanning manipulation and embodied decision-making. Across this agenda, she has advanced <u>learning from demonstrations</u>, <u>long-horizon task planning</u>, and <u>3D/scene understanding</u> as core building blocks for robots that can operate robustly in complex environments, reflected in a highly cited body of work (over **14,646** citations).\n\n---\n\n## Research Areas\nXu’s research centers on representation learning and decision-making for embodied agents, with contributions that bridge visual understanding, geometric perception, and robot policy learning. In 3D vision, she co-developed **3D-R2N2** (“*3d-r2n2: A unified approach for single and multi-view 3d object reconstruction*,” 2016), introducing a recurrent architecture that integrates multiple views to infer 3D shape, and she has contributed to multimodal 3D perception via **PointFusion** (“*Pointfusion: Deep sensor fusion for 3d bounding box estimation*,” 2018) and RGB-D pose estimation with **DenseFusion** (“*Densefusion: 6d object pose estimation by iterative dense fusion*,” 2019), emphasizing iterative fusion of complementary sensing modalities for cluttered scenes. In structured scene understanding, her work on “*Scene graph generation by iterative message passing*” (2017) helped establish iterative relational reasoning as a practical approach to predicting object relationships and enabling richer semantic interpretations of images. On the robotics side, Xu has advanced <u>imitation learning</u> and <u>planning for long-horizon manipulation</u>, including systematic analysis of offline demonstration learning (“*What matters in learning from offline human demonstrations for robot manipulation*,” 2021), hierarchical and compositional task generalization (“*Neural task programming*,” 2018; “*Neural task graphs*,” 2019), and language-augmented planning with large models (“*ProgPrompt: Generating situated robot task plans using large language models*,” 2022/2023). More recently, she has contributed to data and model scaling for robotics through community-scale resources (“*Open X-Embodiment: Robotic learning datasets and RT-X models*,” 2023), reflecting a sustained focus on *generalization across tasks, embodiments, and environments*.\n\n---\n\n## Notable Works\n- **3D reconstruction with learned shape priors and multi-view integration:** In “*3d-r2n2: A unified approach for single and multi-view 3d object reconstruction*” (2016), Xu helped introduce a recurrent reconstruction framework that unifies single- and multi-view inference, enabling robust 3D object reconstruction by aggregating evidence across views.  \n- **Relational visual understanding via structured message passing:** “*Scene graph generation by iterative message passing*” (2017) advanced the use of iterative reasoning to jointly infer objects and their relationships, strengthening scene graph generation as a foundation for higher-level visual semantics.  \n- **Scaling robot learning through data, models, and situated planning:** Xu contributed to major steps toward scalable embodied intelligence, including “*Open X-Embodiment: Robotic learning datasets and RT-X models*” (2023) for cross-institution, multi-embodiment learning, and “*ProgPrompt: Generating situated robot task plans using large language models*” (2022) for integrating LLMs into task planning in grounded robotic settings.\n\n---\n\n## Academic Background\nXu’s publication trajectory indicates a research formation grounded in **computer vision**, **robotics**, and **machine learning**, with early work addressing tactile perception and active exploration (“*Tactile identification of objects using Bayesian exploration*,” 2013) and model-based manipulation of deformable objects (e.g., “*Folding deformable objects using predictive simulation and trajectory optimization*,” 2015; “*Regrasping and unfolding of garments using predictive thin shell modeling*,” 2015). Her subsequent highly cited contributions in 3D reconstruction, scene graph reasoning, and RGB-D perception (2016–2019) reflect deep engagement with top-tier vision and robotics communities and collaborations spanning perception and embodied intelligence. As her agenda expanded into *learning from demonstrations*, *hierarchical task generalization*, and *language- and model-driven planning* (2019–present), she has maintained strong cross-disciplinary affiliations consistent with an Interactive Computing faculty role, contributing both methodological work and field-shaping resources—most notably large-scale datasets and models for robotics (“*Open X-Embodiment*,” 2023). The breadth and impact of her work, evidenced by substantial citation influence across vision and robotics, align with a record of sustained recognition and leadership within the research communities she serves."}, "JvY2c4YAAAAJ": {"keywords": "Computer Security, Cyber-Crime, Censorship, Measurement, DNS manipulation, Internet-wide scanning, IPv6 address discovery, Online advertising fraud (click fraud), Threat intelligence, Malware and botnet analysis", "summary": "## Overview\nPaul Pearce is an Assistant Professor at **Georgia Tech**, where he leads a research agenda at the intersection of **computer security** and *large-scale Internet measurement*, with a particular emphasis on <u>cyber-crime ecosystems</u>, <u>censorship and network interference</u>, and <u>DNS- and protocol-level security</u>. His work is characterized by **empirical, data-driven methodologies** that combine active measurement, longitudinal analysis, and systems building to reveal how real-world adversaries—ranging from monetization-driven criminals to censorship operators—shape the modern Internet. Across projects spanning Android ad libraries, click-fraud botnets, DNS manipulation, Tor discrimination, and IPv6 scanning, Pearce’s scholarship uses *measurement as a lens* for understanding and improving security at Internet scale.\n\n---\n\n## Research Areas\nPearce’s research centers on understanding security and abuse through rigorous measurement and systems analysis. In mobile and advertising security, his influential work on Android’s ad ecosystem—most notably **“AdDroid: Privilege Separation for Applications and Advertisers in Android” (2012)**—interrogates how bundling third-party advertising libraries into apps collapses security boundaries, motivating architectural approaches to isolate privileges and reduce risk. Complementing this systems perspective, his studies of ad-fraud and manipulation quantify criminal profit mechanisms and the technical means by which they persist, including **“What’s Clicking What? Techniques and Innovations of Today’s Clickbots.” (2011)**, **“Characterizing Large-Scale Click Fraud in ZeroAccess” (2014)**, and **“Ad injection at scale: Assessing deceptive advertisement modifications” (2015)**, which collectively illuminate the operational pipelines behind click-fraud, search hijacking, and browser/session tampering. A second major thrust is <u>censorship measurement</u> and network interference, where Pearce has advanced global, repeatable methodologies for detecting manipulation—especially in the DNS—through **“Global Measurement of DNS Manipulation” (2017)** and the Augur line of work (e.g., **“Augur: Internet-Wide Detection of Connectivity Disruptions” (2017)**; **“Toward Continual Measurement of Global Network-Level Censorship” (2018)**), as well as methodological refinements such as **“Many Roads Lead To Rome: How Packet Headers Influence DNS Censorship Measurement” (2022)** and studies of instability and routing effects (e.g., **“Understanding Routing-Induced Censorship Changes Globally” (2024)**). He has also examined how security practices and discrimination shape access and risk, including **Tor exit blocking** (2017) and the reliability of industry “threat intelligence” in **“Reading the Tea Leaves: A Comparative Analysis of Threat Intelligence” (2019)**. More recently, his measurement program extends to <u>IPv6 security and Internet-wide scanning</u>, contributing tools and analyses such as **ZDNS** (2022), **6SENSE** (2024), and multiple studies on IPv6 discovery, scanning strategies, NAT64 deployment, and aliasing (2023–2025), reflecting a sustained effort to make Internet-scale measurement more reproducible, interpretable, and actionable.\n\n---\n\n## Notable Works\n- **AdDroid: Privilege Separation for Applications and Advertisers in Android (2012)** — A foundational contribution to mobile security that identified how embedded third-party ad libraries blur trust boundaries and motivated **privilege separation** designs to isolate advertisers from app privileges while preserving the ad-supported ecosystem.\n- **Global Measurement of DNS Manipulation / Global-Scale Measurement of DNS Manipulation (2017)** — A widely cited measurement study that advanced <u>global censorship and manipulation detection</u> by systematically characterizing DNS interference at scale, helping establish DNS-based measurement as a core tool for tracking censorship practices longitudinally.\n- **Reading the Tea Leaves: A Comparative Analysis of Threat Intelligence (2019)** — A data-driven assessment of commercial and community threat intelligence that scrutinized the *coverage, agreement, and operational value* of shared indicators (e.g., malicious domains/IPs), clarifying the limits of “threat intel” as a defensive primitive.\n\n---\n\n## Academic Background\nPearce’s publication record reflects a trajectory from **systems and OS research** into **security measurement and cyber-crime**, beginning with early work on resource isolation in manycore operating systems (e.g., **“Resource management in the Tessellation manycore OS” (2010)**) and then pivoting to Internet abuse, advertising fraud, and adversarial ecosystems (2011–2015). His subsequent scholarship consolidated around **Internet-scale security measurement**, producing a sustained series of highly cited works on DNS manipulation, censorship detection, and connectivity disruptions (2017–2019), alongside studies of attacker tooling and human-in-the-loop abuse (e.g., DarkComet RAT operator monitoring and stakeholder profiling, 2017–2018). As an academic leader at Georgia Tech, he has also contributed community-facing measurement infrastructure—most notably **ZDNS (2022)**—supporting reproducibility and broader adoption of high-performance DNS measurement. The breadth and continuity of his research program, together with a citation profile exceeding **1,900** citations, indicate strong influence across *mobile security*, *cyber-crime measurement*, and <u>censorship and network interference</u>, with recent work extending these themes into **IPv6 measurement and scanning methodology** (2023–2025) and privacy/security risks in modern web platforms (e.g., browser extensions, 2024)."}, "KY-eoQkAAAAJ": {"keywords": "Cybersecurity, Computer architecture, Machine learning, Cyber-physical systems (CPS), Vulnerability localization, Graph neural networks (GNNs), Binary analysis and reverse engineering, Secure processor design, Resilient control and safety guarantees, Trusted execution environments (TEE) for GPUs", "summary": "## Overview\nJ. Sukarno Mertoguno is a researcher at the **Georgia Institute of Technology**, where his work spans the intersection of **cybersecurity**, **computer architecture**, and **machine learning** with a sustained emphasis on *safety- and mission-critical* <u>cyber-physical systems (CPS)</u>. Across multiple decades of publications and inventions, he has advanced both hardware- and software-centric approaches to security: from *functional-level* processor architectures for accelerating complex content-service operations (e.g., file-access lookups) to modern graph- and learning-based methods for <u>vulnerability localization</u> and resilient CPS design under adversarial conditions.\n\n---\n\n## Research Areas\nMertoguno’s research portfolio integrates (1) **specialized instruction-set and microarchitectural support** for high-level operations, (2) **secure and observable computing substrates**, and (3) **AI-enabled software and CPS security**. Early, highly cited work on application-layer acceleration proposed processor mechanisms that elevate “lookup” and content-service operations into *native* execution, exemplified by the three-level lookup-cache concept for variable-length keys in “**Native lookup instruction for file-access processor searching a three-level lookup cache for variable-length keys**” (2006) and the broader *functional-level instruction-set computing* vision in “**Functional-level instruction-set computer architecture for processing application-layer content-service requests such as file-access requests**” (2007; and the later 2011 version). This architectural trajectory is complemented by security-oriented processor ideas, including biometric-aware secure processing in “**SCAN secure processor and its biometric capabilities**” (2011) and instruction-level observability/control in “**Cognizant engines: systems and methods for enabling program observability and controlability at instruction level granularity**” (2020). In parallel, his recent work reflects a strong shift toward **machine-learning-driven security analytics** and **CPS resilience**, including graph-based program representations for pinpointing vulnerable lines of code in “**{VulChecker}: Graph-based vulnerability localization in source code**” (2023), as well as physics- and timing-grounded resilience frameworks for safety-critical CPS in “**A physics‐based strategy for cyber resilience of CPS**” (2019) and “**A timing-based framework for designing resilient cyber-physical systems under safety constraint**” (2023). Additional threads—such as NLP for binary/ISA analysis (“**A natural language processing approach for instruction set architecture identification**,” 2023) and multimodal understanding of technical documents (2020–2021)—underscore a broader interest in *knowledge extraction* and *automation* for security and engineering workflows.\n\n---\n\n## Notable Works\n- **Instruction-set and microarchitectural acceleration for application-layer services:** Proposed a *native* lookup instruction and a three-level lookup-cache strategy for variable-length keys in “**Native lookup instruction for file-access processor searching a three-level lookup cache for variable-length keys**” (2006), establishing a hardware pathway for efficient content-service and file-access processing.\n\n- **Functional-level instruction-set computing for content-service requests:** Developed the **functional-level instruction-set computer architecture** paradigm for processing application-layer requests—especially file-access protocols—in “**Functional-level instruction-set computer architecture for processing application-layer content-service requests such as file-access requests**” (2007; extended in 2011), advancing the idea that higher-level operations (e.g., lookups/bit-compares over variable-length operands) can be first-class architectural primitives.\n\n- **Graph-based vulnerability localization in source code:** Introduced a graph-centric deep-learning approach that goes beyond project-level classification to localize vulnerabilities at fine granularity in “**{VulChecker}: Graph-based vulnerability localization in source code**” (2023), addressing a key practical gap in ML-based secure software development.\n\n---\n\n## Academic Background\nAffiliated with the **Georgia Institute of Technology**, Mertoguno’s publication record suggests a long-standing academic and research trajectory beginning with early work in **computer vision hardware**, **neuromorphic/multilayer image-understanding architectures** (the KYDON and Hermes lines of research in the 1990s), and **VLSI reverse engineering** (e.g., the knowledge-based approach for automatic visual VLSI reverse-engineering in 2002). His mid-career contributions increasingly emphasized **processor architecture** and **instruction-set innovation**, including patented and invention-oriented work on functional-level architectures and secure processors, before expanding into **cybersecurity**, **autonomic cyber systems** (2014), and **CPS resilience** (2018–2024). The breadth from ASIC/processor design to modern ML- and graph-based security methods, together with a citation footprint of **701** total citations, indicates sustained influence across multiple communities—spanning architecture, security, and CPS—along with collaborations consistent with large, interdisciplinary research programs typical of a major technical institution."}, "LAuxzv0AAAAJ": {"keywords": "computational behavior science, artificial intelligence, autism spectrum disorder, mild cognitive impairment, digital health, early autism detection, social attention and eye gaze tracking, multimodal behavioral sensing, wearable physiological sensors (EDA/accelerometry), telehealth and remote diagnosis", "summary": "## Overview\nAgata Rozga is a researcher at the Georgia Institute of Technology’s School of Interactive Computing, where she advances **computational behavior science** at the intersection of **AI**, *developmental psychology*, and digital health. Her work centers on <u>objective, technology-enabled measurement of social behavior</u>—especially in early development and clinical contexts—combining longitudinal behavioral science with multimodal sensing (e.g., video, audio, wearables) and machine learning to improve *screening, assessment, and support* for autism and related conditions, and more recently to inform technology design for aging and cognitive health (e.g., **MCI**).\n\n---\n\n## Research Areas\nRozga’s research spans (1) <u>early identification and developmental trajectories in autism</u>, (2) <u>computational sensing and modeling of dyadic social interaction</u>, and (3) <u>AI-enabled health assessment and support in naturalistic settings</u>. In foundational prospective studies, she helped characterize early-emerging behavioral markers—such as diminished **response to name** and atypical social-communication profiles—by tracking infants at elevated likelihood for autism and comparing developmental outcomes (e.g., “A prospective study of response to name in infants at risk for autism,” 2007; “A prospective study of the emergence of early behavioral signs of autism,” 2010; “Behavioral profiles of affected and unaffected siblings…,” 2011). Building on this developmental science base, she has been a key contributor to *behavioral imaging* approaches that translate clinical constructs (eye contact, engagement, joint attention) into measurable signals using computer vision, audio analysis, and wearable sensors (e.g., “Decoding children’s social behavior,” 2013; “Detecting eye contact using wearable eye-tracking glasses,” 2012; “Using electrodermal activity to recognize ease of engagement…,” 2014). Her later work extends these methods toward generalizable attention and gaze estimation in realistic scenes (e.g., “Connecting gaze, scene, and attention…,” 2018) and toward scalable, home- and telehealth-oriented assessment workflows (e.g., “Investigating the accuracy of a novel telehealth diagnostic approach…,” 2017; “A novel system for supporting autism diagnosis using home videos…,” 2015). More recently, she has engaged questions at the boundary of **conversational AI**, caregiving networks, and cognitive health—highlighting limitations of current agent systems and the need for explainability and coordination support for older adults with **MCI** (e.g., “I don’t know how to help with that…,” 2023; “Why Did You Say That?…,” 2023; “Ai-Caring…,” 2024).\n\n---\n\n## Notable Works\n- **Prospective behavioral markers of autism in infancy:** Through large-impact longitudinal work, Rozga helped establish early behavioral signs and their developmental timing, including reduced social orienting and communication indicators, in “**A prospective study of the emergence of early behavioral signs of autism**” (2010).\n- **Response-to-name as an early screening signal:** She contributed evidence on the sensitivity/specificity and developmental meaning of decreased response to name as an early risk marker in “**A prospective study of response to name in infants at risk for autism**” (2007).\n- **Computational measurement of social behavior from multimodal data:** Rozga helped define and operationalize a new activity-recognition domain for clinical interactions in “**Decoding children’s social behavior**” (2013), complementing later work on attention estimation and gaze/scene modeling (e.g., “Connecting gaze, scene, and attention…,” 2018) and scalable measurement of eye contact in natural interactions.\n\n---\n\n## Academic Background\nRozga’s scholarly trajectory reflects an interdisciplinary foundation bridging *developmental and clinical science* with **interactive computing** and **AI**, culminating in a research program housed at Georgia Tech’s School of Interactive Computing. Her publication record indicates sustained collaboration across psychology, psychiatry, computer science, and health-technology communities, with early highly cited contributions to autism developmental science (including longitudinal infant-sibling and high-risk cohort studies) and subsequent leadership in computational approaches to behavioral assessment (e.g., wearable sensing, computer vision for gaze/eye contact, and multimodal engagement modeling). Her influence is reflected in substantial scholarly uptake (over **5,262 citations**), and her later affiliations and outputs suggest expanding engagement with digital health infrastructure and responsible AI for caregiving and cognitive health (e.g., work on telehealth diagnostics, home-video-based assessment systems, and AI support for networks caring for individuals with **MCI**)."}, "Lde9BAgAAAAJ": {"keywords": "International security, cybersecurity, military power, evolution, cyber deterrence, cyber deception and attribution, cyber espionage and intelligence, cross-domain deterrence, nuclear command and control cybersecurity, military artificial intelligence and human judgment", "summary": "## Overview\nJon R. Lindsay is an Associate Professor of International Relations at the Georgia Institute of Technology, where he conducts research at the intersection of **international security** and **technology** with a particular emphasis on *cybersecurity and military power*. His scholarship examines how <u>information technologies</u> reshape strategic interaction—often by expanding opportunities for **deception**, **intelligence collection**, and organizational friction rather than by delivering clean, decisive forms of “cyber war.” Across widely cited work on cyber operations, deterrence, and emerging technologies such as AI and quantum computing, Lindsay’s research highlights how *complex socio-technical systems* mediate the political utility of force and the limits of state control.\n\n---\n\n## Research Areas\nLindsay’s research program spans cyber conflict, deterrence theory, and the political consequences of technological change in military organizations. A central theme is the claim that cyberspace is best understood as an arena of <u>secret statecraft</u>—where espionage, subversion, and influence are more common than catastrophic disruption—an argument developed through landmark analysis of **Stuxnet** in “Stuxnet and the limits of cyber warfare” (2013) and extended in his work on cyber operations as intelligence contests (e.g., “Weaving tangled webs: Offense, defense, and deception in cyberspace,” 2015; “Tipping the scales: the attribution problem and the feasibility of deterrence against cyberattack,” 2015). He has also advanced debates on strategic stability under technological complexity, especially where cyber capabilities interact with other instruments of power, as in “Thermonuclear cyberwar” (2017) and “Cross-domain deterrence: Strategy in an era of complexity” (2019). Complementing these strategic studies, Lindsay analyzes how states and organizations actually generate military advantage from information systems—emphasizing <u>information friction</u>, institutional constraints, and the micro-foundations of performance (e.g., “Information technology and military power,” 2020; earlier work on user innovation in military software). More recently, he has contributed to security scholarship on AI and decision-making (“Prediction and judgment: Why artificial intelligence increases the importance of humans in war,” 2021) and on the political logic of cryptology and quantum risk (“Demystifying the quantum threat,” 2020), while also interrogating how commercial threat intelligence can bias what scholars and policy communities “see” (“A tale of two cybers—how threat reporting… underrepresents threats to civil society,” 2021). Regionally, his work on China and cybersecurity (“The impact of China on cybersecurity: Fiction and friction,” 2014; “China and cybersecurity: Espionage, strategy, and politics in the digital domain,” 2015) connects technological competition to political economy, escalation dynamics, and the narratives that drive mistrust.\n\n---\n\n## Notable Works\n- **Reframing cyber conflict through the Stuxnet case**: In “**Stuxnet and the limits of cyber warfare**” (2013), Lindsay uses the first widely known cross-border cyber operation to cause physical damage to argue that cyber-physical sabotage is exceptional, operationally demanding, and politically constrained—challenging sweeping claims about a new decisive form of warfare.\n- **Deception-centered theory of offense, defense, and attribution**: Through “**Weaving tangled webs: Offense, defense, and deception in cyberspace**” (2015) and “**Tipping the scales: the attribution problem and the feasibility of deterrence against cyberattack**” (2015), he develops a theory in which deception is a double-edged sword that shapes both attack and defense, complicating simple “offense-dominant” narratives and refining how deterrence can (and cannot) function under uncertainty.\n- **Cross-domain deterrence under complexity**: In “**Cross-domain deterrence: Strategy in an era of complexity**” (2019), Lindsay extends deterrence theory beyond the nuclear dyad to a multi-domain environment—integrating cyber, space, autonomous systems, and conventional forces—to show how <u>strategic interaction</u> is conditioned by interdependence, institutional constraints, and escalation pathways across domains.\n\n---\n\n## Academic Background\nBased at the Georgia Institute of Technology as an Associate Professor, Lindsay has built an influential academic profile in international relations and security studies, evidenced by a substantial citation record (over 3,600 citations) and a publication trajectory spanning leading debates on cyber operations, deterrence, and military innovation. His early work on military software and user-driven innovation (e.g., “War upon the map” and related studies) foreshadowed a career-long focus on how organizations absorb and operationalize technology under bureaucratic and political constraints. Over time, his scholarship broadened from information technology and military performance to major contributions on cybersecurity, including high-impact analyses of Stuxnet, attribution and deterrence, and the strategic implications of U.S.–China cyber competition. His more recent work extends these foundations into adjacent frontiers—AI in war, quantum security, and the institutional politics of cyber commands and threat intelligence—reflecting affiliations and intellectual engagement with interdisciplinary communities in security studies, science and technology studies, and policy-relevant research on emerging technologies."}, "Lxv3W74AAAAJ": {"keywords": "Scientific Machine Learning, Large-scale Optimization, Numerical Methods, Inverse Design in Optics, Physics-informed Neural Networks, Topology Optimization, Metasurface Inverse Design, Meta-optics and Metalenses, Deep Neural Operators for PDE Surrogates, End-to-end Computational Imaging and Polarimetry", "summary": "## Overview\nRaphael Pestourie is an Assistant Professor in the School of Computational Science and Engineering at Georgia Tech, where he leads research at the intersection of **scientific machine learning**, **large-scale optimization**, and **computational electromagnetics**. His work centers on *algorithmic and numerical foundations* for <u>inverse design</u>—particularly in **nanophotonics and meta-optics**—with an emphasis on making optimization tractable at device scales that challenge conventional full-wave solvers. Across his publications, he advances *physics-aware learning* and *adjoint-driven* optimization frameworks that enable the practical design of complex optical systems, including large-area metasurfaces and end-to-end computational imaging pipelines.\n\n---\n\n## Research Areas\nPestourie’s research spans <u>physics-constrained learning</u>, <u>scalable topology optimization</u>, and <u>fast numerical methods</u> for PDE-governed design problems. A central theme is the integration of **physics-informed neural networks** with explicit feasibility guarantees, exemplified by “**Physics-informed neural networks with hard constraints for inverse design**” (2021), which addresses inverse design settings where satisfying boundary conditions and conservation laws is as important as minimizing an objective. In parallel, he has contributed foundational work on **large-area metasurface optimization**, including “**Inverse design of large-area metasurfaces**” (2018) and “**Topology optimization of freeform large-area metasurfaces**” (2019), targeting regimes with *10^5–10^6 degrees of freedom* and device diameters spanning *hundreds to thousands of wavelengths*. His group also develops **surrogate and operator-learning methods** to reduce reliance on expensive solvers, including “**Multifidelity deep neural operators…**” (2022), “**Active learning of deep surrogates for PDEs: application to metasurface design**” (2020), and “**Physics-enhanced deep surrogates for partial differential equations**” (2023), which collectively emphasize data efficiency, multifidelity training, and embedding physical structure into learned models. Application-driven work extends these methods to **meta-optics for imaging** and **virtual/augmented reality**, notably “**Inverse design enables large-scale high-performance meta-optics reshaping virtual reality**” (2022), and to *co-designed optical-computational systems* in “**End-to-end nanophotonic inverse design for imaging and polarimetry**” (2021) and “**End-to-end metasurface inverse design for single-shot multi-channel imaging**” (2022).\n\n---\n\n## Notable Works\n- **Hard-constraint scientific ML for inverse design:** In “*Physics-informed neural networks with hard constraints for inverse design*” (2021), Pestourie helped establish a framework for **PINNs** that enforce <u>hard physical constraints</u>, aligning learning-based inverse design with strict feasibility requirements common in engineering PDE problems.  \n- **Scalable inverse design of large-area meta-optics:** Through “*Inverse design of large-area metasurfaces*” (2018) and “*Topology optimization of freeform large-area metasurfaces*” (2019), he advanced **optimization and numerical strategies** enabling metasurface design at unprecedented scales, addressing the multiscale gap between subwavelength patterning and macroscopic apertures.  \n- **Large-scale meta-optics for real-world systems:** In “*Inverse design enables large-scale high-performance meta-optics reshaping virtual reality*” (2022), he contributed to translating inverse-designed meta-optics into system-level performance gains, demonstrating how <u>optimization-driven design</u> can overcome limitations of conventional forward design for complex, large-aperture functionality.\n\n---\n\n## Academic Background\nAs an Assistant Professor at Georgia Tech’s School of Computational Science and Engineering, Pestourie’s academic trajectory reflects sustained focus on **computational optimization** and **physics-based modeling** for photonics and broader PDE-constrained engineering design. His publication record—spanning foundational methods (e.g., hard-constraint PINNs, multifidelity neural operators, physics-enhanced surrogates) and high-impact applications (e.g., large-area metasurfaces, VR meta-optics, end-to-end nanophotonic imaging)—suggests deep engagement with interdisciplinary communities in *computational science*, *applied mathematics*, and *optics/photonics*. With approximately **2,910 citations**, his work has achieved substantial uptake across both scientific machine learning and nanophotonics, indicating broad influence and sustained collaboration networks consistent with major research hubs in computational electromagnetics and inverse design."}, "Lz6-_iIAAAAJ": {"keywords": "Machine Learning, Natural Language Processing, Artificial Intelligence, Privacy in Language Models, Membership Inference Attacks, Masked Language Models, Controlled Text Generation, Energy-Based Language Models, Differentiable Decoding (Beam Search & Scheduled Sampling), Low-Resource NLP & Named Entity Recognition", "summary": "## Overview\nKartik Goyal is a researcher at the Georgia Institute of Technology, where he works at the intersection of **Machine Learning** and **Natural Language Processing**, with an emphasis on *principled modeling and evaluation of modern language models*. His work centers on <u>understanding and controlling the behavior of neural sequence models</u>—from the privacy and memorization properties of **masked language models (MLMs)** to the design of *decoding- and energy-based* methods for controllable generation—while also contributing tools and analyses that connect NLP to linguistics and the digital humanities.\n\n---\n\n## Research Areas\nGoyal’s research spans several tightly connected themes in contemporary AI. A major thrust is <u>privacy and data leakage in language models</u>, exemplified by his 2022 study “**Quantifying privacy risks of masked language models using membership inference attacks**,” which systematically evaluates when and how MLMs reveal training-set membership—an issue made urgent by the deployment of language models on sensitive legal and medical corpora. Complementing this, he investigates the *foundations of masked modeling as a scoring and sampling paradigm*, including “**Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings**” (2021), which frames MLMs through an energy-based lens and studies whether they induce a coherent distribution over sequences. He also develops methods for <u>controllable text generation without costly fine-tuning</u>, as in “**Mix and match: Learning-free controllable text generation using energy language models**” (2022), which uses global scoring/energy formulations to steer generation. A further line of work addresses *training–inference mismatch and structured decoding* in neural sequence models, including “**Differentiable scheduled sampling for credit assignment**” (2017) and “**A continuous relaxation of beam search for end-to-end training of neural sequence models**” (2018), which propose differentiable approximations to discrete decoding to improve credit assignment. Beyond core LM methodology, his publications reflect sustained engagement with linguistically grounded representations and low-resource settings—e.g., “**Panphon: A resource for mapping IPA segments to articulatory feature vectors**” (2016) and work on NER and orthographic inference for Sorani Kurdish and Tajik (2016)—as well as computational approaches to historical texts and printing, including probabilistic and neural methods for early modern typographical analysis (2020–2024).\n\n---\n\n## Notable Works\n- **Privacy evaluation of masked LMs via membership inference**: “*Quantifying privacy risks of masked language models using membership inference attacks*” (2022) provides a quantitative framework for assessing <u>training-data privacy leakage</u> in widely deployed MLMs, helping clarify when membership inference is effective and what model behaviors correspond to elevated risk.\n- **Energy-based interpretation and sampling for MLMs**: “*Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings*” (2021) articulates how MLM scoring can be viewed as defining an implicit energy function, using MCMC-style sampling to probe whether MLMs behave like coherent probabilistic sequence models.\n- **Learning-free controllable generation with energy language models**: “*Mix and match: Learning-free controllable text generation using energy language models*” (2022) advances <u>controllable text generation</u> by combining global energy/score components to steer outputs without attribute-specific fine-tuning, broadening practical control mechanisms for large pretrained LMs.\n\n---\n\n## Academic Background\nAffiliated with the Georgia Institute of Technology, Goyal’s publication record indicates a trajectory from earlier work in *linguistically informed NLP and structured models*—including metaphor identification (“*Identifying metaphorical word use with tree kernels*,” 2013), distributional semantics for event and word meaning (2013–2014), and phonological feature representations (“*Panphon*,” 2016)—toward modern neural sequence modeling, decoding-aware training, and language-model evaluation (2017–2019). His later contributions emphasize <u>foundational questions about language model behavior</u>, including privacy, energy-based perspectives on MLMs, and controllable generation (2021–2022), alongside interdisciplinary collaborations in digital humanities and computational bibliography (2020–2024). With approximately **950 citations**, his work has achieved broad uptake across NLP subcommunities, reflecting sustained influence in both methodological machine learning for language and applied, linguistically grounded resource development and analysis."}, "Mm-dHpMAAAAJ": {"keywords": "Computer Architecture, Datacenter Systems, Memory Systems, Networking, Rack-Scale Computing, Scale-Out NUMA, Microsecond-Scale RPCs, SmartNICs and Network Interfaces, Persistent Memory (NVM/SCM) Durability, CXL Memory Pooling and Memory Expansion", "summary": "## Overview\nAlexandros Daglis is a researcher affiliated with the **University of Edinburgh** and the **Georgia Institute of Technology**, working at the intersection of **computer architecture** and *warehouse-scale systems*. His work centers on rethinking how servers, memory, and networks are co-designed to meet the stringent performance demands of modern online and data-intensive applications, with an emphasis on <u>rack-scale and scale-out architectures</u>, **low-latency RPC-centric datacenters**, and *memory system evolution* (from DRAM-centric designs to CXL-enabled disaggregation and persistent memory). Across this agenda, Daglis has consistently advanced <u>hardware–software co-design</u> that targets tail latency, bandwidth efficiency, and scalable resource composition in production-like datacenter environments.\n\n---\n\n## Research Areas\nDaglis’s research spans **datacenter systems**, **memory systems**, and **networking**, unified by a focus on *latency-critical, communication-heavy* workloads. A major theme is <u>distributed shared memory and rack-scale computing</u>, exemplified by *Scale-Out NUMA* (2014) and later work on *Scale-Out Non-Uniform Memory Access* (2017), which articulate architectures and programming/communication models that make remote memory access practical for in-memory services. Complementing this line, he has investigated load imbalance and skew in data serving—both diagnosing the phenomenon (*An Analysis of Load Imbalance in Scale-Out Data Serving*, 2016) and proposing rack-scale remedies (*The Case for RackOut*, 2016; *Mitigating Load Imbalance… with Rack-Scale Memory Pooling*, 2019). A second pillar of his portfolio addresses <u>µs-scale RPC performance and tail latency</u> in tiered services, where he explores NIC- and architecture-driven approaches to reduce the “RPC tax” and balance fine-grained fan-outs (e.g., *RPCValet*, 2019; *The NeBuLa RPC-Optimized Architecture*, 2020; *Cerebros*, 2021; and SmartNIC-enabled balancing in *Turbo*, 2023). A third thrust targets <u>next-generation memory hierarchies</u>, including design guidance for storage-class memory (*Design Guidelines for High-Performance SCM Hierarchies*, 2018), mechanisms for atomic durability on persistent memory (*Distributed Logless Atomic Durability with Persistent Memory*, 2019; *COSPlay*, 2021), and recent CXL-centric designs for scalable servers (*CoaXiaL*, 2024; *StarNUMA*, 2024; and subsequent work on CXL-enabled bandwidth harvesting, 2025). He has also contributed broader perspectives on efficiency and resilience, such as *Exploiting Errors for Efficiency: A Survey from Circuits to Applications* (2020), and has extended his systems interests into geo-distributed/edge control planes (*OneEdge*, 2021; *eCloud*, 2021) and emerging security considerations in memory systems (*Citadel*, 2025).\n\n---\n\n## Notable Works\n- **Scale-Out NUMA (2014)** — Introduced a foundational <u>scale-out NUMA</u> vision for datacenters running in-memory, communication-intensive applications, motivating architectures that treat remote memory access as a first-class performance path for key-value stores and graph workloads.  \n- **The Mondrian Data Engine (2017)** — Advanced a systems-and-architecture approach to sustaining data analytics performance under *post-Dennard* constraints, illustrating how re-architected execution and data movement can improve performance density for large-scale data processing.  \n- **RPCValet: NI-Driven Tail-Aware Balancing of µs-Scale RPCs (2019)** — Demonstrated how **network-interface-driven** scheduling and balancing can reduce <u>tail latency</u> for microsecond RPCs, directly targeting the fan-out and contention patterns that dominate modern multi-tier online services.\n\n---\n\n## Academic Background\nDaglis’s publication trajectory—spanning early, highly cited work on rack-scale memory and scale-out NUMA (2014–2016), followed by sustained contributions to RPC-centric datacenter architectures, persistent memory, and CXL-era memory expansion (2017–2025)—suggests a career shaped by close engagement with both **computer architecture** and *production-inspired datacenter systems research*. His dual affiliation with the **University of Edinburgh** and **Georgia Institute of Technology** indicates active collaboration across leading academic environments in systems and architecture, and his body of work reflects participation in major research communities bridging architecture, networking, and operating systems. With **1,241 citations** and multiple widely referenced papers (notably *Scale-Out NUMA*, *The Mondrian Data Engine*, and *RPCValet*), Daglis’s impact is marked by influential ideas in <u>rack-scale resource composition</u>, *tail-latency-aware RPC processing*, and **memory-system evolution**—areas that commonly attract competitive publication venues and sustained cross-institutional collaboration in the field."}, "N8RBFoAAAAAJ": {"keywords": "Computational Biology, Bioinformatics, Machine Learning, Drug-target interaction prediction, Computational drug repositioning, Graph neural networks, Protein function (enzyme) annotation, Protein engineering and fitness landscape modeling, Geometric deep learning for protein structure and binding affinity, Few-shot and self-supervised (contrastive) learning in biomedicine", "summary": "## Overview\nYunan Luo is a researcher at the Georgia Institute of Technology, where he develops **machine learning** and **computational biology** methods that translate large, heterogeneous biomedical datasets into actionable biological and therapeutic insights. His work centers on *data-efficient* and *generalizable* learning for biological systems, with particular emphasis on <u>network-based drug discovery</u>, <u>protein representation learning</u>, and <u>structure-aware modeling</u> for molecular function and interaction prediction. Across a highly cited body of work (3606 total citations), Luo has advanced algorithmic frameworks that integrate genomic, chemical, and pharmacological evidence and that leverage modern deep learning paradigms—especially contrastive learning and geometric deep learning—to address long-standing bottlenecks in annotation, engineering, and translational prediction.\n\n---\n\n## Research Areas\nLuo’s research spans several tightly connected areas at the interface of bioinformatics and machine learning. A major thrust is <u>heterogeneous network integration</u> for drug discovery and repositioning, exemplified by DTINet—“A network integration approach for drug-target interaction prediction and computational drug repositioning from heterogeneous information” (2017)—which models drug–target relationships by fusing multi-relational biomedical data into a unified predictive pipeline. Building on this translational emphasis, he has contributed integrative frameworks for urgent therapeutic discovery, including “An integrative drug repositioning framework discovered a potential therapeutic agent targeting COVID-19” (2021), which leverages multi-source evidence to prioritize candidate interventions for SARS-CoV-2. In parallel, Luo has made influential contributions to <u>protein function annotation</u> and <u>protein engineering</u> using modern representation learning: “Enzyme function prediction using contrastive learning” (2023) addresses the challenge of annotating less-studied proteins via contrastive objectives, while ECNet—“ECNet is an evolutionary context-integrated deep learning framework for protein engineering” (2021)—integrates evolutionary context to improve predictive modeling for sequence-to-function landscapes. His group also develops <u>geometric deep learning</u> methods for molecular interactions and mutational effects, including GeoPPI—“Deep geometric representations for modeling effects of mutations on protein-protein binding affinity” (2021)—which uses structure-derived geometric representations to estimate binding-affinity changes upon mutation. Complementing these biological applications, Luo’s publication record reflects broader methodological interests in robust learning under limited labels (e.g., “Few-shot learning creates predictive models of drug response that translate from high-throughput screens to individual patients,” 2021) and principled modeling (e.g., “When causal inference meets deep learning,” 2020), underscoring a persistent focus on *generalization across contexts* in biomedical prediction.\n\n---\n\n## Notable Works\n- **DTINet: Heterogeneous network integration for drug–target interaction prediction and repositioning** — “A network integration approach for drug-target interaction prediction and computational drug repositioning from heterogeneous information” (2017), introducing a widely adopted framework for <u>multi-source biomedical network fusion</u> to infer novel drug–target links and support computational repositioning.\n- **Contrastive learning for enzyme function annotation at scale** — “Enzyme function prediction using contrastive learning” (2023), advancing <u>protein representation learning</u> to improve enzyme commission (EC) prediction, particularly for under-annotated proteins and sparse functional classes.\n- **Evolutionary-context deep learning for protein engineering** — “ECNet is an evolutionary context-integrated deep learning framework for protein engineering” (2021), demonstrating how <u>evolutionary information</u> can be integrated into deep sequence models to strengthen predictive performance for engineering-relevant fitness landscapes.\n\n---\n\n## Academic Background\nAt Georgia Institute of Technology, Luo has built an interdisciplinary research profile spanning **bioinformatics**, **machine learning**, and *translational computational medicine*, reflected in sustained high-impact publications and broad topical reach. His scholarly trajectory suggests a progression from early work in algorithmic and data-integration problems (e.g., metagenomic binning via hashing in “Low-density locality-sensitive hashing boosts metagenomic binning,” 2016; “Metagenomic binning through low-density hashing,” 2019) toward biomedical network modeling and drug discovery (DTINet, 2017; COVID-19 repositioning, 2021), and then to modern deep representation learning for proteins and molecular interactions (ECNet, 2021; GeoPPI, 2021; contrastive enzyme annotation, 2023). The presence of a dissertation titled “Machine learning for large and small data biomedical discovery” (2021) indicates formal doctoral training focused on data-scarce and data-rich regimes in biomedicine, aligning with later contributions in few-shot learning for clinical translation (“Few-shot learning creates predictive models of drug response…,” 2021). His collaborations and publication venues—spanning drug response modeling, multi-omics challenges (“Crowdsourcing assessment of maternal blood multi-omics for predicting gestational age and preterm birth,” 2021), and kinase inhibitor target mapping (“Crowdsourced mapping of unexplored target space of kinase inhibitors,” 2021)—reflect affiliations and partnerships across computational biology, systems pharmacology, and applied machine learning communities, consistent with an academic profile anchored at Georgia Tech and oriented toward <u>integrative, generalizable biomedical AI</u>."}, "NLeeizQAAAAJ": {"keywords": "Networking, Distributed Systems, P2P, Internet measurement, Broadband measurement, ISP characterization, Internet outages and shutdown detection, Internet topology mapping, Submarine cable network analysis, Router geolocation from hostnames", "summary": "## Overview\nZachary S. Bischof is a Georgia Tech–affiliated researcher working at the intersection of **networking** and **distributed systems**, with a sustained emphasis on *measurement-driven* understanding of real-world Internet behavior. His work centers on bringing rigorous experimentation to end-host vantage points—particularly the <u>Internet’s edge</u>—to study broadband performance, ISP behavior, and large-scale connectivity. Across projects spanning measurement platforms, infrastructure mapping, and disruption analysis, Bischof’s research agenda advances **Internet measurement** as both a methodological foundation and a practical tool for accountability, reliability, and policy-relevant insight.\n\n---\n\n## Research Areas\nBischof’s research areas span (1) <u>edge-based Internet measurement</u> and experimentation, (2) broadband and ISP characterization, (3) Internet infrastructure mapping across physical and logical layers, and (4) disruption and outage analysis. A core throughline is the design of systems that enable measurement at scale from end-user environments, exemplified by **Dasu** in *“Dasu: Pushing Experiments to the {Internet’s} Edge”* (2013), which frames the edge as an experimentally useful and socially scalable vantage point. Building on this, he has contributed to **crowdsourced ISP characterization** (e.g., *“Crowdsourcing ISP characterization to the network edge”*, 2011) and to richer interpretations of broadband performance that incorporate user demand, pricing, and service attributes (e.g., *“Need, want, can afford: Broadband markets and the behavior of users”*, 2014), as well as application-informed ISP profiling (*“Up, down and around the stack: ISP characterization from network intensive applications”*, 2012). In parallel, Bischof has worked on connecting measurement to Internet “ground truth” by mapping and interpreting infrastructure and topology, including submarine cable systems (*“Untangling the world-wide mesh of undersea cables”*, 2018; *“Out of sight, not out of mind…”*, 2020) and router geolocation via learning-based extraction of geographic hints from naming conventions (*“Learning to extract geographic information from internet router hostnames”*, 2021). More recently, his work has expanded toward longitudinal, methodologically grounded analyses of Internet disruptions and shutdowns (*“Destination unreachable: Characterizing internet outages and shutdowns”*, 2023), and toward datasets and methodologies that improve how the community links physical and logical Internet maps (*“iGDB: connecting the physical and logical layers of the internet”*, 2022).\n\n---\n\n## Notable Works\n- **Dasu edge experimentation platform** — Introduced and evaluated an extensible platform enabling controlled experimentation and broadband characterization from end hosts in *“Dasu: Pushing Experiments to the {Internet’s} Edge”* (2013), helping establish the <u>edge</u> as a scalable measurement substrate.\n- **Crowdsourced broadband/ISP characterization from the network edge** — Advanced methods for large-scale ISP evaluation using end-user participation in *“Crowdsourcing ISP characterization to the network edge”* (2011), complementing later work on multi-layer and application-informed profiling such as *“Up, down and around the stack…”* (2012).\n- **Internet infrastructure and disruption measurement** — Produced influential measurement studies that connect infrastructure structure to user experience and resilience, including submarine cable mapping in *“Untangling the world-wide mesh of undersea cables”* (2018) and longitudinal disruption analysis in *“Destination unreachable: Characterizing internet outages and shutdowns”* (2023).\n\n---\n\n## Academic Background\nBased at **Georgia Tech**, Bischof has built a publication record that bridges systems-building with empirically grounded Internet analysis, reflecting an academic trajectory rooted in **networking**, **distributed systems**, and *public-interest measurement*. His early work includes simulation and tooling for wireless sensor networks (e.g., *“Sidnet-swans…”*, 2008) and P2P-oriented measurement perspectives (e.g., *“Distributed systems and natural disasters: BitTorrent as a global witness”*, 2011), which foreshadowed later emphasis on leveraging widely deployed systems and end-host vantage points for observational power. Over time, his scholarship has increasingly engaged with broadband reliability, market and policy implications, and the interpretability of Internet datasets—seen in work on broadband reliability characterization (2018), data sanitization for consumer broadband measurement (2020), and cross-layer mapping via **iGDB** (2022). With approximately **790 citations**, his contributions indicate sustained impact and ongoing engagement with the Internet measurement community, including collaborations that span infrastructure mapping, measurement methodology, and disruption-focused research aligned with both scientific and societal priorities."}, "Ntekq2MAAAAJ": {"keywords": "Human Computer Interaction, Inclusive Design, Assistive Technology, Workplace Accommodations, Universal Design, Augmentative and Alternative Communication (AAC), Accessibility in Informal Learning Environments, Sonification and Auditory Displays, Dementia Communication Environments, Web Accessibility Testing (WCAG)", "summary": "## Overview\nCarrie Bruce is a **Principal Research Scientist** at the **Georgia Institute of Technology**, where she contributes to research at the intersection of **Human–Computer Interaction (HCI)**, *inclusive design*, and **assistive technology**. Her work centers on <u>accessibility as a socio-technical and environmental problem</u>, spanning both digital systems and built environments. Across projects in informal learning environments (e.g., aquariums and museums) and employment contexts, Bruce advances *evidence-based* approaches to designing and evaluating accommodations that improve participation for people with disabilities—particularly through <u>universal design</u>, communication supports, and tools that translate assessment frameworks into practical interventions.\n\n---\n\n## Research Areas\nBruce’s research portfolio integrates three tightly connected areas: (1) <u>communication and participation in workplace contexts</u>, (2) <u>accessible interpretation in dynamic informal learning environments</u>, and (3) <u>environmental assessment for cognitive and communication disability</u>. In workplace accessibility, she has examined how language use differs for employees who rely on augmentative and alternative communication (AAC), including detailed analyses of linguistic co-occurrence patterns in real work settings in “**Linguistic characteristics of AAC discourse in the workplace**” (2013) and the development of shared research infrastructure through “**The AAC and non-AAC workplace corpus (ANAWC)**” (2009) and its later overview “**An introduction to the ANAWC**” (2019). In accessible informal learning, Bruce has been a key contributor to sonification-based exhibit interpretation, developing and evaluating soundscapes that convey dynamic, typically visual information—exemplified by “**Aquarium sonification: Soundscapes for accessible dynamic informal learning environments**” (2006), “**Aquarium fugue: interactive sonification for children and visually impaired audience in informal learning environments**” (2012), and subsequent design-oriented work on interactive sonification for live exhibits (2013). Complementing these HCI contributions, she has also advanced environmental and communication assessment approaches in healthcare and long-term care settings, including toolkits for dementia care (“**Development and evaluation of the environment and communication assessment toolkit with speech-language pathologists**,” 2013; “**Evaluating and modifying the communication environment for people with dementia**,” 2011) and broader measurement frameworks for the physical environment (“**Measuring the impact of the physical environment**,” 2010). More recently, her applied accessibility scholarship extends to professional practice and tooling, including work on web accessibility evaluation (“**Improving web accessibility testing with microsoft’s accessibility insights for web**,” 2023) and studies of practitioners’ experiences in accessibility-focused UX work (2021), reinforcing a throughline of <u>translating accessibility principles into real-world methods</u>.\n\n---\n\n## Notable Works\n- **Operationalizing rehabilitation assessment frameworks for practice**: Through “**Rehabilitation and health assessment: applying ICF guidelines**” (2009), Bruce contributed to the dissemination and application of **ICF-aligned** approaches, supporting rigorous, standardized thinking about function, environment, and participation in rehabilitation-related assessment and education.\n- **Pioneering accessible interpretation for dynamic exhibits via sonification**: In “**Aquarium sonification: Soundscapes for accessible dynamic informal learning environments**” (2006) and follow-on work such as “**Aquarium fugue**” (2012), she helped establish design and evaluation directions for <u>sound-based mediation</u> that enables visitors with vision impairments to engage with live, changing animal exhibits.\n- **Building an empirical foundation for AAC-inclusive workplace communication design**: With “**Linguistic characteristics of AAC discourse in the workplace**” (2013) and the resource contribution “**The AAC and non-AAC workplace corpus (ANAWC)**” (2009), Bruce advanced corpus-driven methods to inform AAC vocabulary and interaction design grounded in authentic workplace discourse.\n\n---\n\n## Academic Background\nBruce’s publication trajectory indicates an interdisciplinary academic formation spanning **rehabilitation and health assessment**, **human factors**, and **HCI-oriented inclusive design**, with sustained collaborations across clinical and design communities. Her work with structured frameworks (e.g., ICF-aligned assessment in 2009) and applied tool development for clinicians (e.g., dementia-focused environment and communication toolkits developed and evaluated with speech-language pathologists in 2011–2013) suggests deep engagement with rehabilitation science and clinical practice, while her accessible aquarium and museum research demonstrates long-standing involvement in accessibility for public-facing informal learning environments. The breadth of topics—from workplace accommodation assessment frameworks (e.g., evidence-based conceptual models and remote/technology-mediated assessment approaches in the mid-2000s) to universal design synthesis efforts (e.g., “**The state of the science in universal design: Emerging research and developments**,” 2010)—aligns with affiliations to multi-institution accessibility and universal design initiatives and a career focused on translational research. With **498 citations**, Bruce’s influence reflects both highly cited foundational contributions (notably in assessment and accessibility mediation) and durable community impact through reusable resources (e.g., ANAWC) and practice-facing guidance for inclusive environments and accessibility evaluation."}, "OLDMURQAAAAJ": {"keywords": "Combinatorial Scientific Computing, High Performance Computing, Parallel Computing, Biomedical Informatics, Hypergraph Partitioning, Sparse Matrix Partitioning, Parallel Sparse Matrix-Vector Multiplication, Dynamic Load Balancing, Large-Scale Graph Analytics, Bioinformatics and Computational Genomics", "summary": "## Overview\nÜmit V. Çatalyürek is a Professor in **Computational Science and Engineering** at the **Georgia Institute of Technology**, where he leads research at the intersection of *algorithmic foundations* and *systems-aware implementation* for large-scale computation. His work centers on <u>combinatorial scientific computing</u>, with particular emphasis on **hypergraph/graph models** for partitioning, ordering, and load balancing that enable efficient **parallel and high-performance computing (HPC)**. Across a highly cited body of work (over **13,871 citations**), he has advanced both the theory and practice of scalable computation—spanning sparse linear algebra, graph analytics, and data-intensive biomedical applications—by designing methods that explicitly model *communication costs* and *irregularity* in modern parallel architectures.\n\n---\n\n## Research Areas\nÇatalyürek’s research portfolio is anchored in hypergraph-based modeling for parallelism in irregular applications, especially sparse matrix computations. In “**Hypergraph-partitioning-based decomposition for parallel sparse-matrix vector multiplication**” (2002), he demonstrates that standard graph partitioning fails to capture true communication volume in SpMV and introduces hypergraph models that more faithfully represent communication requirements—an agenda extended in “**On two-dimensional sparse matrix partitioning: Models, methods, and a recipe**” (2010) and earlier foundational work on decomposing irregular sparse matrices (1996, 2001). Complementing these models, he has contributed to practical, scalable tooling and algorithmic infrastructures for combinatorial kernels, including “**Parallel hypergraph partitioning for scientific computing**” (2006), the widely used **PaToH** hypergraph partitioner (“PaToH (Partitioning Tool for Hypergraphs)” (2011); “PaToH: a multilevel hypergraph partitioning tool, version 3.0” (1999)), and community toolkits such as “**The Zoltan and Isorropia parallel toolkits for combinatorial scientific computing**” (2012). Beyond sparse linear algebra, he has advanced scalable graph analytics and irregular graph kernels on leadership-class systems—e.g., distributed traversal in “**A scalable distributed parallel breadth-first search algorithm on BlueGene/L**” (2005), streaming and incremental approaches to structural decompositions (“**Streaming algorithms for k-core decomposition**” (2013); “Incremental k-core decomposition” (2016)), and dense subgraph discovery (“**Finding the hierarchy of dense subgraphs using nucleus decompositions**” (2015)). A distinctive aspect of his trajectory is the sustained translation of HPC and combinatorial methods into biomedical and bioinformatics domains, including gene expression pattern mining (“**A comparative analysis of biclustering algorithms for gene expression data**” (2013)), next-generation sequencing evaluation (“**Benchmarking short sequence mapping tools**” (2013)), and computational pathology for cancer prognosis (neuroblastoma and lymphoma image analysis, 2009), reflecting a consistent emphasis on *scalability*, *data intensity*, and <u>communication-aware computation</u>.\n\n---\n\n## Notable Works\n- **Communication-accurate hypergraph models for sparse matrix–vector multiplication (SpMV)** — In “*Hypergraph-partitioning-based decomposition for parallel sparse-matrix vector multiplication*” (2002), Çatalyürek establishes hypergraph partitioning as a superior abstraction to graph partitioning for modeling and minimizing communication volume in parallel SpMV, shaping subsequent work in sparse linear algebra partitioning (including 2D methods in 2010).\n- **Scalable hypergraph partitioning algorithms and tools for scientific computing** — Through “*Parallel hypergraph partitioning for scientific computing*” (2006) and the **PaToH** tool papers (1999; 2011), he delivers practical multilevel hypergraph partitioning capabilities that support load balancing and communication minimization across a wide range of irregular HPC applications.\n- **Leadership-class scalable graph traversal on distributed systems** — In “*A scalable distributed parallel breadth-first search algorithm on BlueGene/L*” (2005), he develops a distributed BFS approach that scales to graphs with billions of vertices and tens of billions of edges, contributing to the foundations of large-scale graph analytics on supercomputers.\n\n---\n\n## Academic Background\nAs a senior academic in **Computational Science and Engineering** at **Georgia Tech**, Çatalyürek’s career reflects a sustained commitment to the algorithmic and software foundations of scalable computation, evidenced by influential publications spanning the late 1990s through the 2020s and a citation record exceeding **13,871**. His early and continued focus on sparse matrix decomposition and hypergraph partitioning (e.g., 1996–2002) matured into widely adopted software artifacts such as **PaToH** (1999; 2011) and broader community infrastructure via toolkits like **Zoltan/Isorropia** (2012), indicating deep engagement with collaborative, multi-institutional HPC ecosystems. The breadth of his coauthored application-driven work—ranging from BlueGene/L-era distributed graph algorithms (2005) to biomedical informatics and computational pathology (2003–2013) and later advances in graph mining (2013–2016) and partitioning surveys (2023)—suggests longstanding interdisciplinary collaborations and affiliations across **high-performance computing**, **data-intensive systems**, and **biomedical data science**, with recognized impact through widely cited methodological contributions and widely used partitioning technology."}, "OT79Y9UAAAAJ": {"keywords": "Networking, Networked Services, Wireless and Mobile Networks, Multicast, Multimedia Communication, Delay-Tolerant Networks (DTN), Message Ferrying, Opportunistic Routing, Network Virtualization (NFV/SFC), Edge Computing and Mobile Offloading", "summary": "## Overview\nMostafa Ammar is a **Regents' Professor** and *Interim Chair* in the School of Computer Science at **Georgia Tech**, where he has led and shaped research on modern computer networks and networked systems at scale. His work centers on the design, analysis, and measurement of **networked services** across heterogeneous environments—spanning *wireless and mobile networks*, **multicast**, and **multimedia communication**—with a sustained emphasis on making networks more robust, efficient, and service-aware under real-world constraints. Across a highly cited body of scholarship (over **24,000 citations**), Ammar has repeatedly advanced foundational ideas for <u>communication under disruption</u>, <u>scalable content delivery</u>, and <u>virtualized network infrastructures</u>, linking principled algorithmic design to practical architectures.\n\n---\n\n## Research Areas\nAmmar’s research spans several interlocking areas of networking, unified by the question of how to deliver communication and computation reliably when the network is heterogeneous, dynamic, or only intermittently connected. In **wireless and mobile networking**, he is widely associated with *delay-/disruption-tolerant networking* and the <u>message ferrying</u> paradigm, introduced and refined through work on proactive routing in highly partitioned ad hoc networks (e.g., “**Message ferrying: Proactive routing in highly-partitioned wireless ad hoc networks**,” 2003) and extended via route design and multi-ferry control (e.g., “**Message ferry route design for sparse ad hoc networks with mobile nodes**,” 2006; “**Controlling the mobility of multiple data transport ferries in a delay-tolerant network**,” 2005). This line of work also broadened into capacity augmentation mechanisms such as *throwboxes* (“**Capacity enhancement using throwboxes in DTNs**,” 2006) and semantic/routing formulations for DTN multicast (“**Multicasting in delay tolerant networks: semantic models and routing algorithms**,” 2005). In **network virtualization and softwarized infrastructures**, he contributed influential algorithmic formulations for mapping virtual networks to shared substrates (“**Algorithms for Assigning Substrate Network Resources to Virtual Network Components**,” 2006) and later addressed orchestration challenges in **NFV**, including service function chain routing and VNF migration (“**An approach for service function chain routing and virtual function network instance migration in network function virtualization architectures**,” 2017; “**Migration energy aware reconfigurations of virtual network function instances in NFV architectures**,” 2017). A third major thread targets **scalable multimedia and content distribution**, where he studied multicast-based video-on-demand and fairness/heterogeneity in multicast video delivery (e.g., “**The use of multicast delivery to provide a scalable and interactive video-on-demand service**,” 2002; “**On the use of destination set grouping to improve fairness in multicast video distribution**,” 1996; “**Layered video multicast with retransmissions (LVMR)**,” 1998). Complementing these systems contributions are influential efforts in **network measurement and operational realism**, including TCP throughput predictability (“**On the predictability of large transfer TCP throughput**,” 2005), privacy-preserving trace sharing via prefix-preserving anonymization (“**Prefix-preserving ip address anonymization…**,” 2002/2004), and large-scale simulation methodology (“**Large-scale network simulation: how big? how fast?**,” 2003). Finally, he has engaged security and incentive questions in decentralized systems (“**A reputation system for peer-to-peer networks**,” 2003) and explored *edge/mobile computing* through opportunistic remote computing and device clouds (“**Serendipity**,” 2012; “**Femto clouds**,” 2015; “**Cosmos**,” 2014), reflecting a consistent interest in <u>service delivery over constrained, evolving networks</u>.\n\n---\n\n## Notable Works\n- **Message ferrying for disrupted connectivity**: Developed and popularized the <u>message ferrying</u> paradigm for sparse, partitioned mobile ad hoc networks, demonstrating how controlled mobility can enable reliable delivery when end-to-end paths rarely exist (notably “**A message ferrying approach for data delivery in sparse mobile ad hoc networks**,” 2004; building on “**Message ferrying: Proactive routing in highly-partitioned wireless ad hoc networks**,” 2003).\n- **Virtual network embedding and Internet evolvability**: Provided highly influential algorithmic foundations for **network virtualization**, addressing the core resource-assignment problem of embedding virtual components on a shared substrate (“**Algorithms for Assigning Substrate Network Resources to Virtual Network Components**,” 2006), a cornerstone for later work in programmable and virtualized network infrastructures.\n- **Scalable multicast-based multimedia delivery**: Advanced architectures and control mechanisms for **multicast** and **multimedia communication**, including scalable interactive VoD via multicast delivery (“**The use of multicast delivery to provide a scalable and interactive video-on-demand service**,” 2002) and receiver heterogeneity/fairness mechanisms in multicast video distribution (e.g., “**On the use of destination set grouping to improve fairness in multicast video distribution**,” 1996; “**Layered video multicast with retransmissions (LVMR)**,” 1998).\n\n---\n\n## Academic Background\nAs a long-standing faculty member at **Georgia Tech**’s School of Computer Science, Ammar has built an academic career marked by sustained, high-impact contributions across multiple generations of networking research—from early work on broadcast information systems and replicated services (e.g., teletext/videotex scheduling and replicated data protocols) to Internet-era multicast and application-layer anycasting, and onward to DTNs, virtualization, and edge/mobile computing. His appointment as **Regents' Professor** reflects a record of exceptional scholarly influence and leadership, supported by a publication trajectory that spans foundational systems research, rigorous algorithmic modeling, and measurement-driven studies that have shaped community practice (e.g., IP trace anonymization and large-scale simulation). His leadership roles—including serving as *Interim Chair*—align with a career combining research excellence, mentorship, and institutional service, while his extensive citation impact underscores broad adoption of his ideas across networking, distributed systems, and networked multimedia communities."}, "OlRjTCIAAAAJ": {"keywords": "virtualization, hypervisors, GPU virtualization, heterogeneous memory management, non-volatile memory (NVM) systems, LSM-tree key-value storage, checkpointing and fault tolerance, edge computing, data center energy and thermal management, network function virtualization (NFV) security (Intel SGX)", "summary": "## Overview\nAda Gavrilovska is a faculty researcher at the Georgia Institute of Technology, where she leads research at the intersection of **systems software** and *large-scale computing infrastructures*. Her work centers on building **efficient, secure, and adaptable platforms** for modern computing—from virtualized **HPC** and cloud environments to the emerging *edge-to-cloud continuum*—with a sustained emphasis on <u>resource management</u>, <u>virtualization</u>, and <u>next-generation memory and storage</u>. Across her scholarship, Gavrilovska has advanced the state of the art in enabling high-performance applications to thrive under consolidation, heterogeneity, and stringent operational constraints such as power, latency, and reliability.\n\n---\n\n## Research Areas\nGavrilovska’s research spans several tightly connected areas of computer systems. A foundational theme is **virtualization for performance-critical environments**, demonstrated by early contributions on hypervisor design and multicore virtualization impacts (e.g., *“High-performance hypervisor architectures: Virtualization in hpc systems”* and *“Performance implications of virtualizing multicore cluster machines”*), as well as practical mechanisms for I/O isolation and differentiation in virtualized settings (e.g., *“On disk I/O scheduling in virtual machines”* and *“Differential virtual time (DVT)”*). She also pioneered approaches for bringing accelerators into virtualized platforms—most notably enabling <u>GPU access in VMs</u> in *“GViM: GPU-accelerated virtual machines”*—and later expanded this direction toward multi-tenant accelerator management (e.g., *“GPUShare: Fair-sharing middleware for GPU clouds”*). A second major thrust is **heterogeneous and persistent memory systems**, where her work addresses both OS-level management (e.g., *“HeteroOS: OS Design for Heterogeneous Memory Management in Datacenter”*, *“CoMerge”*, and *“Kleio”*) and the design of persistent data structures and storage engines for <u>nonvolatile memory (NVM)</u> (e.g., *“pVM: persistent virtual memory”* and *“Redesigning LSMs for nonvolatile memory with NoveLSM”*). Complementing these systems efforts is a strong line of research in **datacenter efficiency and autonomic management**, including VM-level power accounting (*“VM power metering”*) and energy/thermal-aware operation (*“Energy efficient thermal management of data centers”*), as well as broader perspectives on self-managing systems (*“Autonomic computing: concepts, infrastructure, and applications”*). In more recent work, Gavrilovska extends systems principles to **edge computing and edge security**, including fast and secure edge function provisioning (*“airbox”*), state protection for virtualized network functions (*“S-NFV: Securing NFV states by using SGX”*), and edge-driven defenses against emerging threats (e.g., *“Towards IoT-DDoS prevention using edge computing”*), alongside forward-looking viewpoints on <u>edge-to-cloud systems</u> and orchestration (e.g., *“The edge-to-cloud continuum”*).\n\n---\n\n## Notable Works\n- **Virtualizing accelerators for high-performance and shared environments:** In *“GViM: GPU-accelerated virtual machines”* (2009), Gavrilovska helped establish practical pathways for exposing GPU acceleration through virtualization layers, addressing a core barrier to consolidating high-performance workloads while preserving access to specialized hardware.\n- **Re-architecting storage engines for persistent, byte-addressable memory:** In *“Redesigning LSMs for nonvolatile memory with NoveLSM”* (2018), she introduced a persistent LSM-based design that exploits NVM properties (e.g., direct mutability and opportunistic parallel reads), advancing <u>low-latency, high-throughput</u> key-value storage on emerging memory technologies.\n- **OS-level management of heterogeneous memory in datacenters:** In *“HeteroOS: OS Design for Heterogeneous Memory Management in Datacenter”* (2017), she advanced operating-system techniques for managing heterogeneous memory under virtualization, targeting scalable page tracking and migration strategies that reduce hypervisor-only bottlenecks.\n\n---\n\n## Academic Background\nBased at the Georgia Institute of Technology, Ada Gavrilovska has built an academic profile characteristic of a senior systems researcher whose work bridges **operating systems, virtualization, and distributed infrastructure** across HPC, cloud, and edge settings. Her publication trajectory—from early work on adaptive/autonomic systems and programmable network services (e.g., *“Service morphing”* and *“C-core”*) to influential contributions in virtualization, power-aware datacenter management, and NVM-centric OS/storage design—reflects sustained leadership in <u>systems architecture</u> and <u>resource-efficient computing</u>. The breadth of her collaborations and topics (spanning hypervisors, accelerators, persistent memory, secure NFV, and edge orchestration) indicates deep engagement with both academic and applied research communities in computer systems. With **4,820 citations**, her work has achieved significant scholarly impact, and her continued output in heterogeneous memory, persistent storage, and edge-to-cloud computing suggests ongoing influence on how modern infrastructures are designed for performance, efficiency, and security under real-world constraints."}, "PN-rzYUAAAAJ": {"keywords": "physical chemistry, theoretical chemistry, computational chemistry, algorithms, electronic structure theory, coupled-cluster methods, symmetry-adapted perturbation theory (SAPT), noncovalent interactions, dispersion-corrected density functional theory (DFT-D), open-source quantum chemistry software (Psi4/Q-Chem)", "summary": "## Overview\nDavid Sherrill is a **Regents’ Professor** of **Chemistry and Computational Science** at the Georgia Institute of Technology (Georgia Tech), where he leads a research program at the interface of *physical chemistry* and *high-performance scientific computing*. His group is widely recognized for advancing **theoretical and computational chemistry** through the development of *reliable electronic-structure methods* and the **software infrastructure** needed to make those methods broadly usable. Across both fundamental theory and practical implementation, Sherrill’s work centers on <u>accurate quantum-mechanical descriptions of intermolecular interactions</u>—especially **noncovalent interactions** such as *π–π stacking*, hydrogen bonding, and dispersion—while simultaneously innovating in <u>algorithms</u> and <u>open, interoperable program packages</u> that enable robust, automated, and scalable computation.\n\n---\n\n## Research Areas\nSherrill’s research spans method development, benchmarking, and software engineering in modern quantum chemistry, with a sustained emphasis on the physics and computation of **noncovalent interactions**. A major thematic pillar is the high-accuracy characterization of *aromatic stacking* and related dispersion-driven phenomena, exemplified by benchmark studies on the **benzene dimer** and substituent effects in π–π interactions (e.g., “Estimates of the ab initio limit for π−π interactions: The benzene dimer,” 2002; “Substituent effects in π−π interactions: Sandwich and T-shaped configurations,” 2004; and subsequent high-accuracy potential energy curves). Complementing these benchmark efforts, he has contributed to methodological frameworks that *explain* intermolecular binding via energy decomposition, including extensive development and assessment of **symmetry-adapted perturbation theory (SAPT)** and its efficient approximations (e.g., “Levels of symmetry adapted perturbation theory (SAPT). I. Efficiency and performance for interaction energies,” 2014; and density-fitting/Cholesky approaches for SAPT). Another central area is the rigorous evaluation and improvement of *approximate electronic-structure models* used in practice—particularly **density functional theory** for dispersion and hydrogen bonding—through systematic comparisons and refined parameterizations (e.g., “Density-functional approaches to noncovalent interactions…,” 2011; “Revised damping parameters for the D3 dispersion correction…,” 2016). Finally, Sherrill has been a key architect of widely used quantum chemistry software ecosystems, contributing to major releases and capabilities in **Q-Chem** (2000, 2006, 2015) and helping shape the open-source **Psi4** platform for automation, interoperability, and high-throughput computation (2012, 2017, 2020), including the pedagogically and methodologically enabling **Psi4NumPy** environment (2018).\n\n---\n\n## Notable Works\n- **Software and algorithms enabling broad access to advanced electronic structure methods**: Sherrill is a leading contributor to the modernization of quantum chemistry program packages, particularly through the widely cited reviews “Advances in methods and algorithms in a modern quantum chemistry program package” (2006) and “Advances in molecular quantum chemistry contained in the Q-Chem 4 program package” (2015), which document major advances in *DFT*, correlated wavefunction methods, and the algorithmic infrastructure required for routine, community-scale use.\n\n- **Benchmark-quality understanding of π–π interactions and the benzene dimer as a prototype system**: His group’s high-accuracy studies established reference points for dispersion-bound complexes, including “Estimates of the ab initio limit for π−π interactions: The benzene dimer” (2002) and subsequent coupled-cluster potential energy investigations such as “Highly accurate coupled cluster potential energy curves for the benzene dimer…” (2004), shaping how π-stacking is computed, interpreted, and used for method validation.\n\n- **Open-source quantum chemistry as a platform for reproducible, automated, and high-throughput research**: Sherrill has played a central role in the development and dissemination of the **Psi4** ecosystem, including “Psi4: an open-source ab initio electronic structure program” (2012), “Psi4 1.1… Emphasizing Automation, Advanced Libraries, and Interoperability” (2017), and “PSI4 1.4: Open-source software for high-throughput quantum chemistry” (2020), establishing Psi4 as a widely adopted foundation for method development, workflows, and interoperable computational chemistry.\n\n---\n\n## Academic Background\nAs a senior scholar at Georgia Tech with the rank of **Regents’ Professor**, Sherrill’s career reflects sustained leadership in *theoretical chemistry* and *computational method development*, evidenced by a publication record spanning foundational work on correlated wavefunction theory (e.g., advanced configuration interaction and coupled-cluster models) through modern algorithmic and software contributions. The trajectory of his highly cited papers—from early advances in **highly correlated approaches** and questions of perturbation-theory convergence (e.g., “The configuration interaction method: Advances in highly correlated approaches,” 1999; “Is Møller–Plesset perturbation theory a convergent ab initio method?,” 2000) to later community-defining benchmarks for noncovalent interactions and major program-package releases—indicates deep and continuous engagement with both *fundamental theory* and *computational practice*. His extensive influence is further reflected in his large citation footprint (over **40,000 citations**) and his prominent affiliations with flagship software efforts in the field, notably **Q-Chem** and the open-source **Psi4** project, through which he has helped shape norms of reproducibility, performance-oriented implementation, and accessible tooling for the broader chemical sciences community."}, "PNalJ58AAAAJ": {"keywords": "Human-Computer Interaction (HCI), Computer-Supported Cooperative Work (CSCW), Ubiquitous Computing, Postcolonial Computing, Intersectional HCI, Global Software Development, Distributed Collaboration and Coordination, Teen Digital Communication (IM/SMS), Usable Security and Privacy, Domestic Technologies and Human-Robot Interaction (Smart Homes/Home Networking)", "summary": "## Overview\nRebecca E. Grinter is a **Professor and Associate Dean** at the Georgia Institute of Technology, where she is a leading scholar in **Human–Computer Interaction (HCI)** and **Computer-Supported Cooperative Work (CSCW)**, with a sustained emphasis on *field-based, empirically grounded* studies of technology in everyday life and organizational settings. Across her work, Grinter has advanced the understanding of how people integrate computing into homes, workplaces, and communities, shaping research agendas around <u>ubiquitous computing</u>, <u>global collaboration</u>, and <u>socially situated design</u>. Her scholarship—spanning domestic technologies, communication practices, and critical perspectives on design—has had broad impact (over **20,000 citations**), reflecting an enduring commitment to connecting **technical innovation** with *human realities*.\n\n---\n\n## Research Areas\nGrinter’s research spans three tightly connected areas. First, she has made foundational contributions to <u>ubiquitous computing in the home</u>, articulating the socio-technical frictions that arise when “smart” systems meet domestic routines, values, and maintenance work—most notably in “**At Home with Ubiquitous Computing: Seven Challenges**” (2001) and later empirical studies of home infrastructure such as “**The work to make a home network work**” (2005) and “**Getting to green: understanding resource consumption in the home**” (2008). Second, she has deeply shaped scholarship on <u>everyday communication and youth practices</u>, documenting how teens adopt and repurpose communication media, including “**Instant messaging in teen life**” (2002), “**y do tngrs luv 2 txt msg?**” (2001), and “**Wan2tlk? Everyday text messaging**” (2003), showing how norms, identity, and coordination practices emerge through mundane messaging. Third, her CSCW and software-organization research examines <u>coordination across distance</u> and the structural coupling of organizations and technical architectures, including “**Splitting the organization and integrating the code: Conway's law revisited**” (1999), “**Distance, dependencies, and delay in a global collaboration**” (2000), and “**An empirical study of global software development: distance and speed**” (2001). Complementing these strands, Grinter has helped define critical and justice-oriented directions in HCI through “**Postcolonial computing: a lens on design and development**” (2010) and identity-aware approaches such as “**Intersectional HCI: Engaging identity through gender, race, and class**” (2017), extending methodological and ethical debates about how design travels across contexts and how “users” are constituted in research.\n\n---\n\n## Notable Works\n- **Postcolonial computing as an analytic orientation for design**: In “**Postcolonial computing: a lens on design and development**” (2010), Grinter (with collaborators) advanced a widely influential framework for analyzing how power, history, and cultural context shape technology design and deployment, reframing HCI’s engagement with global and cross-cultural settings.\n- **Defining the domestic agenda for ubiquitous computing**: “**At Home with Ubiquitous Computing: Seven Challenges**” (2001) crystallized core problems of designing for homes—where values, routines, and accountability differ from workplaces—setting a durable research program later elaborated through studies of home networking (“**The work to make a home network work**,” 2005) and domestic resource practices (“**Getting to green**,” 2008).\n- **Revisiting coordination and Conway’s Law in global development**: Through “**Splitting the organization and integrating the code: Conway's law revisited**” (1999) and related empirical work on distributed collaboration (“**Distance, dependencies, and delay in a global collaboration**,” 2000), Grinter helped establish how organizational distribution, dependencies, and communication breakdowns materially shape software architectures and project outcomes.\n\n---\n\n## Academic Background\nAs a senior academic leader at **Georgia Tech**, Grinter’s career reflects deep engagement with the interdisciplinary traditions of **HCI**, **CSCW**, and **ubiquitous computing**, evidenced by a publication record spanning premier venues associated with these communities and a sustained emphasis on *in-the-wild* and ethnographically informed methods. Her early and highly cited work on software development coordination and distributed R&D (mid-to-late 1990s through early 2000s) suggests formative affiliations with CSCW and empirical software engineering communities, later expanding into domestic computing, communication practices, and the socio-cultural critique of design as technologies moved beyond workplace settings. Over time, her scholarship also intersected with **usable security** (“**Security in the wild**,” 2004; “**In search of usable security**,” 2004) and human–robot interaction in everyday environments (“**“My Roomba is Rambo”**,” 2007; “**Robots in the wild**,” 2009), underscoring a consistent pattern: studying adoption, breakdown, and meaning-making where technologies meet lived practice. Her citation impact and leadership role as **Associate Dean** indicate sustained recognition by the field, alongside ongoing influence on research directions that foreground <u>context</u>, <u>identity</u>, and <u>power</u> in the design and evaluation of computing systems."}, "Q6F3O0sAAAAJ": {"keywords": "robotics, computer graphics, machine learning, deep reinforcement learning, soft actor-critic, legged locomotion, sim-to-real transfer, safe reinforcement learning, differentiable simulation, vision-language navigation", "summary": "## Overview\nSehoon Ha is a faculty researcher at the Georgia Institute of Technology, where he works at the intersection of **robotics**, **computer graphics**, and **machine learning**. His group’s research centers on building *robust, deployable autonomy* by unifying learning-based control with physically grounded simulation, with particular emphasis on <u>reinforcement learning for dynamic locomotion</u> and <u>simulation-to-real transfer</u>. Across highly cited algorithmic and systems contributions, Ha has helped shape modern practice in **deep reinforcement learning** while advancing *legged and humanoid* robot capabilities through scalable training pipelines, principled safety mechanisms, and open-source toolkits.\n\n---\n\n## Research Areas\nHa’s research spans three tightly connected thrusts. First, he has made foundational contributions to **deep reinforcement learning**, most notably in maximum-entropy off-policy methods through *“Soft actor-critic algorithms and applications”* (2018), which addresses sample efficiency and hyperparameter brittleness and has become a widely used baseline for continuous-control RL. Second, he advances <u>learning-based locomotion</u> for legged systems, demonstrating end-to-end policy learning for walking in *“Learning to walk via deep reinforcement learning”* (2018) and extending these ideas to real-world deployment with reduced human intervention in *“Learning to walk in the real world with minimal human effort”* (2020), as well as continued on-hardware adaptation in *“Legged robots that keep on learning: Fine-tuning locomotion policies in the real world”* (2022). Third, his work emphasizes <u>physically grounded simulation and tool support</u>, including community infrastructure via *“DART: Dynamic animation and robotics toolkit”* (2018) and learning methods that exploit differentiable simulators in *“PODS: Policy optimization via differentiable simulation”* (2021). Complementing these pillars are sustained efforts in safety-aware learning (e.g., *“Learning to be safe: Deep RL with a safety critic”*, 2020), perception-conditioned locomotion (e.g., *“Visual-locomotion: Learning to walk on complex terrains with vision”*, 2021), and embodied navigation that integrates semantics and language (e.g., *“VLFM: Vision-language frontier maps for zero-shot semantic navigation”*, 2024), reflecting a broader agenda of building autonomous agents that can *move, perceive, and decide* in complex environments.\n\n---\n\n## Notable Works\n- **Maximum-entropy deep RL at scale:** In *“Soft actor-critic algorithms and applications”* (2018), Ha and collaborators established a highly influential off-policy actor–critic framework that improved **sample efficiency** and **stability** in continuous control, helping define modern practice for <u>robust model-free reinforcement learning</u>.\n- **Deep RL for dynamic legged locomotion (simulation to real world):** Through *“Learning to walk via deep reinforcement learning”* (2018) and the deployment-focused *“Learning to walk in the real world with minimal human effort”* (2020), he advanced end-to-end learned locomotion policies and demonstrated pathways for transferring learned behaviors from simulation into physical robots with *reduced engineering overhead*.\n- **Open, physically grounded simulation infrastructure:** With *“DART: Dynamic animation and robotics toolkit”* (2018), Ha contributed a widely used open-source platform connecting **robotics** and **graphics** communities, enabling reproducible research in <u>rigid-body dynamics, control, and animation</u> and supporting downstream work in learning and planning.\n\n---\n\n## Academic Background\nAt Georgia Tech, Ha has developed an internationally visible research program evidenced by a sustained record of highly cited publications (over **9,000 citations**) spanning premier topics in reinforcement learning, locomotion, and simulation. His publication trajectory reflects deep roots in **computer graphics and physics-based character animation**—including early work on physically simulated falling and landing behaviors (e.g., *“Falling and landing motion control for character animation”*, 2012)—followed by an expansion into **robotics** and **learning-based control** for legged and humanoid systems. His collaborative footprint—evident in community-facing software such as *DART* and in cross-cutting contributions that connect algorithms (e.g., SAC), systems (real-world locomotion pipelines), and evaluation methodology (e.g., later work on navigation evaluation guidelines)—suggests strong interdisciplinary affiliations within Georgia Tech’s robotics and graphics ecosystems and sustained engagement with broader academic and open-source communities."}, "Q_4d9N0AAAAJ": {"keywords": "Computer Graphics, Robotics, Scientific Visualization, Biological Simulation, Surface Reconstruction, Polygon Mesh Processing, Level-of-Detail Simplification, Texture Synthesis, Implicit Surfaces, Physics-Based Simulation", "summary": "## Overview\nGreg Turk is a Professor at Georgia Tech, where he leads and collaborates within a research program spanning **Computer Graphics**, **Robotics**, and **Scientific Visualization**. His work is unified by a commitment to building *computational representations and algorithms* that bridge sensed or simulated data with controllable, high-fidelity digital models—ranging from <u>3D surface reconstruction</u> and <u>geometry processing</u> to <u>physics-based animation</u> and modern <u>learning-based control</u>. Across a career reflected in **24,460 citations**, Turk has repeatedly shaped foundational techniques for converting raw measurements (e.g., range images and point clouds) into robust surface models, synthesizing realistic visual detail (textures on images and surfaces), and extending physically grounded simulation and optimization into robotics and character motion.\n\n---\n\n## Research Areas\nTurk’s research spans several tightly connected areas of graphics and robotics, anchored in the problem of turning complex data into usable models and behaviors. In **geometry acquisition and reconstruction**, he helped define practical pipelines for building watertight meshes from partial scans, most notably in “**Zippered polygon meshes from range images**” (1994), which addresses self-occlusion and multi-view integration by stitching range data into a coherent surface. He advanced **implicit surface representations** for large, noisy point sets through “**Multi-level partition of unity implicits**” (2005), enabling scalable reconstruction via local approximants blended into a global surface, and related work on *variational* and *interpolating* implicits (“Variational implicit surfaces,” 1999; “Modelling with implicit surfaces that interpolate,” 2002). In **mesh processing and level of detail**, Turk contributed influential simplification methods (“**Re-tiling polygonal surfaces**,” 1992; “**Simplification envelopes**,” 1996; “Fast and memory efficient polygonal simplification,” 1998) that formalized error-bounded approximation and efficient decimation—techniques central to interactive rendering and real-time applications. His contributions to **texture synthesis** and visual detail include “**Graphcut textures**” (2003), which uses graph cuts to compute optimal seams for patch-based image/video synthesis, and surface-aware pattern generation (“Texture synthesis on surfaces,” 2001; “Generating textures on arbitrary surfaces using reaction-diffusion,” 1991), emphasizing how geometry and parameterization shape perceived realism. In **scientific visualization**, he addressed perceptual and algorithmic control of visual encodings, such as streamline density management in “Image-guided streamline placement” (1996) and boundary-aware depiction in “LCIS” (1999). Turk has also made substantial contributions to **physics-based animation**—including coupled solid–fluid interaction (“Rigid fluid,” 2004), surface tracking with topology change (“Deforming meshes that split and merge,” 2009), and material models for melting and viscoplastic flow (“Melting and flowing,” 2002; “A finite element method for animating large viscoplastic flow,” 2007). More recently, his work intersects **robot learning** and sim-to-real control, exemplified by “**Preparing for the unknown: Learning a universal policy with online system identification**” (2017) and subsequent studies on locomotion and transfer (“Learning symmetric and low-energy locomotion,” 2018; “Sim-to-real transfer for biped locomotion,” 2019), reflecting a sustained interest in robust behavior under uncertainty.\n\n---\n\n## Notable Works\n- **Range-data to mesh reconstruction:** “**Zippered polygon meshes from range images**” (1994) introduced a practical, influential approach for merging multiple partial range scans into a single coherent polygonal model, addressing occlusion and alignment to support reliable <u>3D digitization</u>.\n- **Patch-based synthesis with optimal seams:** “**Graphcut textures: Image and video synthesis using graph cuts**” (2003) helped establish graph-cut-based seam optimization as a standard tool for high-quality <u>texture synthesis</u> in images and video, enabling visually plausible compositing from example data.\n- **Scalable implicit modeling from massive point sets:** “**Multi-level partition of unity implicits**” (2005) advanced <u>implicit surface reconstruction</u> by blending local quadratic approximations via a multi-level partition-of-unity framework, supporting robust modeling from very large, noisy point clouds.\n\n---\n\n## Academic Background\nAs a long-standing faculty member at Georgia Tech, Turk’s academic trajectory reflects sustained leadership across core areas of computer graphics, from early systems and rendering-era contributions (e.g., “Pixel-planes 5,” 1989) to foundational work in mesh processing, reconstruction, and texture synthesis through the 1990s and 2000s, and into contemporary learning-driven robotics and control. His publication record suggests broad interdisciplinary collaboration—linking graphics with vision-style acquisition (range imagery and point sets), computational physics (fluid/solid coupling and deformable materials), and robotics (policy learning, system identification, and sim-to-real transfer). The scale and longevity of his impact—evidenced by extensive citations and multiple highly cited papers across distinct subfields—indicate a career marked by influential methods, widely adopted algorithms, and deep affiliation with the computer graphics research community alongside expanding contributions to robotics and physically grounded simulation."}, "Q_Pud88AAAAJ": {"keywords": "active learning, optimal experimental design, data-centric machine learning, data subset selection, core-set selection, multimodal dataset curation, concept bottleneck models, interpretable machine learning, weak supervision (programmatic labeling), label-efficient finetuning of large language models", "summary": "## Overview\nStephen Mussmann is an Assistant Professor in **Computer Science** at the Georgia Institute of Technology, where he leads research at the intersection of **active learning**, **experimental design**, and **data-centric machine learning**. His work focuses on making modern ML systems more *label-efficient*, *interactive*, and *auditable* by developing methods for <u>data selection</u>, <u>dataset construction</u>, and <u>human-in-the-loop supervision</u> that improve performance while reducing annotation and computational cost. Across a publication record with **3,366 citations**, Mussmann has advanced both the theoretical foundations and practical tooling needed to treat data—not only models—as a first-class object of optimization.\n\n---\n\n## Research Areas\nMussmann’s research spans several tightly connected themes centered on how learning systems acquire and use information. A major thrust is **data selection and active learning** for deep models, exemplified by *Selection via Proxy: Efficient Data Selection for Deep Learning* (2019), which addresses the computational barriers of core-set selection by using proxy representations to make subset selection feasible at scale. He has also contributed to a more mechanistic understanding of active learning behavior and guarantees, including analyses of uncertainty sampling in *On the relationship between data efficiency and error for uncertainty sampling* (2018) and *Uncertainty sampling is preconditioned stochastic gradient descent on zero-one loss* (2018), as well as algorithmic developments such as *Active learning with expected error reduction* (2022). A second major area is **data-centric evaluation and benchmarking**, including *LabelBench: A comprehensive framework for benchmarking adaptive label-efficient learning* (2023), which systematizes comparisons across adaptive data collection strategies. Mussmann’s data-centric perspective extends to **dataset design for foundation models**, most visibly through *DataComp: In search of the next generation of multimodal datasets* (2023), which reframes multimodal progress as a function of dataset construction choices rather than architecture alone. Complementing these data-centric directions, he has contributed to **interpretability and concept-based interaction** through *Concept Bottleneck Models* (2020), which formalizes learning pipelines that predict intermediate human-meaningful concepts to enable <u>concept-level intervention</u>. Additional lines of work include scalable inference for log-linear models via similarity search (*Learning and inference via maximum inner product search*, 2016; *Fast amortized inference…*, 2017) and the design of label-efficient finetuning protocols for LLMs (*An experimental design framework for label-efficient supervised finetuning of large language models*, 2024), reflecting a consistent emphasis on <u>efficient information acquisition</u> under real-world constraints.\n\n---\n\n## Notable Works\n- **Concept-based, intervention-friendly interpretability:** In *Concept Bottleneck Models* (2020), Mussmann helped establish a widely used framework for building predictors that route decisions through explicit concept predictions, enabling *post-hoc auditing* and <u>counterfactual concept manipulation</u> (e.g., changing whether a “bone spur” is present) to study model behavior.\n- **Efficient subset selection for deep learning pipelines:** In *Selection via Proxy: Efficient Data Selection for Deep Learning* (2019), he advanced practical methods for scalable data selection by avoiding repeated expensive representation learning, helping make active learning and core-set ideas usable in modern deep learning settings.\n- **Re-centering progress on dataset design for multimodal foundation models:** Through *DataComp: In search of the next generation of multimodal datasets* (2023), he contributed to a community-scale effort that treats <u>multimodal dataset construction</u> as an experimental variable, providing a testbed and methodology for comparing data curation strategies that underpin systems like CLIP-style models.\n\n---\n\n## Academic Background\nMussmann’s publication trajectory indicates an evolution from early work on scalable modeling and structure in complex systems (e.g., degree dependence and assortativity in network models, 2014–2015; trajectory mining, 2015) toward the computational and statistical foundations of modern machine learning. His mid-career contributions emphasize efficient learning and inference, including similarity-search-based acceleration for log-linear models (2016–2017) and rigorous analyses of active learning dynamics and data efficiency (2018 onward). In parallel, he has engaged with evaluation methodology and bias in NLP metrics (*The price of debiasing automatic metrics in natural language evaluation*, 2018), reflecting an enduring interest in how measurement choices shape scientific conclusions. As his work matured, it increasingly centered on **data-centric ML**—from programmatic and interactive labeling (*Interactive programmatic labeling for weak supervision*, 2019) to benchmarking and systems for adaptive labeling (*LabelBench*, 2023; *VOCALExplore*, 2023) and experimental design for label-efficient LLM finetuning (2024). Now at Georgia Tech as an Assistant Professor, Mussmann is positioned within a strong interdisciplinary ecosystem spanning machine learning, systems, and human-centered computing, and his high-impact contributions—especially to concept-based modeling, data selection, and dataset design—underscore a sustained commitment to advancing <u>label-efficient</u> and *practically deployable* machine learning."}, "S3yqENkAAAAJ": {"keywords": "HCI, Health, Human-Centered AI, Future of Work, Feminist HCI, Intersectionality, Global South HCI4D, Care work and care infrastructures, Community health and public services, Algorithmic accountability and contestability", "summary": "## Overview\nNaveena Karusala is a researcher at the Georgia Institute of Technology, working at the intersection of **Human-Computer Interaction (HCI)**, **health**, and **Human-Centered AI** with a sustained commitment to *equity-oriented* and *community-engaged* scholarship. Her work centers <u>feminist and intersectional approaches to technology design</u>, examining how sociotechnical systems shape—and are shaped by—gender, class, labor, and institutional power, particularly in Global South and other historically marginalized contexts. Across studies of women’s safety, community health infrastructures, and emerging AI governance, Karusala’s research advances **critical HCI** and **CSCW** perspectives that foreground *care*, *justice*, and <u>accountability in high-stakes settings</u>.\n\n---\n\n## Research Areas\nKarusala’s research spans several tightly connected areas. In **gender, safety, and feminist HCI**, she has examined public safety technologies and their lived consequences, including an in-depth qualitative analysis of panic-button interventions in *“Women’s safety in public spaces: Examining the efficacy of panic buttons in New Delhi”* (2017), which interrogates how safety infrastructures interact with women’s everyday mobility and risk. In **intersectionality and design theory**, she has contributed conceptual and methodological guidance for HCI through *“Designing for intersections”* (2018) and *“Intersectional computing”* (2019), operationalizing intersectionality for technology design and arguing for more rigorous engagement with overlapping structures of oppression and identity in computing research. In **health, care, and community infrastructures**, her work analyzes how care is organized across institutions and informal networks—e.g., *“Bridging disconnected knowledges for community health”* (2018) on Mohalla Clinics and partial perspectives in care ecologies, and *“Mental health in the global south: challenges and opportunities in HCI for development”* (2019), which maps the complex sociocultural and infrastructural constraints shaping mental health interventions. She also investigates **digital labor and the future of work** in care settings, including workplace appropriation of consumer messaging tools in *“Making chat at home in the hospital: Exploring chat use by nurses”* (2020) and broader theorization in *“The future of care work: towards a radical politics of care in CSCW research and practice”* (2021). More recently, her scholarship extends to **responsible and contestable AI** in public services, examining how contestation is enacted at the margins in *“Understanding Contestability on the Margins: Implications for the Design of Algorithmic Decision-making in Public Services”* (2024) and engaging normative questions around explanation and governance in *“Legally-informed explainable ai”* (2025). Complementing these themes, Karusala has also advanced **meta-research on knowledge production** in HCI through *“Braving citational justice in human-computer interaction”* (2021) and related work on citational politics, emphasizing how scholarly practices shape whose knowledge becomes legible and authoritative.\n\n---\n\n## Notable Works\n- **Reframing women’s safety technologies through feminist HCI:** In *“Women’s safety in public spaces: Examining the efficacy of panic buttons in New Delhi”* (2017), Karusala and collaborators critically assess panic-button interventions, showing how public safety tools can misalign with women’s situated needs and the realities of urban infrastructures, thereby advancing <u>feminist critiques of techno-solutionism</u> in safety design.\n\n- **Operationalizing intersectionality for HCI research and practice:** Through *“Designing for intersections”* (2018) and *“Intersectional computing”* (2019), she provides a foundational agenda for integrating **intersectional theory** into design and evaluation, offering HCI a vocabulary and analytic stance for addressing layered inequities rather than treating identity categories in isolation.\n\n- **Building a politics of care across health infrastructures and labor:** In *“The future of care work: towards a radical politics of care in CSCW research and practice”* (2021), alongside empirical work such as *“Making chat at home in the hospital: Exploring chat use by nurses”* (2020), she articulates how technologies restructure care relations and working conditions, foregrounding <u>care work as sociotechnical labor</u> with ethical, political, and organizational stakes.\n\n---\n\n## Academic Background\nAffiliated with the Georgia Institute of Technology, Karusala’s publication trajectory indicates sustained engagement with leading venues in **HCI** and **CSCW** (including CHI- and CSCW-adjacent communities), with a research portfolio spanning 2015–2025 and accumulating approximately **1505 citations**. Her early work reflects deep experience in qualitative and ethnographic inquiry in India (e.g., Mumbai, Delhi, rural West Bengal), including studies of learning environments (*“Care as a resource in underserved learning environments,”* 2017), women’s safety (2017), and community health infrastructures (2018), suggesting formative training in field-based, interpretivist methods and participatory orientations. Over time, her scholarship expands into cross-regional and comparative concerns (including work situated in Kenya and the United States), as well as field-building efforts around **HCI Across Borders** and critical reflection on disciplinary practice via **citational justice** (2020–2022). Her more recent contributions on **contestability**, **AI for social good partnerships**, and **legally-informed explainable AI** signal an evolving agenda that bridges socio-legal questions with human-centered system design, while maintaining a consistent emphasis on *justice*, *care*, and <u>institutional accountability</u> in technology-mediated public life."}, "TIKl_foAAAAJ": {"keywords": "Machine Learning, Offline Reinforcement Learning, Off-Policy Evaluation, Stationary Distribution Correction (DICE), Structured Data Kernels, Variational Autoencoders for Discrete Structures, Graph Neural Networks, Molecular Generation and Retrosynthesis, Score-Based Diffusion Models, Knowledge Graph Reasoning and Completion", "summary": "## Overview\nBo Dai is a machine learning researcher affiliated with **Google Brain** and **Georgia Tech**, where he contributes to the development of principled and scalable learning algorithms at the intersection of **probabilistic modeling**, **reinforcement learning**, and **representation learning**. His work centers on building *theoretically grounded* methods that remain effective under modern constraints—especially limited interaction, large-scale data, and structured/discrete domains—often by leveraging <u>duality</u>, <u>kernel embeddings</u>, and <u>generative modeling</u> to connect statistical estimation with practical learning systems.\n\n---\n\n## Research Areas\nDai’s research spans several tightly connected areas of contemporary machine learning. A major theme is **learning with structured and discrete data**, developing models that respect formal constraints such as syntax and semantics; this is exemplified by the *Syntax-directed variational autoencoder for structured data* (2018), which addresses the challenge of generating valid objects (e.g., programs and molecular structures) by integrating grammar-aware latent-variable modeling. He has also made foundational contributions to **kernel methods and distribution embeddings** for structured prediction and conditional distribution learning, including *Discriminative embeddings of latent variable models for structured data* (2016) and *Learning from Conditional Distributions via Dual Embeddings* (2016), which connect latent-variable structure to discriminative learning via embedding-based representations. In **offline reinforcement learning and off-policy evaluation**, Dai is strongly associated with the DICE line of work—*DualDICE* (2019), *AlgaeDICE* (2019), and *GenDICE* (2020)—which targets <u>stationary distribution correction</u> and behavior-agnostic estimation when only logged data are available. Complementing these contributions, his research includes **stable RL optimization with function approximation** (e.g., *SBEED: Convergent reinforcement learning with nonlinear function approximation*, 2018), as well as newer directions in **foundation-model-driven agents and planning**, such as *Learning universal policies via text-guided video generation* (2023) and *Adaplanner: Adaptive planning from feedback with language models* (2023), which explore how generative models and language feedback can guide generalizable decision-making.\n\n---\n\n## Notable Works\n- **Kernel/distribution-embedding foundations for structured learning:** In *Discriminative embeddings of latent variable models for structured data* (2016), Dai helped establish influential embedding-based techniques for bringing latent-variable structure into discriminative learning for complex objects (e.g., sequences, trees, graphs), strengthening the bridge between probabilistic modeling and scalable prediction.\n- **Grammar- and semantics-aware deep generative modeling:** In *Syntax-directed variational autoencoder for structured data* (2018), he advanced structured generative modeling by incorporating formal constraints to improve validity and controllability when generating discrete structured objects such as programs or molecules.\n- **Behavior-agnostic offline RL evaluation via stationary correction:** In *DualDICE: Behavior-agnostic estimation of discounted stationary distribution corrections* (2019) (and its related DICE successors), he introduced a widely used approach to off-policy evaluation in the fixed-dataset setting, emphasizing <u>distribution correction</u> and principled estimation without requiring behavior-policy knowledge.\n\n---\n\n## Academic Background\nAcross a publication record spanning kernel methods, probabilistic inference, structured generative modeling, and reinforcement learning, Dai’s trajectory reflects training and collaboration patterns typical of a researcher bridging **academic ML** and **industrial research**. His dual affiliation with **Georgia Tech** and **Google Brain** situates him within both a major research university environment and a leading industrial lab, enabling work that combines *theory-informed* algorithm design with large-scale empirical validation. Early highly cited contributions such as *Scalable kernel methods via doubly stochastic gradients* (2014) and embedding-based structured learning papers (2016) suggest a strong foundation in statistical learning theory and scalable optimization, while subsequent influential RL works—particularly the DICE family (*DualDICE*, 2019; *AlgaeDICE*, 2019; *GenDICE*, 2020) and stability-focused methods like *SBEED* (2018)—indicate sustained leadership in offline RL and off-policy evaluation. With **9,373 citations**, his impact is marked by repeated contributions that become methodological reference points, and by active engagement across communities including machine learning, reinforcement learning, and structured/graph-based modeling through cross-institutional collaborations and widely adopted algorithmic frameworks."}, "TaR3dq4AAAAJ": {"keywords": "hci, mHealth, social computing, cognitive science, autism spectrum disorder, assistive technology, pediatric chronic disease management, technology-supported caregiving, telehealth and remote diagnosis, large language models for human behavior simulation", "summary": "## Overview\nRosa I. Arriaga is an **Associate Professor** in **Interactive Computing** at **Georgia Tech**, where she leads research at the intersection of **Human–Computer Interaction (HCI)**, **mHealth**, and **social computing**, grounded in *cognitive science*. Her scholarship centers on designing and evaluating technologies that support people in complex, real-world contexts—particularly children and families managing chronic illness, and individuals with autism and their care networks—while also advancing methodological approaches for studying human behavior with computation. Across this agenda, Arriaga emphasizes <u>ecologically grounded design</u>, *stakeholder-centered evaluation*, and the careful translation of theory into deployable systems.\n\n---\n\n## Research Areas\nArriaga’s research spans three tightly connected areas. First, she develops and studies <u>technology-supported health management</u> for pediatric and family contexts, including mobile and SMS-based interventions for chronic illness management such as asthma and type 1 diabetes (e.g., “Using SMS to provide continuous assessment and improve health outcomes for children with asthma” (2012) and “A pilot randomized trial of text-messaging for symptom awareness and diabetes knowledge in adolescents with type 1 diabetes” (2015)). This work is complemented by design frameworks that account for the lived realities of families, such as “Using an ecological framework to design mobile technologies for pediatric asthma management” (2009), and empirical studies of documentation and tracking practices (e.g., “Grow and know…” (2007) and “Baby steps…” (2009)). Second, she is a leading contributor to <u>social and assistive computing for autism</u>, creating systems that mobilize social support and practical advice for daily living—ranging from social-network designs for independence (“Designing a social network to support the independence of young adults with autism” (2012); “Investigating the use of circles in social networks…” (2013)) to crowdsourced and community-mediated support (“In-group questions and out-group answers…” (2015)) and tools for capturing behavior and enabling remote assessment (“A novel system for supporting autism diagnosis using home videos…” (2015)). Third, Arriaga’s work engages foundational questions in *cognition and learning*—including algorithmic perspectives on concept learning (“An algorithmic theory of learning: Robust concepts and random projection” (2006))—and, more recently, methodological innovation in computational social science and HCI, exemplified by her highly cited contribution on using large language models to simulate participants and replicate human-subject findings (“Using large language models to simulate multiple humans and replicate human subject studies” (2023)).\n\n---\n\n## Notable Works\n- **Methodological innovation for HCI and behavioral science with LLMs:** In “**Using large language models to simulate multiple humans and replicate human subject studies**” (2023), Arriaga and collaborators introduce the *Turing Experiment* paradigm to evaluate the extent to which language models can reproduce patterns from human-subject studies, while surfacing systematic distortions—work that has become influential for <u>AI-assisted experimentation</u> and validity discussions in HCI.\n- **Ecological, family-centered mHealth for pediatric chronic illness:** Through a sequence of studies including “**Using an ecological framework to design mobile technologies for pediatric asthma management**” (2009) and “**Using SMS to provide continuous assessment and improve health outcomes for children with asthma**” (2012), Arriaga helped establish theory-driven, real-world approaches to supporting pediatric asthma management via mobile communication and continuous assessment.\n- **Social computing systems to support independence in autism:** In “**Designing a social network to support the independence of young adults with autism**” (2012) (and related work such as “Investigating the use of circles in social networks…” (2013)), Arriaga advances a design agenda for <u>networked support</u> that operationalizes social resources—family, peers, and caregivers—to scaffold daily living and independence.\n\n---\n\n## Academic Background\nArriaga’s academic trajectory reflects a distinctive integration of *cognitive science*, developmental and learning research, and applied **HCI**. Her early, highly cited contributions address language and conceptual development and developmental cognition (e.g., “Scores on the MacArthur communicative development inventory…” (1998) and “Number discrimination in 10-month-old infants” (2007)), alongside foundational computational perspectives on learning (“An algorithmic theory of learning…” (2006)). As her work increasingly engaged real-world stakeholders, she built a sustained research program in Interactive Computing that bridges theory, design, and deployment—evidenced by long-running collaborations across healthcare, education, and disability communities and by iterative, field-based evaluation methods in studies of pediatric health management, autism support, and remote assessment tools (e.g., “A novel system for supporting autism diagnosis using home videos…” (2015)). Her publication record—spanning HCI venues and interdisciplinary outlets—together with approximately **4,530 citations**, indicates broad influence across **HCI**, **mHealth**, **social computing**, and *cognitive science*, and positions her as a key academic voice in <u>human-centered, context-aware computing</u> for health and wellbeing at Georgia Tech."}, "U-Dk-1QAAAAJ": {"keywords": "Computer Science Education, Computational Thinking, K-12 STEM Integration, Teacher Professional Development, Block-Based Programming, Data-Driven Hint Generation, Intelligent Tutoring Systems, Novice Programming Support, Parsons Problems, Educational Games and Gamified Learning Environments", "summary": "## Overview\nNicholas Lytle is a Georgia Tech–affiliated computing education researcher whose work sits at the intersection of **computer science education** and *learning technologies* for broadening participation in computing. Across K–12 and introductory programming contexts, his scholarship emphasizes <u>integrating computational thinking into core disciplinary courses</u>, designing **teacher-facing professional development** to support sustainable classroom adoption, and developing *data-driven instructional supports* (e.g., automated hints, worked-example feedback, and scaffolds) that improve novices’ learning trajectories. His publication record—spanning CT-infused curriculum design, block-based programming ecosystems, and intelligent tutoring–style feedback—reflects a consistent focus on <u>scalable, classroom-realistic approaches</u> that help both teachers and learners engage meaningfully with computing.\n\n---\n\n## Research Areas\nLytle’s research portfolio is anchored in CT infusion and the design of learning progressions that make computing viable in non-CS classrooms. In **“PRADA: A practical model for integrating computational thinking in K-12 education” (2019)**, he articulates a pragmatic integration model that addresses the dual challenge teachers face: learning CT while simultaneously determining how to embed CT practices into existing disciplinary instruction. Complementing this integration lens, **“Use, modify, create: Comparing computational thinking lesson progressions for STEM classes” (2019)** examines how structured progressions can support learners with limited prior programming experience and teachers who may be new to CT, elaborating design tradeoffs between adopting existing artifacts, adapting them, and culminating in original creation. This line of work extends into teacher learning and implementation supports through the **“Code, connect, create: The 3c professional development model to support computational thinking infusion” (2020)** and analyses of teacher programming products in PD (e.g., *“Infusing computing: Analyzing teacher programming products in K-12 computational thinking professional development,” 2019*), foregrounding how teachers operationalize CT ideas in tools such as Snap!. In parallel, Lytle contributes to intelligent support for novice programming via data-driven methods: he evaluates the effectiveness and quality of automated next-step hints (e.g., **“The impact of adding textual explanations to next-step hints in a novice programming environment,” 2019**; **“A comparison of the quality of data-driven programming hint generation algorithms,” 2019**), explores how data quantity and provenance shape hint utility (*2018*), and proposes approaches that reduce problem state spaces through feature detection (*2018*) to make adaptive support more interpretable and feasible. His broader agenda also includes classroom-ready scaffolds and environments—such as Parsons-problem–supported practice in **“Crescendo” (2020)** and work on block-based scaffolding and collaboration (e.g., pair programming designs and remote collaboration during the pandemic)—as well as emerging efforts in online CS education research ecosystems (e.g., marketplace models for research opportunities and analyses of modality-based performance patterns).\n\n---\n\n## Notable Works\n- **PRADA integration model for CT infusion** — In *“PRADA: A practical model for integrating computational thinking in K-12 education” (2019)*, Lytle advances a classroom-oriented framework that helps teachers translate CT from an abstract goal into actionable integration decisions within K–12 disciplinary courses, directly addressing feasibility and teacher capacity constraints.\n- **Evidence on CT lesson progressions (Use–Modify–Create)** — In *“Use, modify, create: Comparing computational thinking lesson progressions for STEM classes” (2019)* (and later extensions evaluating UMC, 2020), he provides comparative insight into progression-based instructional design for CT in STEM classes, clarifying how staged engagement can support novices and reduce barriers for instructors new to programming.\n- **Teacher professional development model for sustainable CT infusion (3C)** — In *“Code, connect, create: The 3c professional development model to support computational thinking infusion” (2020)*, Lytle offers a structured PD approach that links teachers’ programming learning to disciplinary connections and classroom-ready creation, reinforced by empirical analyses of teacher-created Snap! artifacts (*“Infusing computing…,” 2019*).\n\n---\n\n## Academic Background\nBased on his Georgia Tech affiliation and sustained publication trajectory in computing education venues, Lytle’s academic profile aligns with a research pathway in **learning sciences/CS education** within a computing school context, with extensive collaboration across K–12 partners and teacher professional learning initiatives. His record indicates deep engagement with both *design-based educational research* (e.g., CT-infused curricula and PD models such as PRADA and 3C) and **data-driven learning technology** research (e.g., scalable hint generation, help-need prediction, and state-space reduction for adaptive support), suggesting training and scholarly identity spanning computer science, education research methods, and intelligent tutoring/learning analytics traditions. The breadth of his work—from middle-grade science CT infusion (*“Infusing computational thinking into middle grade science classrooms: lessons learned,” 2018*) to novice programming supports and block-based classroom systems—reflects strong ties to interdisciplinary education research communities and practitioner-facing partnerships. With **931 citations** and multiple highly cited contributions from 2018–2020 that helped shape discourse on CT integration and teacher support, his impact is evidenced through widely referenced models and empirical studies that continue to inform curriculum design, professional development, and scalable support for novice programmers in both K–12 and evolving online learning contexts."}, "VIwtdckAAAAJ": {"keywords": "Distributed Computing Systems, Database Systems, Privacy-Security-Trust, Cloud Computing, Big Data Systems/Data Analytics, Reputation and Trust Management in P2P Networks, Location Privacy and k-Anonymity, Federated Learning Privacy and Security, Differential Privacy, Blockchain Security and Privacy", "summary": "## Overview\nLing Liu is a senior faculty member at the **Georgia Institute of Technology**, where she leads and collaborates within a research group focused on **distributed computing** and **data-intensive systems**. Her scholarship centers on designing *scalable* and *trustworthy* platforms that support modern data and AI workloads—spanning **database systems**, **cloud computing**, and **privacy–security–trust**. Across more than 43,000 citations, Liu’s work has repeatedly shaped how researchers and practitioners think about <u>trust management</u>, <u>privacy-preserving data services</u>, and <u>secure decentralized infrastructures</u> in large-scale networked environments.\n\n---\n\n## Research Areas\nLiu’s research portfolio bridges foundational system-building with security and privacy mechanisms for real-world, high-stakes deployments. A major thread is **trust and reputation in decentralized systems**, exemplified by *PeerTrust: Supporting reputation-based trust for peer-to-peer electronic communities* (2004) and follow-on work such as *TrustGuard* (2005) and *TrustMe* (2003), which investigate how to compute, defend, and manage trust signals under adversarial and incentive-misaligned conditions. A second pillar is **location privacy** in mobile and road-network settings, where she advanced *personalized k-anonymity* through both a formal model and system architectures (e.g., *Location privacy in mobile systems: A personalized anonymization model*, 2005; *Protecting location privacy with personalized k-anonymity: Architecture and algorithms*, 2008) and complementary query frameworks such as *PrivacyGrid* (2008) and mix-zone approaches (e.g., *MobiMix*, 2011). More recently, her work has expanded into **security and privacy for emerging data/AI infrastructures**, including <u>blockchain security</u> (*Security and privacy on blockchain*, 2019; *Security and privacy for healthcare blockchains*, 2021) and <u>privacy and robustness in federated learning</u>—covering attack surfaces and defenses (*Data poisoning attacks against federated learning systems*, 2020; *A framework for evaluating client privacy leakages in federated learning*, 2020), formal privacy mechanisms (*LDP-Fed: Federated learning with local differential privacy*, 2020), and empirical/analytical studies of inference risks (*Demystifying membership inference attacks in machine learning as a service*, 2019). Complementing these themes are contributions to **web and XML data integration** (*XWRAP*, 2000; *WebCQ*, 2000), **cloud and big-data performance** (e.g., *Purlieus*, 2011; studies of virtualization interference, 2010), and **RDF/semantic data management** (e.g., *TripleBit*, 2013; semantic hash partitioning, 2013), reflecting a sustained emphasis on end-to-end systems that remain efficient under scale and resilient under threat.\n\n---\n\n## Notable Works\n- **PeerTrust reputation framework for decentralized communities** — In *“Peertrust: Supporting reputation-based trust for peer-to-peer electronic communities”* (2004), Liu and collaborators introduced a widely cited reputation-based trust model for P2P environments, helping define core design principles for trust computation and robustness in open, decentralized marketplaces and communities.\n- **Personalized k-anonymity for mobile location privacy** — Through *“Location privacy in mobile systems: A personalized anonymization model”* (2005) and the system-level extension *“Protecting location privacy with personalized k-anonymity: Architecture and algorithms”* (2008), she established a personalization-centric approach to location anonymization that balances utility and protection against diverse privacy threats in location-based services.\n- **Security and privacy foundations for blockchain ecosystems** — In *“Security and privacy on blockchain”* (2019), Liu helped synthesize and structure the security/privacy landscape of blockchain systems, clarifying threat models and research directions that later informed domain-specific efforts such as healthcare blockchain security and privacy (2021).\n\n---\n\n## Academic Background\nAt Georgia Tech, Ling Liu has built an internationally visible research program at the intersection of **distributed systems**, **databases**, and **security/privacy**, with impact evidenced by extensive citation metrics and sustained publication across multiple generations of computing platforms (from web information integration and continual queries to cloud, blockchain, and federated learning). Her scholarly trajectory suggests deep interdisciplinary affiliations across systems and security communities—consistent with contributions ranging from reference scholarship (e.g., the *Encyclopedia of Database Systems*, 2009) to high-impact research on trust management, privacy-preserving data services, and secure AI. The breadth of her work—spanning P2P trust, mobile privacy, cloud performance, semantic data systems, and adversarial/privatized learning—reflects a career shaped by long-term engagement with major academic venues and collaborative networks that connect **data management** and **cybersecurity** to emerging socio-technical demands."}, "Vbv-JEwAAAAJ": {"keywords": "Service Computing, Cloud Computing, Big Data, Data Management, Information Security, Buffer Overflow Defense, Virtualization Performance Isolation, Autonomic Resource Management, Web Information Extraction, Differential Privacy", "summary": "## Overview\nCalton Pu is a **Professor** and the **J.P. Imlay, Jr. Chair in Software** in the School of Computer Science at **Georgia Tech**, where he leads research at the intersection of *systems software* and *data-intensive computing*. His work spans **service computing**, **cloud computing**, **big data/data management**, and **information security**, with a sustained emphasis on making large-scale computing infrastructures more dependable, efficient, and secure. Across decades of influential contributions (with **24,041 citations**), Pu has advanced foundational ideas in <u>systems security</u>, <u>distributed data consistency</u>, and <u>cloud performance isolation</u>, often translating core principles into practical mechanisms that improve real-world platforms.\n\n---\n\n## Research Areas\nPu’s research portfolio integrates multiple systems domains, unified by a focus on rigorous mechanisms for correctness, performance, and protection in distributed and virtualized environments. In **information security**, his work helped define modern defenses against memory-corruption exploits through compiler-supported protection in “**Stackguard: Automatic adaptive detection and prevention of buffer-overflow attacks**” (1998) and the broader synthesis in “**Buffer overflows: Attacks and defenses for the vulnerability of the decade**” (2000), articulating threat models and defense strategies that shaped secure systems practice. In **web and data management**, he contributed to extracting and integrating semi-structured web data via “**XWRAP: An XML-enabled wrapper construction system for web information sources**” (2000) and to internet-scale monitoring through *standing query* paradigms in “**Continual queries for internet scale event-driven information delivery**” (1999), enabling timely detection and delivery of changes in dynamic information sources. In **distributed systems and data consistency**, Pu advanced replica-control theory with bounded inconsistency—most notably “**Replica control in distributed systems: an asynchronous approach**” (1991)—and related work on <u>epsilon-serializability</u>, providing principled trade-offs between strict consistency and availability/performance. In **cloud computing and virtualization**, he investigated performance isolation and contention in multi-tenant environments (e.g., “**An analysis of performance interference effects in virtual environments**,” 2007; and “**Understanding performance interference of i/o workload in virtualized cloud environments**,” 2010) and developed adaptive resource-management approaches such as “**Mistral: Dynamically managing power, performance, and adaptation cost in cloud infrastructures**” (2010). More recently, his work extends into privacy-preserving machine learning, exemplified by “**Differentially private model publishing for deep learning**” (2019), reflecting a continued commitment to <u>security and privacy</u> in contemporary data-driven systems.\n\n---\n\n## Notable Works\n- **StackGuard and practical protection against stack smashing**: In “**Stackguard: Automatic adaptive detection and prevention of buffer-overflow attacks**” (1998), Pu and collaborators introduced a systematic compiler-based defense that detects and prevents classic buffer-overflow exploits, helping establish widely adopted approaches to <u>memory-safety hardening</u> in production toolchains.\n- **Continual queries for web-scale event-driven information delivery**: Through “**Continual queries for internet scale event-driven information delivery**” (1999), Pu advanced the concept of standing, event-driven queries (e.g., OpenCQ), enabling scalable monitoring and notification over rapidly changing internet information sources—an early foundation for modern streaming and alerting systems.\n- **Cloud/virtualization performance interference and adaptive infrastructure management**: Pu’s virtualization and cloud research—spanning “**An analysis of performance interference effects in virtual environments**” (2007) and “**Mistral: Dynamically managing power, performance, and adaptation cost in cloud infrastructures**” (2010)—clarified the causes of multi-tenant interference and proposed adaptive strategies for consolidating workloads while balancing <u>QoS</u>, power, and reconfiguration cost.\n\n---\n\n## Academic Background\nAs a long-standing faculty member at **Georgia Tech** and holder of the **J.P. Imlay, Jr. Chair in Software**, Calton Pu has built an internationally recognized research program in computer science with sustained impact across security, distributed systems, data management, and cloud computing. His publication record shows a clear evolution from foundational systems and transaction-processing research—such as early work on *open-ended activities* (“**Split-transactions for open-ended activities**,” 1988) and replicated data correctness (“**Replica control in distributed systems: an asynchronous approach**,” 1991)—to influential contributions in operating systems performance and specialization (e.g., “**Optimistic incremental specialization: Streamlining a commercial operating system**,” 1995), and later to cloud-era challenges in virtualization, consolidation, and interference-aware resource management. The breadth and longevity of his contributions, reflected in high citation counts and widely referenced systems papers, indicate sustained recognition by the research community, extensive collaboration across systems subfields, and a prominent role in shaping academic and practical approaches to <u>secure</u>, <u>data-intensive</u>, and <u>service-oriented</u> computing."}, "W8oVUI8AAAAJ": {"keywords": "Machine Learning, Natural Language Processing, Geotechnical Engineering, Granular Materials, Pore Space Characterization, Sphere Packing, Porosity and Void Ratio Modeling, Discrete Element Method (DEM), Post-Disaster Damage Assessment, Semantic Segmentation and Domain Adaptation", "summary": "## Overview\nMax Mahdi Roozbahani is a **Senior Lecturer** in the Georgia Institute of Technology’s **Computational Science and Engineering** unit, where he contributes to teaching and research at the intersection of data-driven computing and applied engineering systems. His scholarly profile centers on *computational modeling* and **machine learning** methods that translate complex, real-world structure into analyzable representations, with particular emphasis on <u>granular and porous media microstructure</u>, *data-centric assessment pipelines*, and the practical deployment of computational tools for high-impact engineering and societal applications. Across a publication record spanning geotechnical mechanics, disaster informatics, and computing education, Roozbahani’s work is characterized by an emphasis on **algorithmic rigor**, *measurable structure–property links*, and <u>scalable computational workflows</u>.\n\n---\n\n## Research Areas\nRoozbahani’s research areas integrate **geotechnical engineering** with modern **machine learning** and **natural language processing**-adjacent data practices (e.g., large-scale data curation, automated evaluation, and information extraction from heterogeneous sources). A major thread of his work develops computational techniques to quantify pore geometry and packing structure in particulate systems—most notably in “**Pore Size Distribution in Granular Material Microstructure**” (2017), which uses the **Euclidean Distance Transform (EDT)** to compute effective local pore size and polyhedral pore structure, and in “**Quantifying three-dimensional bodies and throats of particulate system pore space**” (2023), which advances more faithful characterization of pore bodies and throats for improved links to fluid flow and constitutive behavior. Complementing this microstructural focus, he has published extensively on **sphere packing and porosity**—including sensitivity to stochastic initialization (“**The effect of different random number distributions on the porosity of spherical particles**,” 2013) and boundary effects (“**Effect of rectangular container's sides on porosity for equal-sized sphere packing**,” 2012)—as well as particle-scale mechanisms relevant to filtration and soil behavior (“**Mechanical trapping of fine particles in a medium of mono‐sized randomly packed spheres**,” 2014; “**Size Effects on the Void Ratio of Loosely Packed Binary Particle Mixtures**,” 2014). In parallel, Roozbahani has extended computational methods to **disaster damage assessment** and post-event analytics, demonstrated by “**Black marble nighttime light data for disaster damage assessment**” (2023) and more recent work on social-media-based damage segmentation and dataset fidelity (2025). A further applied-computing line addresses **scalable education and assessment infrastructure**, such as automated evaluation of visualization assignments (“**VISGRADER: Automatic Grading of D3 Visualizations**,” 2023) and DevOps-enabled assignment validation at scale (2024), reflecting a broader interest in <u>reproducible, scalable computational ecosystems</u>.\n\n---\n\n## Notable Works\n- **Microstructure-aware pore quantification for porous media modeling:** In “*Pore Size Distribution in Granular Material Microstructure*” (2017), Roozbahani advances pore-scale characterization using **EDT-based** computation of local pore sizes and polyhedral pore geometry, supporting improved understanding of flow-relevant microstructure in granular materials.  \n- **Foundational computational studies of packing, porosity, and boundary/stochastic effects:** Through works such as “*The effect of different random number distributions on the porosity of spherical particles*” (2013) and “*Effect of rectangular container's sides on porosity for equal-sized sphere packing*” (2012), he clarifies how initialization distributions and container geometry influence porosity outcomes in loose packing simulations, strengthening the methodological basis for virtual packing studies.  \n- **Mechanistic links between particulate structure and filtration/void behavior:** In “*Mechanical trapping of fine particles in a medium of mono‐sized randomly packed spheres*” (2014) and “*Size Effects on the Void Ratio of Loosely Packed Binary Particle Mixtures*” (2014), he analyzes how fines content and particle-size effects reshape void structure and trapping behavior—results with direct implications for filtration systems and multisized geomaterials.\n\n---\n\n## Academic Background\nRoozbahani’s academic trajectory reflects sustained engagement with **computational geomechanics** and the algorithmic study of particulate media, beginning with early contributions on gravitational sphere packing and void ratio estimation (2012–2014) and expanding into broader microstructure characterization and interdisciplinary applications (2017–present). His appointment as **Senior Lecturer** in Georgia Tech’s **Computational Science and Engineering** indicates a dual emphasis on scholarship and large-scale instructional leadership, reinforced by publications on automated grading, DevOps-enabled course operations, and computing curriculum design (2023–2025). His publication history suggests active collaboration across engineering and computing communities—linking geotechnical infrastructure topics (e.g., materials in geotechnical systems; biologically inspired soil mechanics) with data-driven disaster assessment and scalable educational tooling. With **311 total citations**, his most-cited works have become reference points in computational characterization of <u>granular packing and pore geometry</u>, underscoring a research identity grounded in *method development* and **cross-domain computational translation** between physical structure and actionable analytical models."}, "WBRatTAAAAAJ": {"keywords": "data activism, ethic of care, critical data literacy, feminist HCI, data ethics education, participatory design, collaborative data work, open government data, civic technology, internet shutdowns", "summary": "## Overview\nAmanda Meng is a researcher at the Georgia Institute of Technology whose work sits at the intersection of **human-centered computing**, **data activism**, and *critical approaches to data*. Across ethnographic, participatory, and design-oriented scholarship, she investigates how communities and institutions *produce, contest, and mobilize data* in pursuit of democratic accountability and social justice. Her research foregrounds <u>an ethic of care</u> as a guiding concept for understanding and reshaping data practices—particularly in contexts where data work becomes a form of civic participation, community organizing, and public pedagogy.\n\n---\n\n## Research Areas\nMeng’s research areas span critical data studies, civic technology, and computing ethics education, with a sustained emphasis on how data practices are socially situated and politically consequential. In “**Grassroots resource mobilization through counter-data action**” (2018), she examines *data activism* in an Atlanta affordable-housing advocacy group, detailing how communities engage in “counter-data” practices to challenge dominant narratives and resource allocations. This civic orientation deepens in “**Collaborative Data Work Towards a Caring Democracy**” (2019), where two years of ethnographic fieldwork support a theory of collaborative data work as a means to enact and sustain *democratic caring*—linking everyday data practices to <u>caring democracy</u> and collective responsibility. Her work also interrogates professional and institutional settings of data practice: “**Care and the Practice of Data Science for Social Good**” (2018) articulates how care can reframe multi-stage data science workflows when multiple stakeholders and public-facing consequences are involved, while “**Interrogating Data Work as a Community of Practice**” (2022) uses Lave and Wenger’s framework to map participation, expertise, and marginality within data work communities. Alongside these contributions, Meng has advanced *critical data literacy* and ethics pedagogy through methods for teaching ethics in data science and computer science curricula, notably “**Re-Shape: A Method to Teach Data Ethics for Data Science Education**” (2020) and “**Using Role-Play to Scale the Integration of Ethics Across the Computer Science Curriculum**” (2021), which address the practical challenge of embedding ethics across technical training. Complementing her civic and educational work, she has published on open government data and contentious politics (e.g., “**The social impact of open government data in Hong Kong**,” 2019; “**Investigating the Roots of Open Data’s Social Impact**,” 2014) and on political communication and democratic participation via social media (e.g., “**Twitter democracy**,” 2015), as well as on infrastructural disruptions and governance (e.g., “**Destination Unreachable: Characterizing Internet Outages and Shutdowns**,” 2023).\n\n---\n\n## Notable Works\n- **Articulating care as a democratic and methodological orientation for data practice:** In “**Collaborative Data Work Towards a Caring Democracy**” (2019), Meng develops an empirically grounded account of how collaborative data work can cultivate <u>caring democracy</u>, connecting feminist ethics of care to the sustained labor of collecting, maintaining, and acting with data in civic life.  \n- **Advancing community-centered data activism through counter-data practices:** “**Grassroots resource mobilization through counter-data action**” (2018) documents how grassroots housing advocates in Atlanta mobilize data as an organizing resource—demonstrating how *counter-data action* can contest institutional power and reframe policy debates.  \n- **Building scalable pedagogical methods for data ethics and critical data literacy:** Through “**Re-Shape: A Method to Teach Data Ethics for Data Science Education**” (2020) (and related curriculum work such as “Using Role-Play to Scale the Integration of Ethics Across the Computer Science Curriculum,” 2021), Meng contributes concrete instructional approaches for embedding ethical reasoning throughout data science and computing education, emphasizing reflection, positionality, and socio-technical consequence.\n\n---\n\n## Academic Background\nBased at the Georgia Institute of Technology, Meng’s publication trajectory indicates an interdisciplinary formation spanning **HCI**, *civic technology*, and critical social inquiry, with recurring use of ethnography, engaged research, and participatory design across community and institutional settings. Her earlier work on open government data and democratic participation (2014–2016) and on social media in electoral and deliberative contexts (e.g., “Twitter democracy,” 2015; “Lessons in Social Election Monitoring,” 2016) suggests sustained affiliations with civic informatics and public-interest technology networks, later extending into community advocacy and urban policy analysis in Atlanta (e.g., studies of anti-displacement policy evaluation, 2020). The consistent emphasis on <u>critical data literacy</u>, **data activism**, and an **ethic of care**—paired with contributions to ethics integration in computing curricula—positions her as a scholar contributing both to theoretical development in critical data studies and to applied educational and civic interventions. With **569 total citations**, her work has achieved notable visibility across human-centered computing, data ethics, and civic data practice research communities."}, "WBvt5A8AAAAJ": {"keywords": "High Performance AI, Computer Vision, Multimodal, Creative AI, AI Systems, Semantic Segmentation, Vision Transformers, Diffusion Models, Text-to-Video Generation, Image Super-Resolution", "summary": "## Overview\nHumphrey Shi is a researcher affiliated with **NVIDIA** and holds academic ties to **Georgia Tech** and **UIUC**, where he has contributed to and collaborated across industry and university research environments. His work sits at the intersection of **Computer Vision**, **Multimodal learning**, and **High Performance AI**, with a sustained emphasis on building *efficient, scalable, and practically deployable* models. Across a highly cited publication record (≈**23,555** citations), Shi’s research advances <u>attention-based visual understanding</u>, <u>diffusion-driven creative AI</u>, and <u>AI systems-oriented model design</u>, spanning from core representation learning to generative and multimodal foundations.\n\n---\n\n## Research Areas\nShi’s research portfolio combines foundational model innovations with application-facing benchmarks and systems considerations. In dense prediction and segmentation, he is known for introducing efficient global-context modeling via **CCNet: Criss-Cross Attention for Semantic Segmentation** (2020), which targets full-image dependency capture without the prohibitive cost of full self-attention, and for pushing toward unified segmentation paradigms through **OneFormer: One Transformer to Rule Universal Image Segmentation** (2022). His work also addresses supervision scarcity and generalization, including **Revisiting Dilated Convolution** (2018) for weakly- and semi-supervised semantic segmentation, and transfer adaptation mechanisms such as **SpotTune: Transfer Learning through Adaptive Fine-tuning** (2019). In efficient vision architectures, he has contributed to transformer scalability through locality-aware attention—most notably **Neighborhood Attention Transformer** (2022) and **Dilated Neighborhood Attention Transformer** (2022)—as well as data/parameter efficiency via **Escaping the Big Data Paradigm with Compact Transformers** (2021). On the generative and creative AI side, Shi has co-developed influential diffusion-based methods for controllable and safe generation, including **Text2Video-Zero** (2023) for *zero-shot* text-to-video generation using pretrained text-to-image diffusion models, **More Control for Free! Image Synthesis with Semantic Diffusion Guidance** (2021) for semantic control, and **Forget-Me-Not** (2023) for post-hoc concept removal in diffusion models. Complementing these model contributions are widely used datasets and challenges that shape evaluation culture in vision, such as the **NTIRE 2017 single image super-resolution challenge** (2017) and domain benchmarks like **Agriculture-Vision** (2020).\n\n---\n\n## Notable Works\n- **CCNet: Criss-Cross Attention for Semantic Segmentation (2020)** — Proposed an efficient attention mechanism to model <u>full-image contextual dependencies</u> for dense prediction, substantially impacting segmentation architectures by reducing the cost of capturing long-range interactions while maintaining strong accuracy.\n- **Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators (2023)** — Helped establish *zero-shot* text-to-video generation by repurposing pretrained diffusion models, demonstrating that temporally coherent video synthesis can be achieved without expensive video training pipelines.\n- **OneFormer: One Transformer to Rule Universal Image Segmentation (2022)** — Advanced the goal of <u>universal image segmentation</u> by unifying multiple segmentation tasks within a single transformer-based framework, influencing subsequent work on general-purpose vision foundation models for dense outputs.\n\n---\n\n## Academic Background\nAcross affiliations with **Georgia Tech** and **UIUC** and his role at **NVIDIA**, Shi’s career reflects a hybrid trajectory typical of leading contemporary AI researchers: academic research depth coupled with industry-scale impact and deployment awareness. His publication history indicates sustained contributions from early work in video understanding (e.g., **Seq-NMS for Video Object Detection**, 2016) and image restoration (including community-defining efforts such as the **NTIRE 2017** super-resolution challenge and later attention-based restoration methods), through to modern foundation-model directions in transformers, diffusion, and multimodal systems (e.g., **Neighborhood Attention Transformer**, **Text2Video-Zero**, and multimodal encoder work such as **VCoder**, 2023, and **Eagle**, 2024). The breadth of highly cited papers, repeated leadership in benchmark/challenge efforts (e.g., NTIRE, AI City/VisDrone-style evaluation ecosystems), and cross-domain collaborations suggest strong standing in the computer vision community and sustained recognition through high-impact venues and widely adopted methodological contributions."}, "XM97iScAAAAJ": {"keywords": "Computer Vision, Artificial Intelligence, Machine Learning, Computer Graphics, Robotics, Video Texture Synthesis, Image/Video Segmentation, Facial Expression Recognition, Ubiquitous Computing (Smart Homes), Generative Video Models (Diffusion/Transformers)", "summary": "## Overview\nIrfan Essa is a **Distinguished Professor of Computing** at **Georgia Tech** and a **Research Scientist** at **Google**, where he works at the intersection of **Computer Vision**, **Artificial Intelligence**, and **Machine Learning** with deep ties to *Computer Graphics* and *Robotics*. Across academia and industry, his work has consistently advanced <u>computational understanding of visual and multimodal data</u>, spanning foundational methods for video and facial motion analysis, scalable approaches to spatiotemporal segmentation and stabilization, and modern generative and embodied systems that connect perception to action. With **29,094 citations**, Essa’s research portfolio reflects sustained leadership in building *practical, high-impact* algorithms and platforms that bring <u>vision-based intelligence</u> into real-world settings—from smart environments to large-scale learned models for images and video.\n\n---\n\n## Research Areas\nEssa’s research spans several tightly connected areas, unified by the goal of extracting structure, meaning, and controllability from visual experience. In **computer vision and video understanding**, he has contributed influential techniques for modeling and interpreting motion, including early work on facial motion estimation and expression analysis (e.g., “Coding, analysis, interpretation, and recognition of facial expressions,” 1997) and robust tracking formulations (e.g., “Motion regularization for model-based head tracking,” 1996). In **video analysis and computational photography**, his work includes scalable spatiotemporal grouping (“Efficient hierarchical graph-based video segmentation,” 2010), camera motion modeling for stabilization (“Auto-directed video stabilization with robust l1 optimal camera paths,” 2011), and handling sensor artifacts (“Calibration-free rolling shutter removal,” 2012). In **computer graphics and synthesis**, he helped define and popularize example-based visual generation, notably through graph-cut-based texture/video synthesis (“Graphcut textures: Image and video synthesis using graph cuts,” 2003) and optimization-based texture modeling (“Texture optimization for example-based synthesis,” 2005), alongside stylization methods (“Image and video based painterly animation,” 2004). In **ubiquitous computing and human-centered sensing**, he has shaped the concept of instrumented environments through the Aware Home initiative (“The aware home: A living laboratory for ubiquitous computing research,” 1999) and related work on sensing for everyday activities and aging (“Increasing the opportunities for aging in place,” 2000; “Ubiquitous sensing for smart and aware environments,” 2002). More recently, his research also engages **deep learning at scale** and **generative multimodal modeling**, including low-shot dense prediction (“One-shot learning for semantic segmentation,” 2017), distributed reinforcement learning for embodied navigation (“Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames,” 2019), and large generative systems for video and multimodal synthesis (e.g., “Videopoet,” 2023; “Magvit,” 2023; “Photorealistic video generation with diffusion models,” 2024), reflecting a trajectory from *model-based vision* to <u>data-driven representation learning</u> and <u>foundation-model-era generative perception</u>.\n\n---\n\n## Notable Works\n- **Graph-cut-based example synthesis for images and video:** In “**Graphcut textures: Image and video synthesis using graph cuts**” (2003), Essa and collaborators introduced a highly influential approach that stitches transformed patches along optimal seams, helping establish graph cuts as a practical tool for <u>image/video synthesis</u> and inspiring later work in texture and content-aware editing.\n- **Living laboratories for ubiquitous sensing and smart environments:** “**The aware home: A living laboratory for ubiquitous computing research**” (1999) articulated a landmark vision for instrumented domestic spaces as research platforms, grounding later advances in <u>activity-aware environments</u>, assistive technologies, and human-centered sensing systems.\n- **Low-shot dense recognition with learned parameter prediction:** “**One-shot learning for semantic segmentation**” (2017) extended low-shot learning beyond classification to dense prediction by training networks to generate parameters for segmentation models from few annotated examples, influencing subsequent research on <u>data-efficient</u> and *adaptable* visual recognition.\n\n---\n\n## Academic Background\nAs a long-standing faculty leader at **Georgia Tech**, Essa has built an academic career characterized by cross-disciplinary impact across **vision**, **graphics**, and **ubiquitous computing**, while also maintaining a strong industry research presence as a **Research Scientist at Google**. His publication history shows sustained contributions from foundational, model-driven vision methods in the 1990s (notably on facial motion, eye tracking, and head tracking) through major 2000s advances in video/texture synthesis and activity understanding, and into the 2010s–2020s with deep learning for segmentation, embodied navigation, self-supervision, and large-scale generative video systems. The breadth of his affiliations and themes—spanning the Aware Home and smart environments, spatiotemporal video segmentation and stabilization, and modern multimodal generative modeling—suggests a career deeply embedded in collaborative research communities across **computer science**, **human-centered computing**, and **AI**. His citation record (over **29k**) and repeated appearance of widely adopted methods and platforms indicate recognition consistent with senior academic distinction and broad influence across both scholarly research and applied technology development."}, "XjeIqxYAAAAJ": {"keywords": "Parallel computing, Programming languages, Compilers, Program analysis, Runtime systems, Task-parallel programming models (X10/Habanero-Java), Work-stealing scheduling, Dynamic data race detection, Java virtual machines and JIT compilation (Jikes RVM/Jalapeño), DNN accelerator dataflow and mapping optimization", "summary": "## Overview\nVivek Sarkar is a senior computer science researcher and faculty member in the School of Computer Science at the Georgia Institute of Technology, where he leads research at the intersection of **parallel computing**, **programming languages**, **compilers**, and **runtime systems**. His work focuses on making parallel and heterogeneous computing both *productive* and *high-performance* through principled language design, compiler optimization, and scalable runtime support. Across decades of widely cited contributions (over **22,329 citations**), Sarkar has advanced foundational ideas in <u>task-parallel programming models</u>, <u>compile-time and runtime scheduling</u>, and <u>performance-aware program analysis</u> for modern multicore, cluster, and accelerator-based systems.\n\n---\n\n## Research Areas\nSarkar’s research spans the full software stack for parallel and high-performance computing, with a sustained emphasis on turning theoretical parallelism into practical speedups. Early, highly influential work on *compile-time* techniques for exploiting parallelism—such as **partitioning and scheduling** of parallel programs for multiprocessors—helped formalize how dependence representations can be transformed into executable task graphs (e.g., “**Partitioning and scheduling parallel programs for execution on multiprocessors**,” 1987; “**Compile-time partitioning and scheduling of parallel programs**,” 1986; and “**Automatic partitioning of a program dependence graph into parallel tasks**,” 1991). He also made key contributions to **compiler optimization and code generation**, including fast global allocation (“**Linear scan register allocation**,” 1999) and analyses/transformations that improve locality and safety overheads (e.g., bounds-check elimination in “**ABCD: eliminating array bounds checks on demand**,” 2000, and loop/array optimizations such as “**Collective loop fusion for array contraction**,” 1992, and “**Array SSA form and its use in parallelization**,” 1998). In parallel, Sarkar’s work has shaped managed-runtime performance through the **Jalapeño/Jikes RVM** line (“**The Jalapeno virtual machine**,” 2000; “**The Jalapeno dynamic optimizing compiler for Java**,” 1999; “**The Jikes Research Virtual Machine project**,” 2005), including precise modeling of language features important for analysis and optimization (e.g., “**Efficient and precise modeling of exceptions for the analysis of Java programs**,” 1999). A major through-line is *structured parallelism* and its runtime implications: he advanced higher-level concurrency constructs and scheduling policies for async-finish tasking (“**Work-first and help-first scheduling policies for async-finish task parallelism**,” 2009), synchronization abstractions (“**Phasers**,” 2008), and dynamic correctness tools (“**Efficient and precise datarace detection for multithreaded object-oriented programs**,” 2002; “**Scalable and precise dynamic datarace detection for structured parallelism**,” 2012). His language and system-building impact is exemplified by **X10**, a modern object-oriented language for scalable concurrency and distribution (“**X10: an object-oriented approach to non-uniform cluster computing**,” 2005; “**X10: concurrent programming for modern architectures**,” 2007), and by related frameworks such as **Habanero-Java** (“**Habanero-Java: the new adventures of old X10**,” 2011; “**Habanero-Java library: a Java 8 framework for multicore programming**,” 2014). More recently, his interests extend to the performance modeling of specialized hardware for machine learning, using *data-centric* analyses of reuse and mapping for DNN accelerators (“**Understanding reuse, performance, and hardware cost of dnn dataflow**,” 2019; “**Maestro**,” 2020), reflecting a continued focus on <u>performance portability</u> across rapidly evolving architectures.\n\n---\n\n## Notable Works\n- **X10 programming language for scalable parallelism and distribution** — In “**X10: an object-oriented approach to non-uniform cluster computing**” (2005) and “**X10: concurrent programming for modern architectures**” (2007), Sarkar helped define a high-level, object-oriented model aimed at unifying concurrency and locality for clusters and multicore systems, shaping subsequent research on <u>structured task parallelism</u> and <u>place-based locality</u>.\n- **Foundations of partitioning and scheduling parallel programs** — “**Partitioning and scheduling parallel programs for execution on multiprocessors**” (1987), together with related compile-time frameworks (1986–1991), established influential approaches to transforming program dependence information into efficient task decompositions and schedules, clarifying the trade-offs among parallelism, communication, and synchronization overheads.\n- **Managed runtime and compiler infrastructure for high-performance Java** — Through “**The Jalapeno virtual machine**” (2000), “**The Jalapeno dynamic optimizing compiler for Java**” (1999), and the community-facing “**The Jikes Research Virtual Machine project**” (2005), Sarkar contributed to end-to-end JVM research platforms and optimization techniques that enabled rigorous experimentation in dynamic compilation, memory layout, and runtime scalability.\n\n---\n\n## Academic Background\nBased at Georgia Tech’s School of Computer Science, Vivek Sarkar has built a career that bridges academic research and large-scale systems impact, with a publication record spanning from foundational work in the mid-1980s on multiprocessor scheduling and program dependence methods to modern studies of exascale software challenges and accelerator-centric performance modeling. His trajectory—evident from extensive contributions to production-grade compiler and VM technology (e.g., IBM-centric compiler innovations and the Jalapeño/Jikes RVM lineage) and to widely adopted research languages and models (notably **X10**, and later **Habanero-Java** and structured-parallel runtime techniques)—suggests deep engagement with both industrial research ecosystems and open research communities. His influential role in articulating community directions is also reflected in exascale-focused works (e.g., “**The opportunities and challenges of exascale computing**,” 2010; “**Top ten exascale research challenges**,” 2014), indicating sustained leadership in shaping the agenda around <u>extreme-scale systems</u>. With more than **22,329 citations**, Sarkar’s standing reflects broad recognition across programming languages, compiler construction, runtime systems, and parallel computing communities, supported by long-term collaborations and affiliations spanning language design, high-performance compilation, and architecture-aware software systems."}, "Y67rbX0AAAAJ": {"keywords": "Human-Computer Interaction, Culturally Relevant CS Education, K-12 Artificial Intelligence Education, Middle School Computing Curriculum, Co-Design with Teachers, AI Learning Assessment, Teacher Professional Development, Place-Based and Indigenous Education, Generative AI Educational Technology, Batteryless and Sustainable Ubiquitous Computing", "summary": "## Overview\nWilliam Gelder is a researcher at the Georgia Institute of Technology whose work sits at the intersection of **Human–Computer Interaction (HCI)** and *education-centered technology design*, with a primary focus on <u>culturally relevant computer science (CS) education</u> and the emerging challenges of **AI literacy** in K–12 contexts. Across collaborations with teachers, school systems, and community stakeholders, Gelder’s research emphasizes *co-design* and *assets-based* approaches to ensure that AI-powered and educational technologies are not only usable, but also socially grounded, locally viable, and responsive to the lived realities of learners and educators.\n\n---\n\n## Research Areas\nGelder’s research areas span (1) **AI education design and evaluation** for middle school learners, (2) **culturally relevant and place-based educational technology**—particularly in Hawaiian bilingual immersion (Kaiapuni) contexts—and (3) sustainability-oriented systems work that enables **batteryless computing**. In K–12 AI education, he investigates how to translate ubiquitous exposure to AI-driven products into age-appropriate curricular experiences through teacher partnership and curriculum co-design, as demonstrated in “From lecture hall to homeroom: co-designing an AI elective with middle school CS teachers” (2025) and the broader implementation efforts described in “Artificial intelligence education in Georgia middle schools” (2025). Complementing curriculum design, he contributes to assessment methodology in “Design Considerations for Evaluating Middle School AI Knowledge” (2025), addressing the underdeveloped problem of how to measure both technical and ethical AI understanding in younger learners. In culturally relevant CS education, Gelder advances an *assets-based* framing that resists one-size-fits-all “emerging technology” interventions; this is exemplified by “Those don’t work for us”: An Assets-Based Approach to Incorporating Emerging Technologies in Viable Hawaiian Teacher Support Tools for Culturally Relevant CS Education” (2024) and extended through design thinking for <u>place-based generative educational technology</u> in “Kumu Connect: Design Thinking for Place-Based Generative Educational Technology in Hawaiian Immersion Schools” (2025). In parallel, his systems-oriented contribution “User-directed Assembly Code Transformations Enabling Efficient Batteryless Arduino Applications” (2024) reflects a commitment to practical, sustainable computing—developing techniques that reduce energy demands and support battery-free device operation.\n\n---\n\n## Notable Works\n- **Co-designing middle school AI curriculum with teachers**: In “From lecture hall to homeroom: co-designing an AI elective with middle school CS teachers” (2025), Gelder advances a teacher-centered co-design approach for building an age-appropriate AI elective that supports responsible engagement with everyday AI technologies.\n- **Assets-based, culturally relevant technology support for Hawaiian immersion education**: In “Those don’t work for us”: An Assets-Based Approach to Incorporating Emerging Technologies in Viable Hawaiian Teacher Support Tools for Culturally Relevant CS Education” (2024), Gelder articulates an assets-based design stance that foregrounds local educational values and constraints when introducing emerging technologies.\n- **Techniques for efficient batteryless embedded applications**: In “User-directed Assembly Code Transformations Enabling Efficient Batteryless Arduino Applications” (2024), Gelder contributes methods for transforming low-level code to improve efficiency, enabling more feasible battery-free computing for ubiquitous and sustainable devices.\n\n---\n\n## Academic Background\nAffiliated with the Georgia Institute of Technology, William Gelder’s publication record from 2024–2025 indicates an interdisciplinary trajectory bridging **HCI**, **learning sciences**, and systems research in sustainable computing. His work reflects sustained collaboration with educators and institutional partners, including multi-organization efforts described in “Artificial intelligence education in Georgia middle schools” (2025), developed with universities, a state department of education, and multiple school districts—an indicator of strong engagement with large-scale educational implementation and professional development. His contributions to culturally relevant CS education—particularly in Hawaiian immersion settings—suggest ongoing partnerships with schools and educators and a methodological grounding in *participatory design*, *design thinking*, and <u>place-based education</u>. With a growing citation profile (13 total citations) led by highly applied, partnership-driven studies, Gelder’s emerging scholarly identity is defined by translating research into viable educational tools, curricula, and evaluation practices that are attentive to culture, context, and sustainability."}, "YOGOScoAAAAJ": {"keywords": "High Performance Computing, Data Science, Computational Biology, Combinatorial Scientific Computing, Genome assembly, Sequence alignment, Read mapping (short and long reads), Average Nucleotide Identity (ANI) and microbial species delineation, Error correction for next-generation sequencing, Parallel algorithms for string data structures (suffix arrays/LCP)", "summary": "## Overview\nSrinivas Aluru is a **Regents' Professor** and **Senior Associate Dean** in the **College of Computing** at **Georgia Tech**, where he leads research at the intersection of **high-performance computing (HPC)**, **data science**, and *computational biology*. His work centers on designing scalable algorithms and systems that turn rapidly growing biological and scientific datasets into actionable insight, with particular emphasis on <u>genome-scale sequence analysis</u>, <u>parallel and distributed computing</u>, and *architecture-aware acceleration* for modern multicore and heterogeneous platforms. With **21,936 citations**, Aluru’s scholarship reflects sustained influence across both foundational algorithmics and high-impact genomics applications.\n\n---\n\n## Research Areas\nAluru’s research spans **combinatorial scientific computing** and **computational genomics**, unified by a focus on scalability, memory efficiency, and algorithm–architecture co-design. A major theme is <u>large-scale comparative genomics</u> and microbial taxonomy, exemplified by the widely cited study “**High throughput ANI analysis of 90K prokaryotic genomes reveals clear species boundaries**” (2018), which operationalizes **Average Nucleotide Identity (ANI)** at unprecedented scale to interrogate whether prokaryotic diversity forms a continuum or discrete species clusters. Another core area is <u>genome assembly and reference construction</u>, including contributions to plant genomics through “**The B73 maize genome: complexity, diversity, and dynamics**” (2009) and earlier assembly-centric work such as “**PCAP: a whole-genome assembly program**” (2003), addressing accuracy and throughput via parallelism. In parallel, Aluru has advanced <u>high-throughput sequence alignment</u> and preprocessing pipelines, notably “**Efficient architecture-aware acceleration of BWA-MEM for multicore systems**” (2019), which targets the computational bottlenecks created by modern sequencers through multicore-aware optimization. Complementing these applied genomics efforts is a sustained algorithmic foundation in string and sequence computing—e.g., “**Space efficient linear time construction of suffix arrays**” (2003/2005)—and broader work on parallel methods for sequence comparison, phylogenetics, and emerging representations such as graph-based references, reflecting an enduring commitment to *scalable algorithm design for biological data*.\n\n---\n\n## Notable Works\n- **Microbial species boundaries at massive scale:** In “*High throughput ANI analysis of 90K prokaryotic genomes reveals clear species boundaries*” (2018), Aluru and collaborators scaled ANI computation to tens of thousands of genomes, providing influential evidence for <u>discrete genomic species boundaries</u> and establishing a practical computational pathway for high-resolution microbial taxonomy.\n- **Landmark contribution to plant genomics infrastructure:** Through “*The B73 maize genome: complexity, diversity, and dynamics*” (2009), Aluru contributed to a foundational reference for maize genomics, enabling downstream studies of <u>genome complexity</u>, repetitive structure, and gene annotation at crop-genome scale.\n- **Architecture-aware acceleration of core genomics software:** In “*Efficient architecture-aware acceleration of BWA-MEM for multicore systems*” (2019), he advanced <u>multicore-optimized sequence alignment</u> by aligning algorithmic structure with modern CPU architectures, substantially improving throughput for a central read-mapping workload in next-generation sequencing pipelines.\n\n---\n\n## Academic Background\nAluru’s career reflects a trajectory characteristic of senior leaders in computational science: sustained contributions to **parallel algorithms** and **scientific computing** beginning with foundational work in distributed-memory methods (e.g., space-filling-curve load balancing and parallel random number generation in the late 1990s), followed by a long-running expansion into **computational molecular biology** and genome-scale data analysis. His publication record shows deep engagement with interdisciplinary, large-consortium biology—especially plant genomics and genome assembly—alongside continued advances in algorithmic string processing and parallel sequence analysis, consistent with affiliations spanning **computing** and **life sciences**. As a **Regents' Professor** at **Georgia Tech** and a senior academic administrator (Senior Associate Dean), Aluru’s standing indicates major institutional recognition for research and leadership; his highly cited body of work and repeated contributions to widely used genomic methods and reference resources further suggest sustained involvement in premier venues and collaborative networks across **HPC**, **bioinformatics**, and <u>data-intensive genomics</u>."}, "YON32W4AAAAJ": {"keywords": "AI Safety, Human-centered AI, Visual Analytics, Graph Mining, Network Visualization, Deep Learning Interpretability, Adversarial Machine Learning, Large Language Model Safety, Graph Anomaly Detection, Malware Detection", "summary": "## Overview\nPolo Chau is a Professor at the **Georgia Institute of Technology** and an affiliated researcher at **Apple**, where he leads and collaborates with teams at the intersection of **Human-centered AI**, **Visual Analytics**, and **Graph Mining**. His work centers on making complex machine learning systems *understandable, trustworthy, and usable* by people—especially through <u>interactive visualization</u> and <u>human-in-the-loop</u> methods—while also advancing <u>AI Safety</u> via research on adversarial robustness and the evaluation of safety risks in modern models.\n\n---\n\n## Research Areas\nChau’s research spans three tightly connected areas: (1) <u>visual analytics for deep learning</u>, (2) <u>AI safety and adversarial machine learning</u>, and (3) <u>large-scale graph mining and network visualization</u>. In visual analytics, he has helped define research frontiers for interpreting deep models, synthesizing directions and challenges in *Visual analytics in deep learning: An interrogative survey for the next frontiers* (2018) and building practical systems for model understanding at scale, such as *ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models* (2018) and *Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations* (2019). In AI safety, his contributions address both attacks and defenses: *Shapeshifter: Robust physical adversarial attack on faster r-cnn object detector* (2019) demonstrates realistic physical-world threats to object detection, while *Keeping the bad guys out: Protecting and vaccinating deep learning with jpeg compression* (2017) and *Shield: Fast, practical defense and vaccination for deep learning using jpeg compression* (2018) propose deployable robustness techniques. More recently, his work extends to safety in language models, including *LLM Self Defense: By self examination, LLMs know they are being tricked* (2023) and *Navigating the safety landscape: Measuring risks in finetuning large language models* (2024). Complementing these ML-focused efforts, Chau has a long-standing thread in graph-based anomaly detection and inference, with systems and algorithms for fraud and malware detection such as *NetProbe* (2007) and *Polonium* (2011), reflecting a sustained emphasis on scalable computation and *sensemaking* over complex networked data.\n\n---\n\n## Notable Works\n- **Visual analytics for deep learning interpretation and research roadmapping**: In *Visual analytics in deep learning: An interrogative survey for the next frontiers* (2018), Chau and collaborators articulated a structured agenda for how visualization can support understanding, debugging, and improving deep models, shaping subsequent work on <u>interpretable</u> and <u>human-centered</u> ML.\n- **Practical, real-world adversarial robustness—attacks and defenses**: *Shapeshifter: Robust physical adversarial attack on faster r-cnn object detector* (2019) advanced the study of physically realizable adversarial examples for detection systems, while *Keeping the bad guys out* (2017) introduced a pragmatic defense strategy using JPEG compression, later refined for fast deployment in *Shield* (2018), collectively influencing applied <u>AI Safety</u> practice.\n- **Scalable graph inference for security and fraud detection**: *NetProbe: a fast and scalable system for fraud detection in online auction networks* (2007) and *Polonium: Tera-Scale Graph Mining and Inference for Malware Detection* (2011) exemplify Chau’s foundational contributions to <u>graph mining</u> for high-impact anomaly detection, leveraging network structure and large-scale inference to identify malicious behavior.\n\n---\n\n## Academic Background\nAcross a publication record spanning early work in human-computer interaction and user-facing systems (e.g., *Answering why and why not questions in user interfaces*, 2006) through large-scale network analysis and security (e.g., *NetProbe*, 2007; *On the vulnerability of large graphs*, 2010; *Polonium*, 2011) and into modern human-centered machine learning and AI safety (e.g., *ActiVis*, 2018; *FairVis*, 2019; *LLM Self Defense*, 2023), Chau’s career reflects a consistent focus on <u>making complex computational systems intelligible and dependable</u>. As a faculty member at Georgia Tech and a collaborator with industry research at Apple, he has maintained strong cross-sector affiliations that align with his emphasis on *industry-scale* models and real-world deployment constraints (as highlighted by *ActiVis*, 2018, and robustness work like *Shield*, 2018). With **13,826 citations**, his scholarship demonstrates broad impact across visualization, machine learning interpretability, adversarial robustness, and graph-based security analytics, indicating sustained recognition by multiple research communities and a trajectory that bridges foundational methods with applied, high-stakes domains."}, "YP23eR0AAAAJ": {"keywords": "SAT/SMT Solvers, symbolic execution, concolic testing and whitebox fuzzing, string constraint solving (word equations, regex, length constraints), decision procedures for bit-vectors and arrays, conflict-driven clause-learning (CDCL) heuristics and restarts, software security and vulnerability detection, machine learning for solver heuristics and algorithm selection, formal verification and access-control policy analysis, quantum foundations", "summary": "## Overview\nVijay Ganesh is a Professor at the Georgia Institute of Technology (Atlanta, GA, USA), where he leads research at the intersection of **SAT/SMT solving**, **software engineering**, and **AI** with a strong grounding in *mathematical logic*. His work centers on making automated reasoning systems more capable and practical for real-world use—especially for <u>program analysis and security</u>, <u>string and bit-vector reasoning</u>, and <u>solver heuristics guided by learning</u>. With **9,685 citations**, Ganesh is widely recognized for contributions that translate foundational ideas in satisfiability and decision procedures into high-impact tools for bug-finding, verification, and emerging applications spanning *privacy*, *cryptography*, and aspects of *quantum foundations*.\n\n---\n\n## Research Areas\nGanesh’s research spans several tightly connected areas of automated reasoning and its applications. A major theme is <u>constraint solving for software analysis</u>, exemplified by the decision-procedure work behind STP in “**A decision procedure for bit-vectors and arrays**” (2007), which targets quantifier-free formulas common in program verification and symbolic execution. He has also advanced <u>automated bug-finding and vulnerability discovery</u> through symbolic and concolic execution, most prominently in “**EXE: Automatically generating inputs of death**” (2008) and in directed whitebox fuzzing via dynamic taint analysis in “**Taint-based directed whitebox fuzzing**” (2009). Another sustained line of work addresses <u>string constraints in SMT</u> for web and security analysis, including “**HAMPI: a solver for string constraints**” (2009), “**Z3-str: A z3-based string solver for web application analysis**” (2013), and subsequent solver improvements such as “Z3str3” (2017) and “Z3str4” (2021), alongside theoretical investigations like “**Word equations with length constraints: what’s decidable?**” (2012). In parallel, Ganesh has shaped <u>next-generation SAT/SMT heuristics</u> by reframing branching and restarts through measurable learning dynamics—e.g., “**Learning rate based branching heuristic for SAT solvers**” (2016) and “Understanding VSIDS…” (2015)—and by integrating ML into solver control (“Machine learning-based restart policy…” (2018), “MachSMT…” (2021)). More recently, his AI-facing work connects reasoning to societal and systems concerns, including machine unlearning and privacy compliance in “**Amnesiac machine learning**” (2021), as well as tool-building that combines LLMs with symbolic execution feedback (“CoTran…” (2023)). Across these areas, his publications consistently emphasize *scalability*, *robustness*, and the practical deployment of logical methods in complex software ecosystems.\n\n---\n\n## Notable Works\n- **Symbolic execution for automatic crash-inducing input generation** — In “**EXE: Automatically generating inputs of death**” (2008), Ganesh helped establish a highly influential paradigm for running programs on symbolic inputs to systematically derive constraints that produce real crashes, substantially advancing <u>automated bug-finding</u> beyond random testing.\n- **Efficient decision procedures for bit-vectors and arrays (STP)** — “**A decision procedure for bit-vectors and arrays**” (2007) introduced an optimized architecture for solving quantifier-free bit-vector/array formulas at scale, enabling broad impact in <u>software verification</u>, symbolic execution pipelines, and SMT-based analysis tooling.\n- **Machine unlearning for privacy and compliance** — “**Amnesiac machine learning**” (2021) tackled the *Right to be Forgotten* problem by proposing practical mechanisms to remove training data influence, helping bridge **AI** practice with <u>privacy law and ML lifecycle governance</u>.\n\n---\n\n## Academic Background\nGanesh’s publication trajectory reflects a career spanning both *system-level design and architecture exploration* and the later consolidation of his scholarly identity in **automated reasoning** and **software reliability**. Early highly cited work on retargetable compiler/simulator infrastructure—such as “**EXPRESSION: A language for architecture exploration through compiler/simulator retargetability**” (1999)—signals formative expertise in systems and toolchain construction, which later becomes a hallmark of his solver- and analysis-driven research. By the mid-2000s, his work coalesced around SAT/SMT and decision procedures (e.g., STP and related decision-procedure papers in 2007), followed by influential contributions to symbolic execution and fuzzing (EXE (2008), BuzzFuzz (2009), jFuzz (2009)). His sustained output on string solving (HAMPI; Z3-str and successors) and on principled SAT heuristics (LRB/ERWA; analyses of VSIDS and instance structure) demonstrates long-term leadership in both the theory and engineering of solvers. As a Professor at **Georgia Tech**, he is affiliated with a major US research institution known for strengths in computer science and engineering, and his cross-cutting work—spanning verification, security, ML, and even mathematically oriented SAT+CAS efforts—suggests extensive collaboration across academic and applied research communities."}, "Yg_QjxcAAAAJ": {"keywords": "Artificial intelligence, Machine Learning, Storytelling, Explainable AI, Safe AI, Computational narrative intelligence, Interactive narrative, Automated story generation, Procedural content generation, Human-centered AI", "summary": "## Overview\nMark O. Riedl is a Professor of Computing at the Georgia Institute of Technology, where he leads research at the intersection of **artificial intelligence** and *human-centered computing*, with a particular emphasis on <u>computational narrative intelligence</u>. His work integrates **machine learning**, **planning**, and *interactive systems* to build AI that can generate, adapt, and communicate stories—while also advancing <u>explainable AI</u> and <u>safe, accountable AI</u> for real-world sociotechnical settings. Across game AI, storytelling, and explanation, Riedl’s research program foregrounds the idea that intelligent systems should not only optimize performance, but also support *human understanding, creativity, and trust*.\n\n---\n\n## Research Areas\nRiedl’s research spans three tightly connected areas: (1) **computational storytelling and narrative planning**, (2) **interactive narrative and game AI**, and (3) **human-centered explainable and safe AI**. In narrative generation, he is widely associated with plan-based approaches that formalize story construction as a problem of balancing global plot constraints with local character agency, as articulated in “**Narrative planning: Balancing plot and character**” (2010). This line of work extends to interactive narrative systems and experience management—where user freedom must be reconciled with coherent dramatic structure—building on foundational contributions such as “**Interactive narrative: An intelligent systems approach**” (2013) and earlier work on managing player–agent interactions in story worlds (e.g., “Managing interaction between users and agents in a multi-agent storytelling environment,” 2003). In parallel, Riedl has advanced data-driven and neural approaches to story generation, including event-based representations for plot learning (“**Event representations for automated story generation with deep neural nets**,” 2018) and controllable generation via reward shaping (“Controllable neural story plot generation via reward shaping,” 2018), reflecting a sustained effort to combine *symbolic structure* with **machine learning**. More recently, his work has become a key part of the modern **XAI** discourse by reframing explanation as socially embedded and user-dependent, including “**Expanding explainability: Towards social transparency in AI systems**” (2021) and “**Human-centered explainable AI: Towards a reflective sociotechnical approach**” (2020). Complementing conceptual frameworks, he has contributed technical methods for generating natural-language explanations—such as “**Automated rationale generation: a technique for explainable AI and its effects on human perceptions**” (2019) and “Rationalization: A neural machine translation approach to generating natural language explanations” (2018)—and has addressed safety- and values-oriented concerns, including the use of narrative as a vehicle for value alignment (“Using Stories to Teach Human Values to Artificial Agents,” 2016) and the analysis of harmful behaviors in neural dialogue (“Just say no: Analyzing the stance of neural dialogue generation in offensive contexts,” 2021).\n\n---\n\n## Notable Works\n- **Narrative planning for coherent, character-believable story generation** — In “**Narrative planning: Balancing plot and character**” (2010), Riedl formalized core tensions in automated storytelling by treating narrative construction as a planning problem that must jointly satisfy *plot coherence* and <u>character believability</u>, providing a durable conceptual and algorithmic foundation for later interactive and generative narrative systems.\n- **Interactive narrative and experience management as intelligent systems** — Through “**Interactive narrative: An intelligent systems approach**” (2013) and earlier work such as “Managing interaction between users and agents in a multi-agent storytelling environment” (2003), Riedl helped define how AI can mediate user agency in story worlds, establishing methods for <u>drama management</u>, adaptation, and multi-agent narrative control in games and virtual environments.\n- **Human-centered and socially grounded explainable AI** — In “**Expanding explainability: Towards social transparency in AI systems**” (2021) and “**Human-centered explainable AI: Towards a reflective sociotechnical approach**” (2020), Riedl advanced a shift from purely model-centric interpretability toward <u>sociotechnical</u> explanation practices, emphasizing that explanations are *contextual, relational,* and shaped by stakeholder needs—an agenda complemented by empirical and technical work on rationale generation (“Automated rationale generation…,” 2019).\n\n---\n\n## Academic Background\nBased at the Georgia Institute of Technology as Professor of Computing, Riedl’s career reflects sustained leadership across **AI**, *interactive entertainment*, and <u>human-centered AI</u>, evidenced by a publication record spanning early multi-agent narrative architectures (2003–2006), mature frameworks for interactive narrative and drama management (2008–2013), and more recent, high-impact contributions to **explainable AI** and **human-centered machine learning** (2019–2024). The trajectory of his work suggests deep affiliations with research communities in artificial intelligence, game studies, human–computer interaction, and sociotechnical systems, with recurring themes of authorability, user experience, and responsible deployment. With **15,413 citations**, his influence is particularly visible in computational narrative research and the contemporary reframing of XAI toward social transparency and reflective practice, positioning him as a prominent academic voice connecting technical innovation to *human values, safety, and accountability* in intelligent systems."}, "Z6EHuGcAAAAJ": {"keywords": "Computer Networks, Mobile and Wireless Networks, Internet Topology, Humanitarian Computing, Delay-Tolerant Networking (DTN), Mobile Ad Hoc Networks (MANETs), Message Ferrying and Throwboxes, Software-Defined Networking (SDN) and Programmable Networks, Edge Computing and Computation Offloading, Network Measurement and Performance (QoS/QoE, Load Balancing)", "summary": "## Overview\nEllen Zegura is a Professor of Computer Science at the Georgia Institute of Technology, where she leads and collaborates within research efforts spanning **computer networking** and *human-centered, high-impact systems*. Her scholarship centers on designing, measuring, and reasoning about networked systems under real-world constraints—especially where <u>connectivity is intermittent</u>, resources are limited, or the network’s structure is only partially observable. Across decades of influential work, Zegura has advanced foundational methods for <u>Internet topology modeling</u>, pioneered *mobility-assisted communication* in challenged environments, and helped articulate the intellectual lineage of **programmable networks** that culminated in modern SDN.\n\n---\n\n## Research Areas\nZegura’s research areas bridge empirical network science, protocol and systems design, and socially motivated computing. A major thread of her work establishes rigorous approaches to <u>graph-based modeling of internetworks</u>, including early and widely cited treatments of what it means to model the Internet and why model choice matters for evaluating routing and resource allocation (e.g., *“How to model an internetwork”* (1996) and *“Modeling internet topology”* (1997)). She extended this line with comparative and analytical techniques that scrutinize structural properties of candidate models (e.g., *“A quantitative comparison of graph-based models for Internet topology”* (2002) and *“Spectral analysis of Internet topologies”* (2003)), contributing tools and criteria that shaped how the community evaluates realism and bias in topology generators. A second major area develops mechanisms for communication in sparse and partitioned wireless settings through <u>delay-tolerant networking</u> and *message ferrying*, where controlled mobility becomes a first-class networking primitive (e.g., *“A message ferrying approach for data delivery in sparse mobile ad hoc networks”* (2004), *“Controlling the mobility of multiple data transport ferries in a delay-tolerant network”* (2005), and *“Message ferry route design for sparse ad hoc networks with mobile nodes”* (2006)). Complementing these, her work has addressed practical Internet service performance—such as load balancing and server selection via anycasting (e.g., *“Performance of hashing-based schemes for internet load balancing”* (2000) and *“Application-layer anycasting”* (1997/2000))—and later expanded toward edge/mobile computing under intermittent connectivity (e.g., *“Serendipity”* (2012) and *“Femto clouds”* (2015)). In parallel, Zegura has contributed to the networking community’s understanding of *network programmability’s evolution* through *“The road to SDN: an intellectual history of programmable networks”* (2014), situating contemporary architectures within a longer arc that includes active networking and related design debates.\n\n---\n\n## Notable Works\n- **Internet topology and internetwork modeling foundations:** Through *“How to model an internetwork”* (1996) and the synthesizing perspective of *“Modeling internet topology”* (1997), Zegura helped define methodological norms for representing the Internet as a graph, clarifying assumptions and pitfalls that directly influence conclusions about routing, robustness, and performance.\n- **Mobility-assisted networking in challenged environments:** In *“A message ferrying approach for data delivery in sparse mobile ad hoc networks”* (2004), she advanced the message-ferrying paradigm for partitioned MANETs/DTNs, demonstrating how controlled mobility can raise delivery rates and reshape design trade-offs in disaster relief and sensing scenarios.\n- **Historical framing of programmable networks and SDN:** With *“The road to SDN: an intellectual history of programmable networks”* (2014), Zegura provided a high-impact scholarly account connecting SDN to earlier ideas in active networks and network control, offering a conceptual map that informs both research and pedagogy.\n\n---\n\n## Academic Background\nAs a long-standing faculty member in Georgia Tech’s School of Computer Science, Zegura has built an internationally recognized research portfolio reflected in extensive citation impact (over 22,000 citations) and sustained influence across multiple eras of networking research. Her publication trajectory—from early work on multicast and internetwork modeling in the 1990s, through active networking and Internet service architectures around the turn of the millennium, to DTNs, mobile/edge computing, and SDN-era perspectives—indicates deep engagement with the core venues and research communities of computer networking (e.g., IEEE/ACM flagship conferences and journals). Given the breadth and longevity of these contributions, she is plausibly affiliated with major professional communities such as **ACM** and **IEEE**, and her scholarly record aligns with recognition typical of senior leaders in the field (e.g., invitations to author broad, field-shaping syntheses like the SDN intellectual history and widely used modeling surveys). Her more recent thematic expansion toward *humanitarian computing* reflects a commitment to applying rigorous networking and systems research to contexts where <u>social impact</u> and *operational constraints* are inseparable from technical design."}, "Z9Pfp6UAAAAJ": {"keywords": "Computational Social Science, Social Computing, Mental Health, Language, Social Media Mining, Depression Detection, Suicide Risk Prediction, Online Social Support, Ethical AI in Mental Health, AI-Generated Misinformation", "summary": "## Overview\nMunmun De Choudhury is a faculty researcher at the **Georgia Institute of Technology**, where she leads a program of work at the intersection of **Computational Social Science** and **Social Computing** with a sustained emphasis on *mental health*. Her scholarship advances <u>social media as a measurement and intervention-relevant lens</u> for understanding psychological well-being, leveraging **language**, behavior traces, and machine learning to model risk, disclosure, and support in online communities. Across highly cited contributions (total citations: **26,516**), she has helped establish <u>social platforms as population-scale “sensors” of health</u> while foregrounding *human-centered* and *ethical* considerations in deploying predictive methods for sensitive outcomes.\n\n---\n\n## Research Areas\nDe Choudhury’s research spans methodological, empirical, and translational questions in computational mental health and social computing. A central thread is <u>inferring mental health states from language and behavior</u> in social media, exemplified by early, field-shaping work on depression detection and characterization (e.g., “**Predicting depression via social media**,” 2013; and “**Social media as a measurement tool of depression in populations**,” 2013), as well as longitudinal modeling of life transitions and associated affective change (e.g., “**Predicting postpartum changes in emotion and behavior via social media**,” 2013; and “**Characterizing and predicting postpartum depression from shared facebook data**,” 2014). She has also advanced computational approaches to severe risk contexts, including <u>suicidal ideation forecasting and shift detection</u> (“**Discovering shifts to suicidal ideation from mental health content in social media**,” 2016) and the role of supportive language in risk trajectories (“**The language of social support in social media and its effect on suicidal ideation risk**,” 2017). Another major area examines *stigmatized disclosures* and the social dynamics that shape help-seeking, anonymity, and reciprocity in platforms such as Reddit (e.g., “**Mental health discourse on reddit: Self-disclosure, social support, and anonymity**,” 2014; and “**Understanding social media disclosures of sexual abuse through the lenses of support seeking and anonymity**,” 2016). Complementing these empirical contributions, she has published influential syntheses and frameworks that interrogate <u>validity, ethics, and implementation barriers</u> in AI-enabled mental health research and care (e.g., “**Methods in predictive techniques for mental health status on social media: a critical review**,” 2020; “**A taxonomy of ethical tensions in inferring mental health states from social media**,” 2019; and “**Artificial intelligence for mental health care: clinical applications, barriers, facilitators, and artificial wisdom**,” 2021), alongside related work on online harms such as hate and misinformation (e.g., “**Latent hatred**,” 2021; “**Synthetic lies**,” 2023).\n\n---\n\n## Notable Works\n- **Depression inference from social media language and behavior** — In “*Predicting depression via social media*” (2013), De Choudhury and collaborators demonstrated that social media signals can be used to identify markers consistent with major depressive disorder, helping to anchor a research paradigm around <u>computational detection of mental health risk</u> from everyday digital traces.\n- **Online mental health communities, disclosure, and anonymity** — “*Mental health discourse on reddit: Self-disclosure, social support, and anonymity*” (2014) provided a widely cited account of how platform affordances shape *self-disclosure* and *support exchange*, establishing a foundation for studying <u>help-seeking ecologies</u> in semi-anonymous social spaces.\n- **Forecasting transitions toward suicidal ideation** — “*Discovering shifts to suicidal ideation from mental health content in social media*” (2016) advanced methods to detect <u>temporal shifts</u> in risk-related expressions, moving beyond static classification toward modeling *change* and escalation in high-stakes mental health contexts.\n\n---\n\n## Academic Background\nAt the Georgia Institute of Technology, De Choudhury has built an internationally visible research agenda that bridges **computer science**, **public health**, and **psychology**, reflected in a publication record that ranges from early work in social media systems and diffusion (e.g., “*How does the data sampling strategy impact the discovery of information diffusion in social media?*,” 2010; “*Automatic construction of travel itineraries using social breadcrumbs*,” 2010) to a sustained, highly cited body of scholarship in <u>computational mental health</u> beginning in the early 2010s. Her trajectory shows increasing engagement with clinical translation and responsible AI—articulating methodological standards (“*Methods in predictive techniques for mental health status on social media: a critical review*,” 2020), ethical frameworks (“*A taxonomy of ethical tensions in inferring mental health states from social media*,” 2019), and implementation challenges for AI-informed care (“*Artificial intelligence for mental health care…*,” 2021; “*From promise to practice: towards the realisation of AI-informed mental health care*,” 2022). The breadth of topics—depression, postpartum mental health, suicidality, eating disorders, sexual abuse disclosures, and online harms—alongside substantial scholarly impact (over **26k** citations) indicates strong standing and recognition within **computational social science** and **human-centered machine learning** communities, and sustained collaboration across interdisciplinary and health-facing research networks at Georgia Tech and beyond."}, "ZcguQt4AAAAJ": {"keywords": "Parallel Algorithms, Caching, Performance Engineering, Dynamic Graph Data Structures, Packed Memory Arrays (PMA), Compressed Sparse Row (CSR), Cache-Oblivious and Cache-Adaptive Algorithms, In-Memory Indexing (B-Trees and Skiplists), Sparse Matrix and Tensor Computations (SpGEMM), Batch-Parallel Compressed Data Structures", "summary": "## Overview\nHelen Xu is a researcher at the Georgia Institute of Technology, where she contributes to a research program centered on **parallel algorithms** and **performance engineering** for modern multicore systems. Her work emphasizes *end-to-end efficiency* in data-intensive computation by designing <u>cache-efficient dynamic data structures</u>—especially for graphs and ordered sets—that can sustain high update rates while preserving fast traversals and range queries. Across her publications, Xu repeatedly targets the algorithm–systems boundary, using **locality-aware design**, *batch parallelism*, and principled analysis to translate theoretical guarantees into practical speedups on real hardware.\n\n---\n\n## Research Areas\nXu’s research spans dynamic graph containers, write-optimized and locality-optimized indexing structures, and the theory/practice of caching in shared-memory machines. A major thread is dynamic graph representation: her early work on **Packed Compressed Sparse Row (Packed CSR)** (“Packed compressed sparse row: A dynamic graph representation,” 2018) rethinks the standard CSR layout to accommodate updates without sacrificing the compactness and traversal performance that make CSR dominant in graph analytics. She extends these ideas to skewed, streaming graphs with **hierarchical containers** in “Terrace: A hierarchical graph container for skewed dynamic graphs” (2021), addressing the degree-heterogeneity common in social and web graphs by structuring storage to reduce update hotspots and preserve locality. In parallel, she advances the algorithmic foundations of update-friendly, cache-efficient arrays via PMA-based designs, including “A parallel packed memory array to store dynamic graphs” (2021) and later compression-focused work such as “Cpma: An efficient batch-parallel compressed set without pointers” (2024), which replaces pointer-heavy structures with a compressed, contiguous representation amenable to parallel batch updates. Complementing these data-structure contributions, Xu has published on **write-optimized dictionaries** (“Write-optimized skip lists,” 2017) and on the *limits and models* of caching and adaptivity, including “Closing the gap between cache-oblivious and cache-adaptive analysis” (2020) and subsequent studies of multicore paging/caching competitiveness (2020–2022). Her portfolio also includes performance-oriented sparse linear algebra and tensor work (e.g., fill estimation for blocked sparse formats and SpGEMM optimizations), reflecting a broader interest in <u>data movement minimization</u> as the primary lever for scalable performance.\n\n---\n\n## Notable Works\n- **Packed CSR for dynamic graphs** — In “*Packed compressed sparse row: A dynamic graph representation*” (2018), Xu introduces a CSR-derived layout that preserves CSR’s compact, traversal-friendly structure while enabling efficient insertions/deletions, establishing a widely cited baseline for locality-conscious dynamic graph storage.\n- **Terrace: hierarchical containers for skewed streaming graphs** — In “*Terrace: A hierarchical graph container for skewed dynamic graphs*” (2021), she addresses real-world degree skew by organizing updates and accesses in a hierarchy that mitigates contention and improves locality, enabling fast streaming updates alongside analytics.\n- **Parallel PMA-based dynamic graph storage** — In “*A parallel packed memory array to store dynamic graphs*” (2021), Xu develops a parallel PMA approach that supports both high-throughput updates and efficient range-query-style traversals, strengthening the case for contiguous-memory dynamic structures on multicore hardware.\n\n---\n\n## Academic Background\nBased on her publication record and collaborations, Xu’s academic trajectory bridges **algorithms**, **data structures**, and **computer systems** with a consistent focus on multicore performance and caching. Her earlier work includes applied, security-adjacent systems efforts (e.g., “TWIAD the Write-Optimized IP Address Database,” 2015) that foreshadow her later emphasis on write-optimized structures and high-ingest workloads. Over time, her research consolidates around dynamic graph containers and PMA/CSR-derived representations (2017–2024), alongside a sustained line of inquiry into cache models, cache adaptivity, and the theoretical limits of multicore paging strategies (2018–2022). Her affiliation with Georgia Tech and a citation record of 396 reflect a visible impact in the intersecting communities of parallel algorithms, performance engineering, and dynamic graph processing, with contributions that combine *analytic clarity* and <u>hardware-conscious design</u> across both foundational and benchmarking-oriented work (e.g., “Byo: A unified framework for benchmarking large-scale graph containers,” 2024)."}, "_Z12v2IAAAAJ": {"keywords": "Database Systems, Conceptual Database Design, Entity-Relationship Modeling, Schema Integration, View Integration, Distributed Database Design, Data Fragmentation and Vertical Partitioning, Temporal Databases, Association Rule Mining, Semantic Data Models", "summary": "## Overview\nShamkant B. Navathe is a Professor of Computer Science at the Georgia Institute of Technology, internationally recognized for foundational contributions to **database systems** and the broader discipline of data management. His work spans the full lifecycle of data-intensive systems—from *conceptual modeling* and <u>schema integration</u> to *physical design* and <u>data mining</u>—and is distinguished by an enduring emphasis on rigorous principles that translate into practical methods, tools, and widely adopted educational resources. With an exceptionally high citation impact (approximately **34,400** citations), Navathe’s scholarship has shaped both academic research agendas and professional practice in modern database design, implementation, and analytics.\n\n---\n\n## Research Areas\nNavathe’s research portfolio centers on the theory and practice of **database design**, with seminal work on <u>integrating heterogeneous schemas and user views</u>—a critical challenge in building unified enterprise and federated databases. Early and influential studies such as *“A comparative analysis of methodologies for database schema integration”* (1986) and *“Integrating user views in database design”* (1986) articulated systematic approaches to resolving semantic conflicts and consolidating multiple conceptualizations of organizational data. Complementing this, his contributions to <u>conceptual database design</u> are strongly associated with the entity–relationship tradition, exemplified by *“Conceptual database design: an entity-relationship approach”* (1991), and later reflective scholarship such as *“Evolution of data modeling for databases”* (1992), which situates modeling advances within the broader trajectory of data management. Navathe also advanced <u>physical database design</u> through influential work on *vertical partitioning*—notably *“Vertical partitioning algorithms for database design”* (1984) and subsequent refinements—addressing how attribute grouping can be optimized to match transaction workloads. In the 1990s, his research helped define the emerging field of <u>database mining</u>, including the highly cited *“An Efficient Algorithm for Mining Association Rules in Large Databases”* (1995) and later extensions such as *“Mining for strong negative associations in a large database of customer transactions”* (1998), expanding the analytic scope from frequent co-occurrences to informative absences. Beyond these core areas, his publication record also reflects sustained engagement with <u>temporal databases</u> (e.g., *“A temporal relational model and a query language”*, 1989) and applied database curation in scientific domains (e.g., *MITOMAP* updates), illustrating a consistent interest in how database principles enable reliable knowledge organization across contexts.\n\n---\n\n## Notable Works\n- **Co-authoring a definitive database textbook that shaped education and practice:** *“Fundamentals of database systems”* (2008) synthesized <u>database modeling and design</u>, query languages, and system implementation into a widely cited reference that helped standardize curricula and professional understanding of core DBMS concepts.\n- **Pioneering scalable association-rule mining methods in large databases:** *“An Efficient Algorithm for Mining Association Rules in Large Databases”* (1995) became a cornerstone of **data mining** research by formalizing efficient discovery of association rules from transaction data and influencing subsequent algorithmic and systems work in large-scale analytics.\n- **Establishing methodological foundations for schema and view integration:** *“A comparative analysis of methodologies for database schema integration”* (1986) provided an influential framework for <u>schema integration</u>, clarifying principles and practical steps for unifying data representations across organizational and application boundaries.\n\n---\n\n## Academic Background\nAs a long-standing faculty member at the Georgia Institute of Technology, Navathe has built a research and teaching career deeply embedded in the evolution of modern **database systems** as a field. His publication trajectory—from early foundational work in the 1980s on view and schema integration and physical design (including vertical partitioning), through influential 1990s contributions to data mining and temporal database models, to later applied and interdisciplinary database efforts—reflects sustained leadership across multiple waves of data management research. The extraordinary adoption and citation of his scholarly outputs—particularly his textbook and highly cited research articles—indicate broad recognition by the academic community and enduring influence on both pedagogy and research practice. His collaborations across topics such as conceptual modeling, distributed and temporal data management, and domain databases (e.g., *MITOMAP*) further suggest strong interdisciplinary affiliations and a career characterized by translating core database principles into methods and resources used widely across computing and data-driven sciences."}, "_ZIAy3cAAAAJ": {"keywords": "human-computer interaction, information visualization, visual analytics, human-in-the-loop machine learning, semantic interaction, interactive model steering, analytic provenance, large high-resolution displays, cognitive bias mitigation, visual text analytics", "summary": "## Overview\nAlex Endert is an Associate Professor at Georgia Tech and a core contributor to the university’s visual analytics community, leading research that bridges **human-computer interaction (HCI)**, **information visualization**, and **visual analytics**. His work centers on *interactive sensemaking*—designing systems in which people can externalize reasoning, steer computational models, and reflect on analytic progress through interaction. Across this agenda, Endert has advanced <u>human-in-the-loop analytics</u> by developing interaction techniques and frameworks that translate users’ intent into computational guidance, enabling more transparent, efficient, and trustworthy analysis.\n\n---\n\n## Research Areas\nEndert’s research spans several tightly connected themes in modern visual analytics. A foundational line of work investigates how *physical and digital space* supports cognition, particularly through large, high-resolution display environments that enable analysts to offload memory, reveal relationships, and build spatial schemas during complex investigations (e.g., “**Space to think: large high-resolution displays for sensemaking**,” 2010; and “Information visualization on large, high-resolution displays: Issues, challenges, and opportunities,” 2011). A second major thrust develops interaction paradigms that connect users’ mental models to underlying algorithms—most notably **semantic interaction** for visual text analytics, where user-driven spatial arrangements and direct manipulation serve as implicit feedback to steer statistical models and dimensionality-reduction pipelines (“**Semantic interaction for visual text analytics**,” 2012; and “Semantic interaction for sensemaking: inferring analytical reasoning for model steering,” 2012). Complementing these interaction techniques, Endert has contributed influential conceptual and methodological frameworks for understanding *how analysis unfolds over time*, including provenance models that characterize the types and purposes of analytic history in visualization and data analysis (“**Characterizing provenance in visualization and data analysis**,” 2015; and “Analytic provenance: process+ interaction+ insight,” 2011). More recently, his scholarship has engaged the integration of **machine learning within visual analytics**—surveying challenges and opportunities for combining interactive visualization with analytic models (“**The state of the art in integrating machine learning into visual analytics**,” 2017), while also addressing human factors such as cognitive bias and reflective awareness during analysis (e.g., “Warning, bias may occur,” 2017; and later work on bias design spaces and interaction traces). Together, these threads position his lab’s contributions at the intersection of interaction design, model steering, evaluation, and the socio-cognitive realities of analytical work.\n\n---\n\n## Notable Works\n- **Large-display sensemaking as a cognitive workspace:** In “**Space to think: large high-resolution displays for sensemaking**” (2010), Endert helped establish how expansive visual workspaces support external cognition—enabling analysts to use spatial layout for recall, relationship discovery, and iterative reasoning—an idea later broadened through work on opportunities and design issues for high-resolution display visualization (2011).\n- **A unifying framework for analytic history and accountability:** In “**Characterizing provenance in visualization and data analysis: an organizational framework of provenance types and purposes**” (2015), he advanced a structured account of provenance that clarifies what is captured, why it matters, and how it supports insight, reproducibility, and communication—building on earlier articulation of analytic provenance as a combination of process, interaction, and insight (2011).\n- **Model steering through semantic interaction in text analytics:** In “**Semantic interaction for visual text analytics**” (2012), Endert introduced an influential approach in which users’ direct manipulation of spatializations provides semantically meaningful signals that can be interpreted by algorithms—linking cognition and computation to support iterative sensemaking and interactive analytic modeling.\n\n---\n\n## Academic Background\nAt Georgia Tech, Endert has built a prominent academic profile in **HCI** and **visual analytics**, with a publication record that spans foundational systems research, conceptual frameworks, and evaluative scholarship, reflected in substantial scholarly impact (7,521 citations). His early and sustained focus on large-display environments and cyber-related analytic contexts (e.g., “Visualizing cyber security: Usable workspaces,” 2009) suggests formative engagement with applied, high-stakes analysis settings, later expanding into generalizable interaction paradigms (semantic interaction, observation-level interaction) and methodological contributions to evaluation in visualization (e.g., “Beyond usability and performance,” 2016; and “Task-based effectiveness of basic visualizations,” 2018). Over time, his affiliations and collaborations have remained strongly anchored in the visualization and visual analytics research communities, where his work on <u>human-centered interactive analytics</u>, provenance, and the integration of machine learning into analytic workflows has helped shape both research directions and evaluation norms for the field."}, "_cAwG4gAAAAJ": {"keywords": "Cybersecurity, AI, Blockchain, Digital Signal Processing (DSP), Computers, Industrial Internet of Things (IIoT), Smart Contracts, Cloud Computing, Blockchain Scalability & Interoperability, Differential Privacy for LLMs", "summary": "## Overview\nVijay Madisetti, **Fellow IEEE**, is a **Professor of Cybersecurity & Privacy** at **Georgia Tech**, where he leads research and teaching at the intersection of *secure computing systems* and *data-driven engineering*. His work centers on building **trustworthy cyber-physical and cloud-scale platforms** by integrating **AI**, **blockchain**, and **digital signal processing (DSP)** methods, with particular emphasis on <u>security, privacy, and integrity</u> in real-world deployments—from industrial systems and healthcare to payments and identity infrastructure.\n\n---\n\n## Research Areas\nMadisetti’s research spans several tightly connected domains that reflect a long-running focus on *systems that must work under stringent performance and trust constraints*. In **blockchain and distributed trust**, he has advanced architectures for industrial settings, notably in “**Blockchain platform for industrial internet of things**” (2016), which frames blockchain as an enabling substrate for <u>industrial IoT provenance, automation, and multi-stakeholder coordination</u>. Complementing this systems lens, his work on **scalability and interoperability**—including “**Method and system for tuning blockchain scalability for fast and low-cost payment and transaction processing**” (2018) and later identity/interoperability approaches (e.g., 2022)—addresses the practical trade space among throughput, cost, decentralization, and security in transaction networks. In **cloud computing and data-intensive systems**, he has contributed to workload modeling and large-scale analytics, including “**Synthetic workload generation for cloud computing applications**” (2011) and the industrial analytics pipeline “**Analyzing massive machine maintenance data in a computing cloud**” (2011), which anticipates modern cloud-native telemetry and predictive maintenance. His **health informatics** research, exemplified by “**A cloud-based approach for interoperable electronic health records (EHRs)**” (2013), targets <u>interoperability</u> and stakeholder-aligned architectures for sensitive data ecosystems. Underpinning these applied security and systems efforts is deep expertise in **DSP theory and implementation**, reflected in foundational reference works such as the “**Digital Signal Processing Handbook**” (2010) and technical contributions including “**Cyclostationary signal analysis**” (2017), connecting statistical signal structure to communications and time-series analysis.\n\n---\n\n## Notable Works\n- **Industrial blockchain for IoT trust and coordination:** “**Blockchain platform for industrial internet of things**” (2016) — a highly cited contribution that helped define blockchain-backed approaches to <u>industrial IoT</u> workflows, data integrity, and multi-party automation in manufacturing and supply-chain contexts.  \n- **Authoritative DSP reference and systems perspective:** “**Digital Signal Processing Handbook**” (2010) — a major scholarly reference consolidating theory, algorithms, and implementation considerations that bridge *signal processing fundamentals* with real engineering practice.  \n- **Cloud-enabled interoperability in healthcare data systems:** “**A cloud-based approach for interoperable electronic health records (EHRs)**” (2013) — an influential architecture-oriented work addressing <u>EHR interoperability</u> through cloud design choices, standards challenges, and stakeholder requirements.\n\n---\n\n## Academic Background\nAcross a publication record spanning from early work in **parallel and optimistic simulation** (e.g., “WOLF: A rollback algorithm for optimistic distributed simulation systems,” 1988) through **VLSI/embedded DSP** (e.g., “VLSI digital signal processors,” 1995; “System on chip or system on package?,” 2002) to contemporary **cybersecurity, blockchain, and privacy-preserving AI**, Madisetti’s career reflects sustained leadership in high-impact, systems-oriented research. His election as an **IEEE Fellow** signals internationally recognized contributions to the field, and his longstanding appointment at **Georgia Tech** places him within one of the leading U.S. ecosystems for electrical and computer engineering, computing, and security research. The breadth of his scholarly and educational output—including widely used *hands-on* texts on the **Internet of Things** (2014), **cloud computing** (2014), and **blockchain applications** (2017)—suggests a strong commitment to translating advanced research into curricula and practitioner-facing frameworks, while his citation impact (8,454 total citations) underscores sustained influence across **DSP**, **cloud systems**, and **secure decentralized computing**."}, "_pdhnmwAAAAJ": {"keywords": "Randomized Algorithms, Discrete Probability, Machine Learning, Markov Chain Monte Carlo (MCMC), Glauber Dynamics, Spectral Independence, Mixing Times, Spin Systems (Ising/Hardcore/Potts), Approximate Counting and Sampling (FPRAS), Correlation Decay (Uniqueness/Strong Spatial Mixing)", "summary": "## Overview\nZongchen Chen is a researcher at **Georgia Tech**, working at the interface of **randomized algorithms**, **discrete probability**, and *theoretical machine learning*. His work centers on <u>Markov chain Monte Carlo (MCMC)</u> and the mathematical foundations of efficient sampling and counting in high-dimensional discrete models, with a particular emphasis on **Glauber dynamics**, **spectral independence**, and *entropy/functional-inequality techniques* that yield sharp convergence guarantees. Across a body of work exceeding **900 citations**, Chen has helped shape modern understanding of when local stochastic dynamics provably mix in near-optimal time, and how these guarantees translate into algorithmic results for <u>spin systems</u>, graphical models, and related probabilistic structures.\n\n---\n\n## Research Areas\nChen’s research program develops rigorous tools for analyzing mixing times and concentration phenomena in discrete probabilistic models that arise in statistical physics and machine learning. A central theme is <u>optimal mixing of local Markov chains</u>, especially the **Glauber dynamics** (Gibbs sampling): in “**Optimal Mixing of Glauber Dynamics: Entropy Factorization via High-Dimensional Expansion**” (2021), he advances the spectral-independence framework by connecting it to *entropy factorization* and high-dimensional expansion, obtaining **O(n log n)**-type optimal mixing bounds in broad settings. He further unifies coupling and functional-inequality perspectives in “**On Mixing of Markov Chains: Coupling, Spectral Independence, and Entropy Factorization**” (2022), showing that contractive couplings for local chains can imply optimal mixing and modified log-Sobolev bounds for dynamics including block heat-bath and **Swendsen–Wang**-type chains. Chen also contributes to phase-transition–sensitive sampling theory for antiferromagnetic models, demonstrating rapid mixing up to (and probing at) uniqueness thresholds—e.g., “**Rapid mixing of Glauber dynamics up to uniqueness via contraction**” (2023) and subsequent works on uniqueness-threshold behavior and random regular graphs. Complementing these discrete results, he has contributed to continuous-state sampling theory through sharp convergence analysis of **Hamiltonian Monte Carlo** in “**Optimal Convergence Rate of Hamiltonian Monte Carlo for Strongly Logconcave Distributions**” (2022). Additional strands include algorithmic consequences of correlation decay and strong spatial mixing (e.g., colorings on trees), polynomial-stability methods for spectral independence (“**Spectral Independence via Stability and Applications to Holant-Type Problems**,” 2022), and complexity-theoretic perspectives on learning/testing in graphical models (e.g., identity testing lower bounds for colorings and antiferromagnetic Ising models, 2020).\n\n---\n\n## Notable Works\n- **Entropy factorization + high-dimensional expansion for optimal Glauber mixing:** In “*Optimal Mixing of Glauber Dynamics: Entropy Factorization via High-Dimensional Expansion*” (2021), Chen establishes near-optimal mixing guarantees by strengthening the **spectral independence** approach and tying it to <u>entropy factorization</u>, yielding broadly applicable **O(n log n)**-scale bounds for Gibbs sampling.\n- **A unifying theory connecting coupling, spectral independence, and functional inequalities:** In “*On Mixing of Markov Chains: Coupling, Spectral Independence, and Entropy Factorization*” (2022), he shows how **contractive couplings** for local dynamics lead to optimal mixing-time and modified log-Sobolev estimates, covering Glauber dynamics, block dynamics, and **Swendsen–Wang**-type chains under a common analytic framework.\n- **Rapid mixing up to uniqueness for antiferromagnetic spin systems:** In “*Rapid mixing of Glauber dynamics up to uniqueness via contraction*” (2023), Chen provides sharp rapid-mixing results for general antiferromagnetic 2-spin systems (including the hardcore and antiferromagnetic Ising models), clarifying the algorithmic reach of MCMC in regimes governed by <u>tree uniqueness</u>.\n\n---\n\n## Academic Background\nBased at **Georgia Tech**, Chen’s publication trajectory indicates a sustained focus on the theoretical foundations of randomized computation, spanning early work in networked systems and combinatorial design (e.g., data-center networking and SBIBDs) and then consolidating into a mature research agenda in **probability, algorithms, and MCMC theory**. His record of highly cited contributions from 2020 onward suggests deep integration into the theoretical computer science and probability communities working on sampling, counting, and graphical models, with recurring engagement in core themes such as **uniqueness thresholds**, **correlation decay**, and <u>functional inequalities</u> (modified log-Sobolev/entropy contraction). The breadth of venues and topics—ranging from discrete spin systems and Holant-type counting problems to **Hamiltonian Monte Carlo** for strongly logconcave targets—reflects affiliations and collaborations that bridge algorithms, probability, and machine learning theory, consistent with an active role in Georgia Tech’s research ecosystem in **randomized algorithms** and *high-dimensional inference*."}, "aD5ev8QAAAAJ": {"keywords": "Feminist HCI, Humanistic Computing, Sustainability, Women’s Health, Design Research, Critical Design, Research through Design (RtD), Care Ethics, Posthuman Design, Maker Cultures and Hackerspaces", "summary": "## Overview\nShaowen Bardzell is a leading scholar in the School of Interactive Computing at the Georgia Institute of Technology, where she contributes to an internationally visible program of research at the intersection of **Human–Computer Interaction (HCI)** and the humanities. Her work is widely recognized for advancing **design research** grounded in *feminist theory* and *critical scholarship*, with a sustained emphasis on <u>Feminist HCI</u>, <u>Humanistic Computing</u>, and socially engaged approaches to technology. Across highly cited agenda-setting publications and empirically grounded studies, Bardzell has shaped how HCI understands questions of **agency**, **equity**, **care**, and **social justice**, while also extending these commitments into domains such as <u>sustainability</u> and <u>women’s health</u>.\n\n---\n\n## Research Areas\nBardzell’s research spans theoretical, methodological, and applied contributions that collectively reframe HCI as a field accountable to cultural critique and ethical inquiry. A foundational strand of her scholarship develops <u>Feminist HCI</u> as both a critical lens and a design agenda, articulated in “**Feminist HCI: taking stock and outlining an agenda for design**” (2010) and further operationalized in “**Towards a feminist HCI methodology: social science, feminism, and HCI**” (2011), where she connects feminist epistemologies to the moral and scientific tensions that arise when HCI engages social change. A second major strand centers <u>critical design</u> and research-through-design as knowledge practices, interrogating what makes design “critical” in “**What is ‘critical’ about critical design?**” (2013) and addressing provocation as an intellectual and practical challenge in “**Critical design and critical theory: the challenge of designing for provocation**” (2012); this line is complemented by contributions to RtD legitimacy and documentation, including “**Immodest proposals: Research through design and knowledge**” (2015) and “Documenting the research through design process” (2016). Bardzell also advances <u>humanistic HCI</u>, explicitly theorizing the role of humanities-based interpretation in computing in “**What is Humanistic HCI?**” (2022) and related work that situates HCI within longer traditions of critique, aesthetics, and meaning-making. In parallel, she has produced influential empirical research on *making cultures* and sustainability—e.g., “**The proper care and feeding of hackerspaces: Care ethics and cultures of making**” (2015) and “Reconstituting the utopian vision of making: HCI after technosolutionism” (2016)—as well as scholarship that brings feminist and interpretive methods to women’s health and intimate life, including research on menopause (“Parting the Red Sea: sociotechnical systems and lived experiences of menopause,” 2019) and sexuality in HCI (“How HCI talks about sexuality,” 2011). Across these areas, her work consistently links design practice to power, values, and lived experience.\n\n---\n\n## Notable Works\n- **Established the intellectual foundation and agenda for <u>Feminist HCI</u> in mainstream interaction design**, synthesizing prior work and setting a forward-looking research program in “**Feminist HCI: taking stock and outlining an agenda for design**” (2010).  \n- **Clarified and strengthened the theoretical infrastructure of <u>critical design</u> in HCI**, explaining how ethics, values, and critique operate within design practice in “**What is ‘critical’ about critical design?**” (2013).  \n- **Developed a methodological bridge between feminist social science and HCI’s design-oriented inquiry**, addressing how HCI can responsibly pursue social change through research and design in “**Towards a feminist HCI methodology: social science, feminism, and HCI**” (2011).  \n\n---\n\n## Academic Background\nBased at Georgia Tech’s School of Interactive Computing, Bardzell has built an academic profile characteristic of a senior, field-shaping HCI scholar whose work is deeply embedded in premier venues such as CHI, CSCW, and design research forums, with sustained influence reflected in a large citation record (11,533 citations). Her publication trajectory indicates long-standing interdisciplinary affiliations spanning **HCI**, **CSCW**, and **design research**, and an intellectual formation that integrates feminist theory, critical theory, and humanities-based interpretive methods into computing scholarship—most explicitly in her contributions to <u>Humanistic HCI</u> (e.g., “What is Humanistic HCI?,” 2022) and her sustained development of research-through-design as a knowledge-making practice (“Immodest proposals,” 2015). The breadth of her work—from foundational theoretical agendas (Feminist HCI; critical design) to empirically grounded studies of making cultures, care ethics, sustainability, and women’s health (e.g., menopause research in 2019)—suggests extensive collaboration across disciplinary communities and a record of scholarly leadership aligned with major contemporary research movements in socially responsible and critically oriented computing."}, "assJWZYAAAAJ": {"keywords": "Ubiquitous Computing, Mobile Health (mHealth), Wearable Sensing, Contactless Physiological Monitoring, Doppler Radar Sensing, Non-speech Body Sound Recognition, Affective Computing, Mindless/Persuasive Interventions, Smartphone-based Point-of-Care Diagnostics, Substance Use and Vaping Behavior Monitoring", "summary": "## Overview\nAlexander Adams is a researcher in **Ubiquitous Computing** at the Georgia Institute of Technology, where he contributes to a program of work centered on building *practical, human-centered sensing and intervention systems* that can operate in everyday life. His scholarship emphasizes <u>unobtrusive, mobile, and wearable sensing</u>—often using **commercial off-the-shelf hardware**—to infer health- and behavior-relevant signals and to translate those signals into *timely, low-burden interventions*. Across projects spanning contactless radar, wearable acoustics, and smartphone-based diagnostics, Adams’ research advances <u>real-world deployable systems</u> that reduce friction in measurement and support **health, well-being, and behavior change** at scale.\n\n---\n\n## Research Areas\nAdams’ research spans three tightly connected areas: (1) **pervasive physiological and behavioral sensing**, (2) **behavioral and affective interventions**, and (3) **accessible health measurement and diagnostics**. In pervasive sensing, he has developed systems that capture subtle physiological cues without disrupting daily routines, exemplified by *DoppleSleep* (2015), which uses short-range Doppler radar to track sleep-related physical and physiological variables in a contactless and continuous manner, and by *BodyBeat* (2014), which demonstrates mobile recognition of <u>non-speech body sounds</u> (e.g., eating, breathing, laughter, cough) using wearable acoustics to enable inference about dietary and respiratory behaviors. In intervention-focused work, Adams has investigated how interfaces can shape experience and behavior through *low-attention pathways*, formalized in “*Mindless computing: designing technologies to subtly influence behavior*” (2015), and operationalized in *EmotionCheck* (2016; 2017), which leverages bodily signals and <u>false/offset biofeedback</u> (e.g., altered heart-rate perception via wearable cues) to support emotion regulation and anxiety reduction. More recently, his portfolio extends toward equity- and access-oriented health technologies, including *OptoBeat* (2022), which addresses skin-tone calibration disparities in pulse oximetry, and a line of smartphone-based and low-cost clinical measurement systems (e.g., point-of-care microscopy and urinalysis prototypes), reflecting a broader commitment to <u>ubiquitous healthcare</u> through scalable, field-ready instrumentation.\n\n---\n\n## Notable Works\n- **DoppleSleep (2015)** — Introduced a <u>contactless sleep sensing</u> approach using short-range Doppler radar to continuously track sleep-related physical and physiological variables with commercial off-the-shelf modules (“*Dopplesleep: A contactless unobtrusive sleep sensing system using short-range doppler radar*”).\n- **BodyBeat (2014)** — Pioneered mobile and wearable acoustic sensing of <u>non-speech body sounds</u> in real-world contexts, enabling recognition of behaviors such as food intake and breathing (“*BodyBeat: a mobile system for sensing non-speech body sounds*”).\n- **EmotionCheck + Mindless Computing (2015–2017)** — Advanced the concept of *mindless* (low-attention) persuasive technology and demonstrated emotion regulation via altered perception of bodily signals through false heart-rate feedback (“*Mindless computing: designing technologies to subtly influence behavior*” (2015); “*EmotionCheck: leveraging bodily signals and false feedback to regulate our emotions*” (2016); “*EmotionCheck: A wearable device to regulate anxiety through false heart rate feedback*” (2017)).\n\n---\n\n## Academic Background\nBased at the Georgia Institute of Technology, Adams’ academic trajectory aligns with the interdisciplinary traditions of **HCI** and **ubiquitous computing**, integrating methods from sensing systems, human-subjects experimentation, and health-oriented design. His publication record shows sustained contributions across premier problem spaces in mobile and wearable computing—sleep, affect, dietary/respiratory behavior, and just-in-time interventions—followed by an expansion into <u>accessible and equitable health measurement</u> (e.g., addressing pulse oximetry bias in *OptoBeat*, 2022). The breadth of his work—from radar-based physiology to wearable acoustics, tangible interfaces for self-report (e.g., *Keppi*, 2018), and smartphone-enabled diagnostics—suggests strong cross-lab and cross-domain collaboration typical of Georgia Tech’s computing and health ecosystem. With **1,196 citations** and multiple highly cited systems papers (e.g., *DoppleSleep*, *BodyBeat*, *EmotionCheck*), Adams has established a visible scholarly footprint in ubiquitous computing research communities and an applied agenda oriented toward scalable, real-world deployment."}, "atNjbs0AAAAJ": {"keywords": "Computer Graphics, Computational Physics, Scientific Machine Learning, Topology Optimization, Microstructure Design, Metamaterials, Differentiable Simulation, Incompressible Fluid Simulation, Symplectic Neural Networks, Reinforcement Learning for Control", "summary": "## Overview\nBo Zhu is a faculty researcher in the School of Interactive Computing at the Georgia Institute of Technology, where he leads a research program at the intersection of **computer graphics**, **computational physics**, and **scientific machine learning**. His work centers on building *physically grounded* algorithms that unify simulation, optimization, and learning—particularly for complex continuum phenomena and engineered structures. Across highly cited contributions in topology optimization, fluid dynamics, and physics-informed learning, Zhu’s group advances <u>differentiable simulation</u> and <u>structure–physics co-design</u> methods that make high-fidelity physical reasoning both *computationally efficient* and *algorithmically expressive*.\n\n---\n\n## Research Areas\nZhu’s research spans three tightly connected areas. First, he has made substantial contributions to **topology optimization and computational design**, developing multi-resolution and geometry-aware formulations that address manufacturability and fine-feature representation. In “**Two-scale topology optimization with microstructures**” (2017), he introduces a two-scale framework that treats **multi-material microstructures** as building blocks, enabling simultaneous optimization of global structure and local material distribution; this line continues through work such as “**Computational discovery of extremal microstructure families**” (2018), which systematizes the search for microstructure families that achieve extreme effective properties. Complementing this, “**Narrow-band topology optimization on a sparsely populated grid**” (2018) targets sparse, thin, intricate features by restricting computation to a narrow active region, substantially reducing the cost of resolving high-resolution details.\n\nSecond, Zhu’s scholarship in **computational fluid dynamics for graphics** emphasizes robust representations of vortical and surface-tension-dominant phenomena. His earlier simulation work addresses challenging codimensional effects—thin sheets, filaments, and droplets—through methods such as “**Codimensional surface tension flow on simplicial complexes**” (2014) and “**Codimensional non-Newtonian fluids**” (2015). More recent contributions pursue mathematically structured solvers and representations for vortical flow, including “**Clebsch gauge fluid**” (2021), “**Impulse fluid simulation**” (2022), and flow-map-based formulations such as “**Fluid simulation on neural flow maps**” (2023) and subsequent Eulerian–Lagrangian flow-map methods.\n\nThird, Zhu contributes to **scientific machine learning** by embedding physical structure—especially Hamiltonian mechanics and constraints—into learning architectures. His work on symplectic learning, including “**Nonseparable symplectic neural networks**” (2020) and “**Symplectic neural networks in Taylor series form for Hamiltonian systems**” (2021), targets long-term stability and faithful energy behavior, while “**Learning physical constraints with neural projections**” (2020) proposes neural operators that explicitly enforce constraints during prediction. This emphasis on differentiability and physical consistency also appears in systems work such as “**Fluidlab: A differentiable environment for benchmarking complex fluid manipulation**” (2023), which positions differentiable fluid simulation as an enabling substrate for learning and control.\n\n---\n\n## Notable Works\n- **Two-scale structural and material co-optimization:** In “**Two-scale topology optimization with microstructures**” (2017), Zhu proposes a two-level optimization pipeline that couples global topology with local **multi-material microstructure** selection, establishing a widely referenced template for <u>multiscale metamaterial design</u> and fabrication-aware optimization.\n- **Efficient representation for thin, sparse optimized structures:** “**Narrow-band topology optimization on a sparsely populated grid**” (2018) introduces a narrow-band strategy that concentrates computation where material exists, enabling high-resolution treatment of slender, intricate features central to both natural morphologies and engineered lightweight designs.\n- **Physics-structured learning for long-term dynamical prediction:** Through “**Nonseparable symplectic neural networks**” (2020) and “**Symplectic neural networks in Taylor series form for Hamiltonian systems**” (2021), Zhu advances machine learning architectures that preserve symplectic structure, improving stability and accuracy for <u>Hamiltonian dynamics</u> under limited observations.\n\n---\n\n## Academic Background\nBased at Georgia Tech’s School of Interactive Computing, Zhu’s publication record reflects an academic trajectory rooted in **computer graphics** and **computational mechanics**, with sustained engagement in the premier venues spanning graphics, simulation, and learning. His early work on fluid animation and interactive systems (e.g., sketch-based fluid illustration and particle/grid formulations) evolved into foundational research on codimensional flow and surface tension, and later expanded into optimization-driven design for structures and devices—exemplified by highly cited contributions to microstructure-based and narrow-band topology optimization. In the last several years, his group’s output indicates a strong affiliation with the **scientific machine learning** movement, particularly in physically structured neural architectures (symplectic and constraint-enforcing models) and differentiable simulation environments (e.g., Fluidlab) that connect learning, control, and robotics. With a citation profile exceeding 2,500 total citations and a body of work spanning simulation, optimization, and learning, Zhu is widely recognized as a cross-disciplinary contributor linking *theory-driven physical modeling* with *modern differentiable and data-driven methods* in interactive computing."}, "bd9Rk1MAAAAJ": {"keywords": "Formal methods, Programming languages, Artificial intelligence, Reactive synthesis, Temporal logic (LTL/LTLf), Automata-based verification, Specification-guided reinforcement learning, Markov decision processes, Quantitative games, Multi-agent systems and Nash equilibria", "summary": "## Overview\nSuguman Bansal is an Assistant Professor in the School of Computer Science at the Georgia Institute of Technology, where he leads research at the intersection of **formal methods**, **programming languages**, and **artificial intelligence**. His work centers on making *high-level, logically precise task descriptions* usable in learning and synthesis pipelines, with an emphasis on <u>temporal-logic specifications</u>, <u>automata-theoretic methods</u>, and <u>correct-by-construction decision-making</u>. Across his publications (with **429 citations**), Bansal has advanced methods that connect reactive synthesis and reinforcement learning, enabling agents and systems to satisfy complex specifications while scaling to practically relevant problem sizes.\n\n---\n\n## Research Areas\nBansal’s research spans specification-driven learning and synthesis, quantitative verification, and scalable automata constructions. A major theme is **reinforcement learning from logical specifications**, where tasks are expressed in temporal logic and compiled into learning objectives: in “**Compositional reinforcement learning from logical specifications**” (2021), he studies how complex logically defined tasks can be decomposed compositionally to improve learning efficiency and modularity. This line is extended toward end-to-end pipelines that reconcile the assumptions of synthesis (known models) with the realities of learning (unknown dynamics), as in “**A framework for transforming specifications in reinforcement learning**” (2022) and “**Specification-guided reinforcement learning**” (2022), which develop mechanisms for transforming or compiling specifications into forms that better support RL optimization and generalization. In parallel, he contributes to **reactive synthesis** and its scalability bottlenecks, notably DFA construction from finite-horizon temporal logic: “**Hybrid compositional reasoning for reactive synthesis from finite-horizon specifications**” (2020) targets the LTLf-to-DFA bottleneck through hybrid and compositional reasoning, while later work on strategy validation (e.g., “**Model checking strategies from synthesis over finite traces**,” 2023) strengthens the verification story after synthesis. A second pillar is **quantitative verification and games**, where he develops automata-based comparison mechanisms—e.g., “**Comparator automata in quantitative verification**” (2022) and related discounted-sum inclusion papers—alongside analysis paradigms such as *satisficing* (e.g., “**On Satisficing in Quantitative Games**,” 2021), which formalize “good-enough” objectives as an alternative to strict optimization in multi-agent and reactive settings.\n\n---\n\n## Notable Works\n- **Compositional reinforcement learning from logical specifications** (2021): Introduces a compositional approach to learning policies for complex tasks defined by temporal/logical specifications, helping bridge <u>automata-based specification handling</u> with practical RL training and modular task structure.\n- **Hybrid compositional reasoning for reactive synthesis from finite-horizon specifications** (2020): Addresses the central scalability challenge in LTLf synthesis—conversion to deterministic automata—via hybrid compositional reasoning, advancing <u>finite-trace temporal logic</u> synthesis toward larger specifications.\n- **A framework for transforming specifications in reinforcement learning** (2022): Develops a unifying framework for compiling and transforming temporal-logic specifications to better support learning in unknown MDPs, clarifying how <u>specification transformations</u> can improve the tractability and effectiveness of specification-driven RL.\n\n---\n\n## Academic Background\nBansal’s publication trajectory indicates a sustained research program across **formal verification**, **automata theory**, and **learning/synthesis**, with early work engaging algorithmic game-theoretic foundations (e.g., “Algorithmic analysis of Regular repeated games,” 2016) and subsequent contributions to quantitative inclusion and discounted-sum reasoning (“Automata vs Linear-Programming Discounted-Sum Inclusion,” 2018; “Safety and co-safety comparator automata for discounted-sum inclusion,” 2019). His later and most-cited work consolidates a cross-disciplinary profile that connects temporal logic, reactive synthesis, and reinforcement learning (2018–2025), including synthesis for asynchronous/coordination programs (“Synthesis of asynchronous reactive programs…,” 2018; “Synthesis of coordination programs…,” 2020) and post-synthesis validation (“Model checking strategies from synthesis over finite traces,” 2023). Now at Georgia Tech as an Assistant Professor, he is affiliated with the broader research communities in *programming languages*, *formal methods*, and *AI*, with impact reflected in the strong citation footprint of his contributions to compositional specification-guided RL and scalable finite-trace synthesis."}, "bsFxGUIAAAAJ": {"keywords": "Cryptography, Cryptocurrencies, Blockchain, Game Theory, Distributed Computing, Universally Composable Security (UC/GUC), Secure Multi-Party Computation (MPC), Proof-of-Stake Consensus (Ouroboros), Byzantine Agreement & Broadcast, Differential Privacy", "summary": "## Overview\nVassilis Zikas is a faculty researcher at **Georgia Tech**, where he leads work at the intersection of **cryptography** and *distributed systems*, with a particular emphasis on the rigorous foundations of **blockchains** and **cryptocurrencies**. His scholarship centers on <u>universally composable (UC) security</u> and its role in designing protocols that remain secure under composition—spanning *permissionless consensus*, *secure multi-party computation (MPC)*, and *incentive-aware* (game-theoretic) models of adversarial behavior. Across these themes, Zikas is known for advancing <u>composable ledger abstractions</u> and for bringing formal cryptographic reasoning to real-world decentralized infrastructures.\n\n---\n\n## Research Areas\nZikas’s research spans several tightly connected areas of modern cryptography and distributed computing. A major thrust is <u>composable security for distributed ledgers</u>, exemplified by his work on Proof-of-Stake protocols and ledger modeling: **“Ouroboros genesis: Composable proof-of-stake blockchains with dynamic availability”** (2018) introduces a PoS design that enables safe (re)joining from genesis information, addressing dynamic participation and availability while maintaining strong composability guarantees; **“Ouroboros crypsinous: Privacy-preserving proof-of-stake”** (2019) extends this line by formalizing *private ledgers* in the (G)UC setting and proving privacy against adaptive attacks. Complementing PoS, Zikas has contributed to <u>UC treatments of Proof-of-Work systems</u>, most notably **“Bitcoin as a Transaction Ledger: A Composable Treatment”** (2024), which reframes Bitcoin’s security via a universally composable ledger functionality—moving beyond property-based proofs to support modular composition with higher-level protocols. In parallel, he has produced foundational results in <u>secure multi-party computation</u> and <u>synchronous/asynchronous protocol theory</u>, including **“Universally composable synchronous computation”** (2013) on achieving guarantees such as input completeness and termination under synchrony, and **“Fair and robust multi-party computation using a global transaction ledger”** (2015), which leverages a global ledger resource to revisit classical impossibility results for fairness/robustness under dishonest majorities. Finally, Zikas has helped shape <u>cryptography under incentives</u>—a line that integrates *game theory* with protocol security—through works such as **“Rational protocol design: Cryptography against incentive-driven adversaries”** (2013) and **“But why does it work? A rational protocol design treatment of bitcoin”** (2018), which analyze how strategic behavior and external incentives interact with cryptographic assumptions in deployed blockchain systems.\n\n---\n\n## Notable Works\n- **Composable Proof-of-Stake with dynamic participation:** In **“Ouroboros genesis: Composable proof-of-stake blockchains with dynamic availability”** (2018), Zikas and coauthors develop a PoS protocol that supports secure joining/rejoining from the genesis block while maintaining <u>composable security</u> under realistic availability dynamics—an influential step in formal PoS protocol design.\n- **Universally composable Bitcoin ledger model:** **“Bitcoin as a Transaction Ledger: A Composable Treatment”** (2024) provides a UC framework for Bitcoin, replacing non-composable, property-based analyses with a <u>UC ledger abstraction</u> that can be safely composed with other cryptographic protocols and applications.\n- **Fairness and robustness via global ledgers:** In **“Fair and robust multi-party computation using a global transaction ledger”** (2015), Zikas and coauthors show how a <u>global transaction ledger</u> can act as a powerful setup resource to overcome classical barriers around fairness and guaranteed output delivery in MPC, connecting cryptocurrency infrastructure to long-standing MPC impossibility frontiers.\n\n---\n\n## Academic Background\nBased on his publication trajectory and long-running engagement with the UC framework, MPC, and distributed protocol theory—from early work on secure computation and corruption models (e.g., *MPC vs. SFE* and studies of realistic failures) through later advances in synchronous UC computation (2013) and ledger-enabled MPC (2015)—Zikas has developed a research profile characteristic of rigorous training in **theoretical computer science** and **cryptography**, with strong ties to both the cryptography and distributed computing communities. His affiliation with **Georgia Tech** situates him within a major research ecosystem for security, systems, and theory, and his cross-cutting output—spanning UC/GUC foundations, adaptive security (e.g., adaptively secure broadcast), and blockchain protocol design (Ouroboros, composable Bitcoin)—indicates sustained collaboration across academic and applied cryptography groups. With approximately **3,300 citations**, his work has achieved broad scholarly uptake, reflecting both foundational impact (e.g., UC treatments of computation and broadcast) and practical relevance to modern decentralized systems (e.g., PoS security, privacy-preserving ledgers, and incentive-aware analyses of blockchain behavior)."}, "bvW5Hs8AAAAJ": {"keywords": "Beyond NP, Automated Reasoning, Formal Methods, Artificial Intelligence, Approximate Model Counting (#SAT), Hashing-based Counting and Sampling, SAT Solving (CNF-XOR/PB-XOR), Weighted Model Counting, Probabilistic Inference, Fairness and Trustworthy Machine Learning Verification", "summary": "## Overview\nKuldeep S. Meel is an **Associate Professor** in the **Department of Computer Science** at the **University of Toronto**, where he leads a research program at the intersection of **automated reasoning**, **formal methods**, and *AI-enabled* trustworthy systems. His work centers on pushing the boundaries of what is computationally feasible *beyond NP* by designing algorithms with provable guarantees for <u>constrained counting and sampling</u>, with particular emphasis on **#SAT/model counting**, **weighted counting**, and **near-uniform witness generation**. With a publication record that has accrued **4,438 citations**, Meel is widely recognized for translating foundational ideas—especially hashing-based reductions to SAT—into scalable systems that make <u>probabilistic inference</u>, <u>verification</u>, and <u>trustworthy AI</u> practically attainable.\n\n---\n\n## Research Areas\nMeel’s research spans the theory and engineering of **model counting** and **sampling** as core primitives for modern AI and verification workflows. A major thread of his work develops *hashing-based* approximate counting with formal accuracy guarantees, as exemplified by early contributions such as **“A scalable approximate model counter” (2013)** and subsequent refinements that reduce oracle complexity in **“Algorithmic improvements in approximate counting for probabilistic inference: From linear to logarithmic SAT calls” (2016)**. He has also advanced **weighted model counting** and **distribution-aware sampling**, notably in **“Distribution-aware sampling and weighted model counting for SAT” (2014)**, enabling probabilistic reasoning pipelines where assignments carry non-uniform semantics. Complementing algorithmic advances, Meel has driven solver-level innovation for mixed constraint classes—especially CNF-XOR—through systems work such as **“BIRD: Engineering an Efficient CNF-XOR SAT Solver and its Applications to Approximate Model Counting” (2019)** and later studies on CNF-XOR solving strategies (e.g., *tinted/detached/lazy* paradigms). Beyond counting/sampling as ends in themselves, his papers demonstrate these primitives as enabling technology for *trustworthy AI* and security: quantitative guarantees for neural networks in **“Quantitative verification of neural networks and its security applications” (2019)** and **“Scalable quantitative verification for deep neural networks” (2021)**, as well as auditing the reliability and faithfulness of post-hoc ML explanations in **“Assessing heuristic machine learning explanations with model counting” (2019)**. Across these domains, his work repeatedly returns to the central methodological idea that <u>provable randomized reductions</u> coupled with high-performance SAT/MaxSAT infrastructure can deliver both *scalability* and *formal guarantees*.\n\n---\n\n## Notable Works\n- **Hashing-based approximate model counting with guarantees**: Meel’s foundational line of work on scalable approximate #SAT—anchored by **“A scalable approximate model counter” (2013)** and strengthened by oracle-efficiency improvements in **“Algorithmic improvements in approximate counting for probabilistic inference: From linear to logarithmic SAT calls” (2016)**—helped establish approximate counting as a practical tool for probabilistic inference and verification.\n- **Weighted counting and distribution-aware sampling for probabilistic reasoning**: In **“Distribution-aware sampling and weighted model counting for SAT” (2014)**, Meel advanced techniques for **weighted model counting** and sampling that respect non-uniform distributions, broadening the applicability of SAT-based reasoning to probabilistic and decision-theoretic settings.\n- **Engineering CNF-XOR infrastructure for counting and sampling at scale**: Through **“BIRD: Engineering an Efficient CNF-XOR SAT Solver and its Applications to Approximate Model Counting” (2019)**, Meel contributed key solver-engineering ideas that improved the performance of counting/sampling pipelines relying on XOR-based universal hashing, bridging algorithmic theory with robust implementation.\n\n---\n\n## Academic Background\nMeel’s academic trajectory reflects a sustained focus on the *beyond-NP* landscape of reasoning tasks—especially the #P-hard core of counting and sampling—and their role as fundamental engines for AI and formal methods. Based on his long-running publication arc (from early seminal works in 2013–2016 on approximate counting and near-uniform generation to later solver engineering and trustworthy-AI applications), his career evidences deep engagement with the **SAT/SMT and automated reasoning communities**, as well as strong ties to **formal verification** and **probabilistic inference** research. His affiliations and impact are anchored in his current role at the **University of Toronto**, where his group’s contributions—spanning approximate and exact counting (e.g., **GANAK: A Scalable Probabilistic Exact Model Counter**, 2019), CNF-XOR solving, and quantitative verification of neural networks—have helped shape how formal guarantees are brought to bear on practical systems. The breadth of topics in his highly cited papers also suggests sustained collaboration across AI, security, and systems verification communities, with counting/sampling serving as a unifying methodological foundation."}, "cQPdjdgAAAAJ": {"keywords": "Social Network Analysis, Big Data Analytics, Computer Vision, Social Media Event Detection, Disaster Informatics, Landslide Detection, Multi-hazard Monitoring, Sentiment Analysis, Transportation Systems Analytics, Multilingual Text Mining", "summary": "## Overview\nAibek Musaev is a researcher affiliated with the **Georgia Institute of Technology**, where he works at the intersection of **social network analysis**, **big data analytics**, and **computer vision** to build *data-driven, real-time* understanding of complex societal and physical phenomena. His scholarship centers on <u>social sensing</u>—leveraging high-volume streams from platforms such as Twitter alongside other data sources (e.g., physical sensors and live cameras) to detect events, quantify public perception, and support decision-making in domains ranging from transportation safety to disaster management and infrastructure monitoring.\n\n---\n\n## Research Areas\nMusaev’s research spans several tightly connected areas unified by the challenge of extracting reliable signals from noisy, rapidly evolving data. A major thread is <u>disaster and multi-hazard informatics</u>, exemplified by the **LITMUS** line of work—e.g., *“LITMUS: a multi-service composition system for landslide detection”* (2014) and *“LITMUS: Landslide detection by integrating multiple sources.”* (2014)—which integrates heterogeneous information services to detect landslides when direct physical sensing is limited. Complementing this, he has advanced methods for *streaming social media analytics* and event detection, including concept drift-aware approaches in *“Concept drift adaptive physical event detection for social media streams”* (2019) and relevance/ranking strategies for improved signal quality in *“Gathering high quality information on landslides from twitter by relevance ranking of users and tweets”* (2016). Another prominent theme is <u>transportation and infrastructure intelligence</u>, where he applies sentiment analysis and perception mining to safety-critical technologies in *“Effects of the autonomous vehicle crashes on public perception of the technology”* (2021), and develops social-sensor approaches to detect infrastructure damage in *“Detection of Damage and Failure Events of Critical Public Infrastructure using Social Sensor Big Data.”* (2016). His portfolio also includes *vision-based* sensing for mobility and retail environments, such as traffic-camera analytics in *“Towards detection of abnormal vehicle behavior using traffic cameras”* (2019), reflecting a broader agenda of fusing computational methods across text, networks, and visual streams to produce actionable, real-time knowledge.\n\n---\n\n## Notable Works\n- **Quantifying public response to autonomous-vehicle incidents:** In *“Effects of the autonomous vehicle crashes on public perception of the technology”* (2021), Musaev analyzes before-and-after sentiment shifts following high-profile crashes (e.g., Uber and Tesla), establishing a data-driven framework for measuring how safety events reshape <u>public perception</u> of emerging transportation technologies.\n- **Multi-source landslide detection via service composition (LITMUS):** Through *“LITMUS: a multi-service composition system for landslide detection”* (2014) and *“LITMUS: Landslide detection by integrating multiple sources.”* (2014), he helps define an architecture for integrating social and other information sources to detect landslides—addressing the core problem that landslides often lack direct physical sensors and require <u>multi-hazard</u> reasoning.\n- **Infrastructure failure detection using social sensor big data:** In *“Detection of Damage and Failure Events of Critical Public Infrastructure using Social Sensor Big Data.”* (2016), Musaev demonstrates how social media can function as a distributed sensing layer for identifying damage and failure events, contributing methods that connect large-scale, unstructured reports to the monitoring needs of critical public systems.\n\n---\n\n## Academic Background\nMusaev’s publication trajectory indicates sustained, interdisciplinary engagement across **computer science**, data-intensive analytics, and applied societal domains, with a strong record of impact (approximately **579 citations**). His earlier work (2014–2017) is anchored in disaster informatics and service-oriented approaches to multi-source event detection—particularly the **LITMUS** landslide detection program—suggesting close collaboration with researchers in hazard science, information services, and social media mining. From 2016 onward, his collaborations and topics broaden toward infrastructure and transportation applications at Georgia Tech (e.g., work spanning civil/environmental engineering, computing, and policy-facing contexts as reflected in the authorship/departmental mix of the infrastructure-focused publications). In later studies (2019–2023), he extends this foundation to *adaptive* stream analytics (e.g., concept drift), camera-based behavioral sensing, and public-opinion measurement for safety-critical technologies, reflecting an evolving agenda around <u>real-time, trustworthy knowledge extraction</u> from heterogeneous big data sources."}, "cRvSNzIAAAAJ": {"keywords": "identity theft, privacy and data protection, information security, cybercrime, consumer protection, financial fraud, identity management, security policy and legislation, risk mitigation, digital forensics", "summary": "## Overview\nKim Luong MacLeod is a researcher affiliated with the Georgia Institute of Technology whose work centers on the societal and policy dimensions of **identity theft** and related forms of *information misuse*. Situated at the intersection of technology and society, MacLeod’s research emphasizes <u>identity theft as a multidimensional harm</u>—not only an economic or credit-related event, but also a phenomenon with significant legal, psychological, and civic implications, particularly as legislation and institutional responses evolve.\n\n---\n\n## Research Areas\nMacLeod’s research examines identity theft through a socio-technical and regulatory lens, with attention to how incidents occur, how victims experience harm, and how institutions respond. In “**The other side of identity theft: Not just a financial concern**” (2006), MacLeod foregrounds the need to move beyond narrow financial framings by analyzing identity theft as a broader threat to society, emphasizing the importance of understanding *all realms* of the problem: pathways of occurrence, the protections available to individuals, and the rights and remedies afforded under changing legal regimes. The paper’s focus on legislative history and “recent updates and efforts” highlights an enduring research interest in <u>policy development and practical countermeasures</u>, including how public awareness, institutional practices, and legal protections shape real-world outcomes for victims.\n\n---\n\n## Notable Works\n- **“The other side of identity theft: Not just a financial concern” (2006)** — A widely cited contribution (7 citations) arguing for a comprehensive framing of identity theft that includes *non-financial* harms, and surveying legislative trajectories and evolving efforts to address the threat.\n- **Legislative and policy synthesis on identity theft (2006)** — Through the same work, MacLeod provides an integrated discussion of the “past history of legislation” and “recent updates,” contributing a structured perspective on how law and policy have adapted to emerging identity-related risks.\n- **Victim protections, rights, and societal impacts of identity theft (2006)** — Also developed in the 2006 publication, MacLeod’s analysis highlights protections and rights available to individuals and underscores the broader societal stakes, reinforcing <u>identity theft as a public-interest and governance challenge</u> rather than solely a consumer finance issue.\n\n---\n\n## Academic Background\nAffiliated with the Georgia Institute of Technology, MacLeod’s scholarly profile reflects engagement with research traditions common to interdisciplinary technology-and-society inquiry, including policy analysis and the study of security-related social harms. The publication record available—anchored by the 2006 paper on identity theft—suggests an academic trajectory attentive to regulatory history, institutional responses, and public-facing implications of technological misuse. With a modest but focused citation footprint (7 total citations), MacLeod’s work has achieved recognized impact within its niche by advancing an argument for broader conceptualization of identity theft and by linking *legislative evolution* to practical questions of victim protection and societal risk."}, "ciQ3dn0AAAAJ": {"keywords": "bioinformatics, gene prediction, genome annotation, ab initio gene finding, hidden Markov models, self-training algorithms, prokaryotic genome annotation, eukaryotic genome annotation, metagenomics, RNA-Seq-guided gene modeling", "summary": "## Overview\nMark Borodovsky is a **Regents' Professor** at **Georgia Tech**, where he leads a research program at the intersection of *computational biology* and **bioinformatics** focused on <u>gene prediction</u> and <u>genome annotation</u>. His work is widely recognized for advancing **algorithmic and statistical methods** that enable accurate identification of protein-coding genes across diverse organisms and data types, from microbial isolates to complex eukaryotic genomes and metagenomes. Across decades of scholarship and community tool-building, Borodovsky has helped define modern *automated annotation* practice through the **GeneMark** family of methods and through integrative pipelines that translate high-throughput sequencing evidence into reliable gene models.\n\n---\n\n## Research Areas\nBorodovsky’s research centers on statistical modeling and machine-learning approaches for computational gene finding, with sustained emphasis on **ab initio** methods that can operate when curated training sets are unavailable. Early foundational contributions established Markov-model-based frameworks for recognizing coding regions (e.g., “GENMARK: parallel gene recognition for both DNA strands,” 1993; “Recognition of genes in DNA sequence with ambiguities,” 1993) and for interpreting “anonymous” bacterial genomes via robust parameter estimation (“How to interpret an anonymous bacterial genome: machine learning approach to gene identification,” 1998; “Heuristic approach to deriving models for gene finding,” 1999). In prokaryotic systems, he advanced self-training and improved translation-start identification (“GeneMarkS: a self-training method for prediction of gene starts in microbial genomes,” 2001) and scalable HMM-based gene finding (“GeneMark. hmm: new solutions for gene finding,” 1998), culminating in widely used community infrastructure for annotation (“GeneMark: web software for gene finding in prokaryotes, eukaryotes and viruses,” 2005) and contributions to production-grade pipelines (“NCBI prokaryotic genome annotation pipeline,” 2016). In eukaryotic genomics, his group has been central to unsupervised training for novel genomes (“Gene identification in novel eukaryotic genomes by self-training algorithm,” 2005; “Gene prediction in novel fungal genomes using an ab initio algorithm with unsupervised training,” 2008) and to evidence-driven annotation that integrates RNA-seq and protein homology (“BRAKER1,” 2016; “BRAKER2,” 2021; “GeneMark-EP+,” 2020; “TSEBRA,” 2021; “BRAKER3,” 2024; “GeneMark-ETP,” 2024). Complementing method development, Borodovsky has contributed to high-impact genome projects and comparative genomics—including landmark early microbial genomes (“Complete genome sequence of the methanogenic archaeon, Methanococcus jannaschii,” 1996; “The complete genome sequence of the gastric pathogen Helicobacter pylori,” 1997) and later eukaryotic references and pan-genomes (e.g., “The genome of woodland strawberry,” 2011; citrus domestication genomics, 2014)—where rigorous annotation is essential for biological interpretation.\n\n---\n\n## Notable Works\n- **Production-scale microbial annotation infrastructure:** Co-development and dissemination of methods that underpin large-scale microbial gene calling and standardized annotation, most prominently through the widely cited “**NCBI prokaryotic genome annotation pipeline**” (2016), reflecting the translation of gene-finding research into community-wide operational practice.\n- **GeneMark and self-training gene prediction paradigms:** Seminal contributions to statistical gene finding via the GeneMark line, including “**GENMARK: parallel gene recognition for both DNA strands**” (1993), “**GeneMark. hmm: new solutions for gene finding**” (1998), and “**GeneMarkS: a self-training method for prediction of gene starts in microbial genomes**” (2001), which collectively advanced HMM/Markov modeling, automated parameter learning, and accurate start-site prediction in prokaryotes.\n- **Automated eukaryotic genome annotation with integrated evidence:** Leadership in next-generation eukaryotic annotation workflows that combine *ab initio* learning with transcriptomic/protein evidence, notably “**BRAKER1**” (2016), “**BRAKER2**” (2021), and “**BRAKER3**” (2024), alongside enabling components such as “**GeneMark-EP+**” (2020) and “**TSEBRA**” (2021), which improved robustness and isoform/transcript selection in complex genomes.\n\n---\n\n## Academic Background\nAs a long-standing faculty member at **Georgia Tech** and a holder of the title **Regents' Professor**, Borodovsky’s academic career reflects sustained leadership in computational genomics and broad influence across both methodology and biological discovery. His publication trajectory—from foundational theoretical work on sequence “linguistics” and Markov-chain properties (e.g., 1989–1992 studies on word statistics in genetic texts) through early genome-era breakthroughs in microbial genomics (notably the mid-1990s landmark genome papers on *Methanococcus jannaschii* and *Helicobacter pylori*) to modern, evidence-integrating eukaryotic annotation systems (BRAKER and GeneMark-ETP/EP+)—suggests deep interdisciplinary training spanning mathematics/statistics, computer science, and molecular biology. His extensive citation impact (over **46,000** citations) and repeated involvement in community-defining resources (GeneMark web software; NCBI’s prokaryotic annotation pipeline; BRAKER workflows) indicate international recognition, sustained research funding and collaboration networks, and strong affiliations with the global genome sequencing and annotation community through multi-institutional consortium-style genome projects in microbes, plants, algae, fungi, and metagenomic contexts."}, "d2HP6SMAAAAJ": {"keywords": "Robot Learning, Learning from Demonstration (Imitation Learning), Multi-Robot Systems, Heterogeneous Multi-Robot Task Allocation and Scheduling, Human-Robot Collaboration, Human Intention Inference, Stable Dynamical Systems and Movement Primitives, Contraction Analysis, Dexterous Manipulation, Koopman Operator Learning for Control and Manipulation", "summary": "## Overview\nHarish Ravichandar is an Assistant Professor at the Georgia Institute of Technology and leads research through the STAR Lab (https://star-lab.cc.gatech.edu/). His scholarship centers on **robot learning** and **multi-robot autonomy**, with a sustained emphasis on *learning from demonstration* and *human-centered autonomy* for real-world deployment. Across his work, he develops principled methods that connect **learning**, **planning**, and **control** to enable <u>robust skill acquisition</u>, <u>safe human–robot collaboration</u>, and <u>scalable coordination in heterogeneous robot teams</u>, including emerging directions in *dexterous manipulation* and *interpretable dynamical representations*.\n\n---\n\n## Research Areas\nRavichandar’s research spans three tightly connected areas. First, he has made foundational contributions to <u>Learning from Demonstration (LfD)</u>, synthesizing the state of the art and outlining open challenges in “Recent Advances in Robot Learning from Demonstration” (2020), while also proposing concrete algorithmic tools for stable skill learning. This includes a line of work on **contracting dynamical system primitives (CDSP)**—e.g., “Learning Partially Contracting Dynamical Systems from Demonstrations” (2017) and “Learning position and orientation dynamics from demonstrations via contraction analysis” (2019)—that leverages contraction theory, Gaussian mixture models, and pose representations (including orientation) to learn motion policies with stability properties important for reliable execution. Second, he advances <u>human–robot collaboration</u> through **human intention inference** and anticipatory control, as seen in “Human intention inference using expectation-maximization algorithm with online model learning” (2016), “Gaze and motion information fusion for human intention inference” (2018), and “Human-in-the-loop robot control for human-robot collaboration” (2020), where intention estimation is integrated with safe tracking and trajectory optimization for close-proximity interaction. Third, he develops methods for <u>heterogeneous multi-robot systems</u> that unify task allocation, planning, and scheduling at scale, including “STRATA: A Unified Framework for Task Assignments in Large Teams of Heterogeneous Agents” (2020) and “GRSTAPS: Graphically recursive simultaneous task allocation, planning, and scheduling” (2022), alongside resilience-aware extensions such as “D-ITAGS” (2023). More recently, his manipulation research explores <u>structured representations</u> (e.g., **Koopman operator** formulations) for dexterous skill learning and improved transparency, including “On the Utility of Koopman Operator Theory in Learning Dexterous Manipulation Skills” (2023) and follow-on work that seeks visualizable and rollout-capable feature/operator models for manipulation.\n\n---\n\n## Notable Works\n- **Field-shaping synthesis of LfD**: Authored “Recent Advances in Robot Learning from Demonstration” (2020), a highly cited survey that consolidates core paradigms, representations, and evaluation considerations for imitation-based robot skill acquisition, helping define research directions for <u>scalable and reliable robot learning</u>.\n- **Stable motion learning via contraction theory (CDSP)**: Developed contracting-dynamics approaches for learning motion skills from demonstrations—most notably “Learning Partially Contracting Dynamical Systems from Demonstrations” (2017) and the pose-generalized “Learning position and orientation dynamics from demonstrations via contraction analysis” (2019)—linking learning with formal stability structure for dependable execution.\n- **Unified coordination for heterogeneous robot teams**: Introduced integrative frameworks for large-team autonomy, including “STRATA: A Unified Framework for Task Assignments in Large Teams of Heterogeneous Agents” (2020) and “GRSTAPS: Graphically recursive simultaneous task allocation, planning, and scheduling” (2022), with later resilience-focused extensions such as “D-ITAGS” (2023) that address failures and dynamic changes in complex missions.\n\n---\n\n## Academic Background\nRavichandar’s publication trajectory indicates a sustained, research-intensive career spanning foundational work in **human intention inference** (2015–2016), theory-grounded **learning from demonstration** and stable dynamical systems (2015–2019), and subsequent expansion into **human-in-the-loop collaboration** and **heterogeneous multi-robot coordination** (2020–present), culminating in newer efforts on **dexterous manipulation** and structured learning (e.g., Koopman-based methods) for practical deployment. As an Assistant Professor at Georgia Tech, he is affiliated with a premier robotics ecosystem and leads a lab program that bridges *algorithmic rigor* with *real-world interaction constraints* (safety, uncertainty, and team-scale complexity). With **2,152 citations** and multiple widely used contributions—particularly his LfD survey and CDSP/coordination frameworks—his record reflects significant scholarly impact and sustained engagement with the robotics research community through cross-cutting work at the intersection of learning, planning, and control."}, "dJy9rgYAAAAJ": {"keywords": "networking, internet of things, mobile applications, software-defined networking (SDN), software-defined internet exchange (SDX), BGP interdomain routing, network security, multicast web delivery, delay-tolerant networking (DTN), transportation and coastal resilience", "summary": "## Overview\nRussell Clark is a Senior Research Scientist at the Georgia Institute of Technology, where he contributes to research and prototyping efforts at the intersection of **computer networking** and *operationally deployable systems*. His work centers on translating foundational networking ideas into practice, with particular emphasis on <u>software-defined networking (SDN)</u>, <u>inter-domain routing and exchange points</u>, and security mechanisms that can be incrementally adopted in real networks. Across a publication record spanning multiple decades and application domains, Clark’s research consistently targets *scalability*, **programmability**, and <u>resilience</u>—from early work on multicast-based Web delivery and multiprotocol interoperability to influential contributions on SDN-enabled Internet exchange design and network security.\n\n---\n\n## Research Areas\nClark’s research areas span three tightly connected themes: (1) **programmable network architectures**, (2) **network and service security**, and (3) **applied systems for mobile/IoT and challenged environments**, with additional recent engagement in resilience-oriented socio-technical systems. In programmable networking, he is closely associated with the SDN-driven rethinking of interconnection and traffic steering, most prominently in “**Sdx: A software defined internet exchange**” (2014), which argues that BGP’s destination-prefix constraints limit wide-area policy expressiveness and that SDN can enable finer-grained control at exchange points. This line of work is complemented by integrative and community-shaping synthesis in “**Advancing software-defined networks: A survey**” (2017), framing SDN’s centralized control model, abstractions, and evolving research directions. In network security, Clark has repeatedly leveraged SDN’s visibility and enforcement capabilities to mitigate common enterprise threats, including ARP spoofing (“**Leveraging SDN for ARP security**,” 2016), rogue access points (“**Leveraging SDN and WebRTC for rogue access point security**,” 2017), and misconfiguration/attack surfaces around dynamic host configuration (“**Leveraging SDN to improve the security of DHCP**,” 2016), alongside policy evolution concerns (“**A security policy transition framework for software-defined networks**,” 2017). His broader systems portfolio includes work on **mobile applications** and challenged networks (e.g., “**D-book: a mobile social networking application for delay tolerant networks**,” 2008), earlier contributions to scalable content delivery via multicast (“**Providing scalable web service using multicast delivery**,” 1995), and foundational interoperability studies (e.g., RFC 1683 “**Multiprotocol Interoperability in IPng**,” 1994). Taken together, these topics reflect an enduring emphasis on <u>deployable mechanisms</u> that improve network functionality under real constraints—policy, scale, heterogeneity, and adversarial behavior.\n\n---\n\n## Notable Works\n- **Software-defined Internet exchange design and policy expressiveness:** Clark’s most visible contribution is tied to “**Sdx: A software defined internet exchange**” (2014), a highly cited work that articulates how SDN can overcome BGP’s coarse-grained forwarding constraints to enable richer, more flexible traffic steering and policy at Internet exchange points, helping define the <u>SDX</u> research agenda.\n- **Field-defining synthesis of SDN concepts and research directions:** In “**Advancing software-defined networks: A survey**” (2017), he coalesces the state of the art in SDN—its architectural principles, controller-centric abstractions, and operational implications—providing a widely used reference that connects systems practice with emerging research challenges.\n- **Practical SDN-based defenses for enterprise LAN threats:** Through applied security studies such as “**Leveraging SDN for ARP security**” (2016), Clark demonstrates how SDN’s centralized control and fine-grained enforcement can be used to detect and mitigate insider-capable attacks (e.g., ARP spoofing) in campus and enterprise settings, exemplifying his focus on <u>operational security</u>.\n\n---\n\n## Academic Background\nClark’s publication trajectory suggests a long-standing academic and research career anchored in networking systems and protocols, with a distinctive breadth that spans early protocol and interoperability work in the 1990s (e.g., OSI/application-layer formalization, multiprotocol architectures, and the informational RFC “**Multiprotocol Interoperability in IPng**,” 1994) and scalable service delivery research (“**Providing scalable web service using multicast delivery**,” 1995), followed by sustained engagement with mobile and multimedia networking/security (e.g., “**Security issues with the IP multimedia subsystem (IMS)**,” 2007; and DTN-enabled mobile social applications in 2008). His later work aligns with the rise of **software-defined networking**, where he contributed both conceptual consolidation (“**Advancing software-defined networks: A survey**,” 2017) and influential systems direction-setting (“**Sdx: A software defined internet exchange**,” 2014), as well as practical SDN security mechanisms (2016–2017). As a Senior Research Scientist at **Georgia Institute of Technology**, he is affiliated with an institution recognized for foundational and applied networking research, and his citation impact (1691 total citations) reflects sustained scholarly influence across multiple waves of networking innovation. The arc of his work indicates a researcher who bridges *standards-aware protocol thinking*, experimental systems building, and security-minded network operations—consistent with long-term collaboration in academic, campus, and research-network environments."}, "dM-nHdMAAAAJ": {"keywords": "computational fluid dynamics, scientific computing, cavitation bubble dynamics, compressible multiphase flow, interface capturing methods, phase-averaged bubbly flow models, quadrature-based moment methods, GPU-accelerated high-performance computing, physics-informed neural networks, quantum algorithms for fluid simulation", "summary": "## Overview\nSpencer H. Bryngelson is a Georgia Tech researcher and a member of the **Computational Physics Group** (https://comp-physics.group/), where he develops *high-fidelity* methods and software for **computational fluid dynamics** and **scientific computing**. His work centers on <u>multiphase and multicomponent compressible flows</u>, with particular emphasis on the numerical physics of cavitation and bubble dynamics, the design of **high-order interface-capturing** schemes, and the translation of these models into *portable, scalable* implementations for modern HPC architectures. Across a publication record exceeding **764 citations**, Bryngelson has combined rigorous model assessment with open-source tooling, advancing both the *predictive accuracy* and the *computational practicality* of next-generation CFD.\n\n---\n\n## Research Areas\nBryngelson’s research spans the modeling, discretization, and high-performance implementation of challenging PDE systems arising in compressible multiphase flow. A major thread is <u>cavitation and bubble-collapse physics</u>, including systematic evaluation of multicomponent formulations and interface-capturing strategies for canonical yet numerically delicate problems (e.g., the collapsing spherical bubble in **“An assessment of multicomponent flow models and interface capturing schemes for spherical bubble dynamics”** (2020)). He has also studied bubble–boundary interactions relevant to cavitation erosion, such as the jetting and pressure-loading mechanisms induced by collapse near geometric imperfections in **“Near-surface dynamics of a gas bubble collapsing above a crevice”** (2020). Complementing resolved-interface simulations, he has contributed to *multiscale phase-averaged* and population-balance approaches for bubbly, cavitating flows, including comparisons of modeling paradigms (**“A quantitative comparison of phase-averaged models for bubbly, cavitating flows”** (2019)) and the development of moment and quadrature-based closures for polydisperse bubble populations (e.g., **“Hybrid quadrature moment method for accurate and stable representation of non-Gaussian processes applied to bubble dynamics”** (2022) and **“Conditional moment methods for polydisperse cavitating flows”** (2023)). \n\nA second major axis is <u>research software and performance engineering</u>: Bryngelson has helped shape open, reproducible CFD infrastructure through **MFC**, a high-order compressible multiphase solver introduced in **“MFC: An open-source high-order multi-component, multi-phase, and multi-scale compressible flow solver”** (2021), and extended through GPU-acceleration and portability efforts (e.g., **“Scalable GPU accelerated simulation of multiphase compressible flow”** (2022) and **“Method for portable, scalable, and performant GPU-accelerated simulation of multiphase compressible flow”** (2024)). He also engages emerging computational paradigms, including machine-learning-enhanced numerical methods (e.g., **“Competitive Physics Informed Networks”** (2023) and **“Rational-WENO: A lightweight, physically-consistent three-point weighted essentially non-oscillatory scheme”** (2024)) and quantum algorithms for fluid simulation, such as **“Fully quantum algorithm for mesoscale fluid simulations with application to partial differential equations”** (2024) and hybrid quantum–classical PDE solvers (**“Incompressible Navier–Stokes solve on noisy quantum hardware via a hybrid quantum–classical scheme”** (2025)).\n\n---\n\n## Notable Works\n- **Model validation and numerical assessment for bubble-collapse CFD:** In **“An assessment of multicomponent flow models and interface capturing schemes for spherical bubble dynamics”** (2020), Bryngelson and collaborators provided a widely cited, careful comparison of compressible multicomponent formulations and interface-capturing strategies for spherical bubble collapse—clarifying how modeling choices and numerics interact in a benchmark problem central to cavitation research.\n\n- **Cavitation near boundaries and erosion-relevant dynamics:** **“Near-surface dynamics of a gas bubble collapsing above a crevice”** (2020) advanced understanding of bubble collapse in the presence of rigid, notched surfaces, connecting *geometric defects* (crevices as nucleation sites) to collapse-induced loading pathways that underpin cavitation damage.\n\n- **Open-source multiphase CFD infrastructure at high order:** Through **“MFC: An open-source high-order multi-component, multi-phase, and multi-scale compressible flow solver”** (2021), Bryngelson helped establish **MFC** as a community-facing platform for simulating multicomponent/multiphase compressible flows (e.g., shock–bubble interaction, atomization, bubble dynamics), bridging <u>thermodynamically consistent models</u> with high-order numerics and scalable implementation.\n\n---\n\n## Academic Background\nAt Georgia Tech, Bryngelson has built an academic profile that integrates **computational physics**, **CFD**, and **HPC-oriented scientific software**, evidenced by a sustained sequence of contributions spanning fundamental numerical modeling (multicomponent and diffused-interface formulations), multiscale bubbly-flow closures (moment and quadrature-based methods), and performance-portable implementations targeting GPUs and emerging architectures. His publication trajectory also reflects broad interdisciplinary engagement: early and parallel work on biofluid and soft-matter-inspired problems (e.g., capsule and red-blood-cell train stability in **“Capsule-train stability”** (2016) and **“Global stability of flowing red blood cell trains”** (2018)) complements later emphases on cavitation, acoustics, and material inference (e.g., ensemble-based data assimilation in **“Characterizing viscoelastic materials via ensemble-based data assimilation of bubble collapse observations”** (2021)). With **764 total citations**, his impact is anchored in both *highly cited domain studies* (bubble dynamics near surfaces and in canonical collapse benchmarks) and *community infrastructure* (the MFC solver and associated acceleration/portability efforts), positioning him within Georgia Tech’s research ecosystem as a contributor to <u>reproducible, high-performance computational science</u> and to emerging directions at the interface of CFD, machine learning, and quantum computing."}, "dYtRH80AAAAJ": {"keywords": "Intersectionality, Black women and girls, computer science education, STEM education, digital divide, equity in technology access and use, computing identity, HBCUs, Black feminist thought and epistemology, racialized spaces and gendered racism in computing education", "summary": "## Overview\nTamara Pearson is **Senior Director of Research** at the **Constellations Center for Equity in Computing** within the <u>College of Computing</u> at the **Georgia Institute of Technology**, where she leads and supports scholarship aimed at advancing *equity-centered* computing pathways. Her work centers on <u>intersectionality</u> in **computer science (CS) education** and broader **STEM education**, with sustained attention to the experiences of **Black women and girls** and to the structural conditions that shape participation, identity, and opportunity. Across her career, Pearson has combined empirical studies of the <u>digital divide</u> with contemporary theorizing grounded in *Black feminist thought* and related critical traditions, positioning computing education as both a site of possibility and a domain requiring ethical, justice-oriented transformation.\n\n---\n\n## Research Areas\nPearson’s research spans three tightly connected areas: (1) the <u>digital divide</u> as an equity problem in schooling, (2) computing participation and identity formation, and (3) intersectional analyses of power in CS education research and practice. Her early, widely cited work articulated how technology inequities extend beyond hardware access to include *equity in use*, teacher decision-making, and standards implementation—arguments developed in studies such as “Falling Behind: A Technology Crisis Facing Minority Students.” (2001) and “Educators and technology standards: Influencing the digital divide” (2002), alongside practitioner-facing synthesis in “Bridging the digital divide: A building block for teachers” (2001). In more recent scholarship, Pearson has advanced <u>computing identity</u> and institutional context as central to broadening participation, including attention to historically Black colleges and universities in “Exploring the Digital Identity Divide: A Call for Attention to Computing Identity at HBCUs” (2021). Her newest contributions foreground *intersectionality* and <u>Black feminist epistemology</u> in computing and STEM education, examining the under-theorized experiences and leadership of Black women educators (“Unpacking the Unique Role of Black Women Computer Science Educators,” 2024), the ethical stakes of representation in the research community (“An Ethical Imperative: Increasing the Participation of Black Women in Computer Science Education Research,” 2024), and emergent conceptual tools such as <u>interruption</u> for understanding how repeated structural disruptions shape Black women’s STEM trajectories (“Understanding the Experiences of Black Women in STEM: A Framework for Interruption,” 2025; “Interruption as a Framework…,” 2025).\n\n---\n\n## Notable Works\n- **Reframing the digital divide through standards, teaching, and equity of use**: In “**Educators and technology standards: Influencing the digital divide**” (2002), Pearson documents how standards-based approaches and educator practices can either mitigate or reproduce inequities, emphasizing that <u>implementation</u> and classroom use—rather than access alone—drive meaningful opportunity.\n- **Diagnosing technology inequity for minority students as a systemic educational crisis**: “**Falling Behind: A Technology Crisis Facing Minority Students.**” (2001) synthesizes evidence on home and school computing disparities and argues for an equity lens that accounts for differential experiences, expectations, and supports shaping minoritized students’ technology learning.\n- **Centering Black women’s expertise and theorizing in CS education**: In “**Unpacking the Unique Role of Black Women Computer Science Educators**” (2024), Pearson applies a framework grounded in <u>Black feminist thought</u> to illuminate Black women CS teachers’ distinctive contributions, making visible forms of pedagogical and cultural labor often rendered invisible in mainstream CS education discourse.\n\n---\n\n## Academic Background\nBased at **Georgia Tech**, Pearson’s publication record indicates a long-standing scholarly engagement with educational equity and technology that began with turn-of-the-century work on instructional technology and teacher learning (e.g., “Students as Producers: Changing the Way we Teach and Learn,” 2000) and matured into influential analyses of the <u>digital divide</u> in K–12 contexts (2001–2003). Her later contributions reflect interdisciplinary reach across STEM education, teacher development, and learning sciences (including work on mathematics/statistics teacher knowledge and technology-supported learning in the 2010s), culminating in a contemporary research agenda that integrates *critical theory*, <u>intersectionality</u>, and computing education reform. Her leadership role at the **Constellations Center for Equity in Computing** aligns with professional service and field-building efforts visible in community-facing scholarship and convenings (e.g., RESPECT-related pieces and SIGCSE community communications), and her impact is reflected in a citation record of **394** total citations, anchored by foundational digital-divide publications that continue to inform equity-oriented technology and CS education research."}, "dio8IesAAAAJ": {"keywords": "Efficient AI algorithms, Deep learning accelerators, Green AI, Neural network pruning and lottery tickets, Model compression and quantization, Hardware-aware neural architecture search (HW-NAS), Edge AI and on-device training/inference, Graph neural network (GCN/GNN) acceleration, Vision Transformer (ViT) acceleration, Small language models and efficient LLM adaptation", "summary": "## Overview\nYingyan (Celine) Lin is an Associate Professor at the Georgia Institute of Technology, where she leads the **Efficient Intelligence Computing Lab (EIC Lab)**. Her scholarship centers on **efficient AI** across the algorithm–system–hardware stack, with an emphasis on *deployable* and *sustainable* machine learning. Bridging **deep learning algorithms**, **accelerator design**, and **hardware-aware automation**, Lin’s work advances <u>Green AI</u> by reducing the energy, latency, and memory costs of both *training* and *inference* while preserving model quality—particularly for edge/mobile platforms and emerging AI workloads.\n\n---\n\n## Research Areas\nLin’s research spans efficient model design, training-time optimization, and customized acceleration for modern deep learning. A major thread is <u>training efficiency</u> and sparsity discovery, exemplified by “**Drawing early-bird tickets: Towards more efficient training of deep networks**” (2020), which investigates early identification of performant sparse subnetworks to cut training cost. Complementing this, her work on *energy-aware training* explores algorithmic knobs such as precision scheduling (e.g., “**CPT: Efficient Deep Neural Network Training via Cyclic Precision**,” 2021) and broader energy-reduction strategies (e.g., “**E2-Train: Training State-of-the-art CNNs with Over 80% Energy Savings**,” 2019). Another core area is <u>model compression and adaptive inference</u> for resource-constrained deployment, including usage-driven compression and selection for mobile settings (“**On-demand deep model compression for mobile devices**,” 2018; and related frameworks such as AdaDeep), as well as controllable inference mechanisms (“**Dual dynamic inference**,” 2020) and hardware-inspired operators (“**ShiftAddNet**,” 2020). On the systems and architecture side, Lin has contributed to <u>hardware-aware neural architecture search</u> and benchmarking (“**HW-NAS-Bench**,” 2021), and to design automation for accelerators (“**AutoDNNchip**,” 2020; “**GPT4AIGChip**,” 2023), reflecting a sustained focus on making accelerator development more predictive and accessible. Her group also advances specialized acceleration for non-CNN modalities, notably graph learning (e.g., “**I-GCN**,” 2021; “**BNS-GCN**,” 2022; “**PipeGCN**,” 2022) and attention-heavy vision models through algorithm–accelerator co-design (e.g., “**ViTCoD**,” 2022; “**ViTALiTy**,” 2022; “**Castling-ViT**,” 2023), as well as on-device neural rendering for AR/VR (e.g., “**RT-NeRF**,” 2022; “**Instant-3D**,” 2023). More recently, her publications extend efficient computing principles to <u>agentic AI and small language models</u> (“**Small Language Models are the Future of Agentic AI**,” 2025; “**EDGE-LLM**,” 2024; “Hymba,” 2025), while also engaging broader questions of robustness, security, and next-generation cognitive systems (e.g., “Patch-Fool,” 2022; “Practical Attacks… by Memory Trojaning,” 2020; neuro-symbolic AI surveys and workload/architecture studies in 2024).\n\n---\n\n## Notable Works\n- **Early-bird sparsity for efficient training**: In “**Drawing early-bird tickets: Towards more efficient training of deep networks**” (2020), Lin helped advance the idea that high-quality sparse subnetworks can be identified *early* during training, enabling substantial reductions in training computation while maintaining competitive accuracy.\n- **Usage-driven mobile deployment via on-demand compression**: “**On-demand deep model compression for mobile devices: A usage-driven model selection framework**” (2018) introduced a practical framework that treats compression as a *deployment-time decision problem*, aligning model selection with real device constraints and usage patterns to improve real-world efficiency.\n- **Hardware-aware automation and benchmarking for efficient AI systems**: Through “**HW-NAS-Bench: Hardware-Aware Neural Architecture Search Benchmark**” (2021) and automation-oriented studies such as “**AutoDNNchip**” (2020) and “**GPT4AIGChip**” (2023), Lin has shaped methodologies for reproducible hardware-aware evaluation and accelerated the path from model design to efficient accelerator implementation.\n\n---\n\n## Academic Background\nAt Georgia Tech, Lin has built an internationally visible research program at the intersection of **machine learning**, **computer architecture**, and **EDA-style design automation**, reflected in broad, high-impact publication themes ranging from CNN compression and energy-efficient training to GNN/ViT acceleration and LLM-efficient adaptation. Her scholarly trajectory suggests sustained collaboration across hardware and ML communities—particularly in venues and problem settings that demand cross-layer thinking (e.g., co-optimizing algorithms with accelerators, and creating benchmarks and predictors for rapid design-space exploration). With **5,547 citations**, her work has achieved substantial uptake, indicating both foundational algorithmic influence (e.g., early-bird lottery ticket training efficiency) and practical systems impact (e.g., mobile compression frameworks and hardware-aware NAS resources). Through the EIC Lab, she maintains affiliations and collaborations consistent with a modern, interdisciplinary efficient-AI agenda—spanning edge/mobile intelligence, specialized accelerators (including PIM-oriented directions such as “TIMELY,” 2020), and emerging efficient paradigms for *agentic* and *small* language models—positioning her contributions squarely within the broader <u>Green AI</u> movement."}, "fFc3ezYAAAAJ": {"keywords": "Information Visualization, Visual Perception, Decision Making, Graphical Perception, Visual Data Communication, Cognitive Biases, Trust in Visualizations, Chart Interpretation, Visualization Evaluation, Visualization Design Guidelines", "summary": "## Overview\nCindy Xiong Bearfield is a researcher at the **Georgia Institute of Technology**, where she investigates how people *perceive*, *interpret*, and *trust* data displays in real decision contexts. Working at the intersection of **Information Visualization**, **Visual Perception**, and **Decision Making**, she studies visualization as a form of communication in which meaning is not simply “read off” a chart, but constructed through human cognition. Her work centers on <u>visual inference under ambiguity</u>—how design choices, prior beliefs, and contextual cues shape what viewers see as salient, what they remember, and what conclusions they draw.\n\n---\n\n## Research Areas\nBearfield’s research program develops an empirically grounded account of how visualization design interacts with human reasoning. A central theme is <u>miscommunication and interpretive divergence</u> in visual data communication: in *“The Curse of Knowledge in Visual Data Communication”* (2019), she shows how priming and expertise can lead different viewers to extract different patterns from the same dataset, creating systematic gaps between what designers intend and what audiences take away. Closely related is her work on <u>causal misinterpretation</u> and cognitive error: *“Illusion of causality in visualized data”* (2019) examines how viewers can incorrectly infer interventions from correlational displays, while *“Seeing what you believe or believing what you see? belief biases correlation estimation”* (2022) and *“Reasoning affordances with tables and bar charts”* (2022) demonstrate how prior beliefs and confirmation bias can distort judgments even when evidence is visually present. Methodologically, she contributes to evaluation and measurement in visualization, articulating multi-level understanding in *“How to evaluate data visualizations across different levels of understanding”* (2020) and mapping experimental approaches in *“A design space of vision science methods for visualization research”* (2020). Another major thread is <u>design guidance validated by behavioral evidence</u>, including empirical tests of practitioner heuristics in *“Declutter and focus”* (2021) and studies of how layout and grouping cues steer comparisons and conclusions (e.g., *“Visual arrangements of bar charts influence comparisons in viewer takeaways”* (2021) and *“What does the chart say? grouping cues guide viewer comparisons and conclusions in bar charts”* (2023)). Finally, she has advanced visualization’s engagement with <u>trust, transparency, and ethics</u>—from early work on trust components in map-based displays (*“Examining the Components of Trust in Map-Based Visualizations”*, 2019) to frameworks and measurement (e.g., *“Vistrust”*, 2023; *“How do we measure trust in visual data communication?”*, 2022), and to contemporary concerns at the boundary of visualization and AI, including how visual design shapes perceived bias in ML systems (*“My model is unfair, do people even care?”*, 2023) and how generative and multimodal models succeed or fail as proxies for human perception (e.g., *“How aligned are human chart takeaways and llm predictions?”*, 2024; *“Llms are not reliable human proxies to study affordances in data visualizations”*, 2025). Her publication record (with **1,341** citations) reflects sustained contributions to the science of visualization as a human-centered, experimentally testable discipline.\n\n---\n\n## Notable Works\n- **Characterizing miscommunication in visual data communication:** In *“The Curse of Knowledge in Visual Data Communication”* (2019), Bearfield demonstrates how viewers can be led to different “obvious” patterns in the same visualization, formalizing a mechanism for designer–audience mismatch grounded in cognitive priming and salience.\n- **Diagnosing reasoning errors from correlational displays:** In *“Illusion of causality in visualized data”* (2019), she empirically shows how visualized correlations can induce causal conclusions and policy-like prescriptions, clarifying a key pathway by which charts can mislead even without deception.\n- **Defining and operationalizing multi-level visualization understanding:** In *“How to evaluate data visualizations across different levels of understanding”* (2020), she advances evaluation beyond single-task accuracy by distinguishing levels from extracting facts to extrapolating context and judging conclusions, providing a framework for more valid assessment of communicative visualization.\n\n---\n\n## Academic Background\nBased on her publication trajectory and collaborations, Bearfield’s academic formation is rooted in the interdisciplinary core of modern visualization research, combining **perception/vision science**, **cognitive psychology**, and **human-centered computing** approaches to study how people read charts and maps. Her early work engages foundational perceptual questions (e.g., *“Perceiving graphs as ambiguous figures”*, 2018; *“Processing fluency improves trust in scatterplot visualizations”*, 2018), and subsequent papers show a progression toward comprehensive theories of interpretation, evaluation, and trust that connect laboratory-style perceptual measurement with ecologically realistic decision-making. Her affiliations center on the **Georgia Institute of Technology**, and her record suggests sustained engagement with the visualization research community through method and evaluation contributions (e.g., *“A design space of vision science methods for visualization research”*, 2020; *“Vishikers’ guide to evaluation”*, 2022) as well as newer work at the interface of visualization and **generative AI** (e.g., *“How aligned are human chart takeaways and llm predictions?”*, 2024; *“Attack-resilient image watermarking using stable diffusion”*, 2024). Collectively, these outputs indicate a researcher recognized for empirically rigorous, theory-linked contributions to <u>human interpretation of visualized data</u>, with impact evidenced by strong citation performance across multiple major themes in the field."}, "f_7ev68AAAAJ": {"keywords": "blockchain, Ethereum, smart contracts, secure multi-party computation (MPC), private computation, verifiable computation, auditability, garbled circuits, garbled state machines, combinatorial game theory", "summary": "## Overview\nAbrahim Ladha is a **Lecturer** at **Georgia Tech**, where he contributes to teaching and research at the intersection of *applied cryptography* and *decentralized systems*. His work centers on enabling <u>privacy-preserving computation</u> on **public blockchains** while maintaining *high availability*, *resilience*, and **auditability**—a theme most clearly articulated in his research on **multi-party computation (MPC)** and Ethereum-based execution models for mission-critical applications.\n\n---\n\n## Research Areas\nLadha’s research spans **secure computation**, **blockchain systems**, and *algorithmic/game-theoretic explorations* in discrete settings. A major thrust of his recent work investigates how to run sensitive computations on public blockchain infrastructure without exposing inputs or internal state, while still preserving verifiability and operational robustness. In “**Auditable, available and resilient private computation on the blockchain via MPC**” (2022), he examines system designs that combine <u>verifiable execution</u> with **availability** and **fault tolerance**, positioning MPC as a practical mechanism for private computation that can still meet the audit requirements of high-stakes internet applications. Closely related, “**The GABLE report: garbled autonomous bots leveraging Ethereum**” (2020) and the accompanying “**GABLE contract source code**” (2022) develop an approach for private execution of **garbled state machines** and **garbled circuits** using on-chain components, highlighting how smart contracts can orchestrate secure off-chain computation while providing on-chain guarantees. Earlier, his paper “**Exploring mod2 n-queens games**” (2015) reflects an additional line of interest in *combinatorial games* and discrete mathematics, analyzing turn-based placement rules governed by modulo-two attack constraints and exploring variations of the resulting game dynamics.\n\n---\n\n## Notable Works\n- **Auditable private computation on public blockchains via MPC** — In “*Auditable, available and resilient private computation on the blockchain via MPC*” (2022), Ladha advances a framework for <u>private, verifiable computation</u> on Ethereum-like platforms, emphasizing **auditability**, **availability**, and resilience for mission-critical applications.\n- **GABLE: Ethereum-orchestrated garbled computation** — Through “*The GABLE report: garbled autonomous bots leveraging Ethereum*” (2020), he contributes to the design of systems that use blockchain coordination to support **private execution** of **garbled state machines/circuits** while leveraging public infrastructure.\n- **Open implementation artifacts for secure blockchain computation** — With “*GABLE contract source code*” (2022), he provides concrete Solidity contract examples that illustrate the GABLE approach, strengthening reproducibility and enabling follow-on work in <u>privacy-preserving smart-contract-adjacent computation</u>.\n\n---\n\n## Academic Background\nBased on his publication trajectory, Ladha’s academic profile reflects a progression from early work in *discrete mathematics and game analysis* (e.g., the 2015 study of modulo-two n-queens gameplay) to a more sustained focus on **cryptography-informed distributed systems**, particularly **secure computation on blockchains** (2020–2022). His association with Ethereum-centered research artifacts (including released smart contract code) suggests active engagement with the applied security and systems community and a commitment to *reproducible research*. As a **Lecturer at Georgia Tech**, he is positioned within a leading computing environment that aligns with his emphasis on <u>secure, reliable computation</u> in adversarial and decentralized settings; his citation record (11 total citations across the listed works) indicates emerging impact concentrated in the private-computation-on-blockchain line of research."}, "gGB3lA4AAAAJ": {"keywords": "UWB, Wireless, Mobility, IoT, Sensing, Indoor localization, UWB ranging (ToF/TDoA), IMU sensor fusion, Cross-technology communication, Drone-enabled wireless networks", "summary": "## Overview\nAshutosh Dhekne is a researcher at the Georgia Institute of Technology, where he contributes to wireless systems research spanning **ultra-wideband (UWB)**, **IoT**, and **mobile sensing**. His work centers on building *practical, deployable* systems that turn radio signals and embedded sensors into robust capabilities for real-world environments—especially where infrastructure is limited or conditions are adversarial. Across his publications, Dhekne repeatedly advances <u>wireless localization</u>, <u>cross-technology communication</u>, and <u>sensor–radio fusion</u>, combining **wireless protocol design**, **signal interpretation**, and **systems engineering** to enable new forms of interaction, tracking, and identification.\n\n---\n\n## Research Areas\nDhekne’s research spans several tightly connected areas of wireless and sensing systems. A foundational thread is **cross-technology communication**, exemplified by *Esense: Communication through energy sensing* (2009) and its expanded formulation in *Esense: Energy sensing-based cross-technology communication* (2012), which demonstrate how devices with fundamentally different PHY layers can still communicate by interpreting *energy profiles*—including scenarios where nodes are outside decode range but inside carrier-sense range. Another major pillar is **UWB-enabled sensing and identification**, notably *LiquID: A Wireless Liquid IDentifier* (2018), which shows liquids can be distinguished by how UWB signals slow down and attenuate through different substances, surfacing discriminative signatures in phase and strength. He also develops **high-precision tracking and localization under mobility and uncertainty**, including indoor and wearable settings such as *{TrackIO}: Tracking first responders {Inside-Out}* (2019), pen-like instrument tracking via *ITrackU* (2021), and deployment-focused indoor localization in *PnPLoc: UWB Based Plug & Play Indoor Localization* (2022). Complementing these are contributions to **mobile and robotic networking** (e.g., *The case for robotic wireless networks* (2016) and *If wifi aps could move: A measurement study* (2018)), **aerial networking and resilience** (e.g., *Extending cell tower coverage through drones* (2017)), and **IoT systems in challenging domains**, including sports instrumentation (*Bringing IoT to Sports Analytics.* (2017)) and safety-oriented wearables (*6fit-a-part* (2020)).\n\n---\n\n## Notable Works\n- **Esense cross-technology communication via energy sensing** — In *“Esense: Communication through energy sensing”* (2009) and *“Esense: Energy sensing-based cross-technology communication”* (2012), Dhekne helps establish a widely cited approach for enabling communication between heterogeneous radios by decoding <u>energy-sensing signatures</u>, extending connectivity beyond conventional packet decoding constraints.\n- **UWB-based material sensing for liquid identification** — In *“LiquID: A Wireless Liquid IDentifier”* (2018), he demonstrates that <u>UWB propagation through liquids</u> can provide reliable identification using measurable changes in attenuation and signal slow-down, framing wireless radios as practical sensing instruments rather than only communication devices.\n- **Embedded IoT sensing for sports analytics under extreme motion** — In *“Bringing IoT to Sports Analytics.”* (2017), he advances an end-to-end IoT system for cricket that estimates a ball’s <u>3D trajectory and spin</u> using low-cost embedded sensors and radios, addressing domain-specific constraints that break standard localization and motion-tracking pipelines.\n\n---\n\n## Academic Background\nBased on his long-running publication record and sustained impact (with approximately **1,197 citations**), Dhekne’s academic trajectory reflects deep engagement with systems-oriented wireless research across multiple eras of the field—from early work on **mesh networking and MAC design** (e.g., *Implementation and evaluation of a tdma mac for wifi-based rural mesh networks*, 2009) to modern **UWB localization, sensing, and security** (e.g., *PnPLoc*, 2022; *UnSpoof*, 2023; and later UWB-centric platforms and methods). His affiliation with the Georgia Institute of Technology situates his work within a leading ecosystem for networking, mobile computing, and ubiquitous sensing research, and his publication themes suggest active collaboration across wireless networking, embedded sensing, and human-centered IoT application domains (e.g., first responder tracking, sports analytics, and in-home activity technologies). The breadth of venues and topics—spanning cross-PHY communication, UWB sensing, mobility/robotic infrastructure, and applied IoT—indicates a career shaped by both foundational wireless systems contributions and *application-driven* research that translates radio and sensor primitives into deployable capabilities."}, "h0wla9sAAAAJ": {"keywords": "Human Computer Interaction, Digital Fabrication, Design, Craft, Paper Mechatronics, Interactive Papercraft, Printed Electronics, Shape-Changing Interfaces, Passive Haptics, Sustainable and Transient Electronics", "summary": "## Overview\nHyunJoo Oh is an **Assistant Professor** in **Industrial Design & Interactive Computing** at **Georgia Tech**, where she leads the *CodeCraft* group (<https://www.codecraft.group/>). Her research advances **Human–Computer Interaction (HCI)** through *research-through-design* and fabrication-centered inquiry, developing <u>craft-informed interactive materials</u> and <u>paper-based mechatronic systems</u> that make computation physically expressive, learnable, and environmentally accountable. Across her work on paper electronics, kirigami/kirigami-inspired sensing, and accessible toolchains for makers and learners, Oh foregrounds **material agency**, **hands-on interaction**, and *approachable fabrication* as foundations for new interactive artifacts and pedagogies.\n\n---\n\n## Research Areas\nOh’s scholarship sits at the intersection of **digital fabrication**, **interactive materials**, and **craft-based design methods**, with a sustained focus on paper as a computational substrate. A central thread is <u>paper electronics and paper mechatronics</u>, exemplified by **PEP (3D Printed Electronic Papercrafts)** (2018), which integrates actuation, sensing, and display into sculpted paper devices, and by broader syntheses such as **“Paper mechatronics: present and future”** (2018) that articulate paper as a serious medium for computational construction. She also advances <u>kirigami- and crease-enabled sensing</u> through material-driven investigations like **“Sensing kirigami”** (2019), which probes the electrical, haptic, and visual affordances of carbon-coated paper shaped into 3D forms, and **SPIN (Self-powered Paper INterfaces)** (2020), which bridges folding creases with triboelectric nanogenerators to enable self-powered interaction. Complementing these interaction techniques are fabrication innovations that lower barriers to prototyping, such as **“Silver tape”** (2020), which peel-and-transfer inkjet-printed circuits onto diverse substrates. More recently, Oh has expanded into <u>sustainable and transient electronics</u>—interrogating the assumed permanence of devices in **“Functional destruction”** (2023) and related work on the **Transient Internet of Things** (2023)—and into <u>passive haptics and compliant mechanisms</u> for tangible interfaces (e.g., **Shape-Haptics** and **FlexHaptics**, 2022). Running in parallel is a robust education-oriented agenda in tangible and inclusive computing, including toolkits and learning systems such as **FoldMecha** (2017), **Cube-in** (2015), and later paper-based co-design resources (e.g., **Making Computing Visible & Tangible**, 2023), reflecting a commitment to making computational making *legible, participatory, and culturally responsive*.\n\n---\n\n## Notable Works\n- **Paper electronics as a craft-forward fabrication paradigm:** In **“PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-Based Electronic Devices”** (2018), Oh and collaborators introduced an integrated technique set for embedding interactive electronic functions—*including actuation, sensing, display,* and more—directly into papercraft through sculpting-oriented workflows, helping establish paper as a viable platform for expressive interactive devices.\n- **Kirigami-enabled sensing and tangible affordances in paper substrates:** Through **“Sensing kirigami”** (2019), Oh advanced a material-driven account of how carbon-coated paper and kirigami structures can be engineered for interaction, emphasizing how *geometry and material treatment* jointly create electrical and tactile behaviors that support new forms of tangible input.\n- **Sustainable interaction through designed impermanence:** In **“Functional destruction: Utilizing sustainable materials’ physical transiency for electronics applications”** (2023), Oh reframed robustness as a design choice rather than a default, arguing for <u>physical transiency</u> as a pathway to reduce environmental costs and to align device lifetimes with actual use contexts—an agenda further echoed in work on the **Transient Internet of Things** (2023).\n\n---\n\n## Academic Background\nOh’s publication trajectory indicates a long-standing interdisciplinary formation spanning **HCI**, **design research**, and **computational fabrication**, with early contributions to interactive installations and culturally situated learning experiences (e.g., **“Ilha Musical”**, 2012; **“The memory of a tree”**, 2012) preceding a sustained program in tangible learning systems and maker-centered pedagogy (**The Digital Dream Lab**, 2013; **Cube-in**, 2015). Her later and most-cited work consolidates an identifiable scholarly signature around <u>paper mechatronics</u> and <u>fabrication techniques for interactive artifacts</u>, including the development of design tools and educational infrastructures such as **FoldMecha** (2015–2017) and broader field-shaping perspectives (e.g., **“Paper mechatronics: present and future”**, 2018). The presence of a dissertation-length contribution—**“Computational Design Tools and Techniques for Paper Mechatronics”** (2018)—suggests advanced doctoral training focused on computational design tools for craft-based mechatronic construction, followed by an expansion into novel transfer methods (**Silver tape**, 2020), self-powered paper interfaces (**SPIN**, 2020), passive haptic mechanisms (**Shape-Haptics** / **FlexHaptics**, 2022), and sustainability-centered electronics (**Functional destruction**, 2023; **Recy-ctronics**, 2024). Now at Georgia Tech, Oh’s role and citation impact (676 total citations) reflect an established research profile with affiliations that bridge **industrial design**, **interactive computing**, and the broader **HCI/digital fabrication** community, alongside an ongoing commitment to *inclusive learning and making* through toolkits, participatory methods, and accessible material practices."}, "hRggMmIAAAAJ": {"keywords": "Algorithms, Randomness, High-dimensional Geometry, Optimization, Foundations of Data Science, Spectral Methods, Randomized Linear Algebra, Dimensionality Reduction (Random Projections), Markov Chain Monte Carlo Sampling (Logconcave Distributions), Fairness in Machine Learning (Fair Clustering/PCA)", "summary": "## Overview\nSantosh S. Vempala is a faculty researcher at **Georgia Tech** (School of Computer Science), where he leads work at the intersection of **theoretical computer science** and *data-driven computation*. His scholarship centers on **algorithms and randomness**, with a sustained emphasis on <u>high-dimensional geometry</u> as a unifying lens for **optimization**, **sampling**, and the **foundations of data science**. Across a publication record exceeding **26,000 citations**, Vempala has helped shape modern understanding of how *spectral structure* and *randomized methods* yield provably efficient procedures for learning, clustering, and computation in high dimensions.\n\n---\n\n## Research Areas\nVempala’s research spans core algorithmic theory and its implications for modern data analysis. A major thread is **spectral and randomized linear-algebraic methods** for large-scale data, including rigorous analyses of latent semantic indexing in “*Latent semantic indexing: A probabilistic analysis*” (1998) and randomized techniques for matrix approximation such as “*Fast Monte-Carlo algorithms for finding low-rank approximations*” (2004) and “*Matrix approximation and projective clustering via volume sampling*” (2006). A second pillar is **clustering theory**, where he developed principled objectives and guarantees for spectral and graph-based clustering—most prominently in “*On clusterings: Good, bad and spectral*” (2004) and related work on clustering large graphs via SVD. A third foundational theme is <u>geometric probability and logconcavity</u>: Vempala has advanced the algorithmic theory of **random walks**, **sampling**, and **volume computation** in convex bodies, including “*Simulated annealing in convex bodies and an O*(n^4) volume algorithm*” (2006), “*Hit-and-run from a corner*” (2004), and “*The geometry of logconcave functions and sampling algorithms*” (2007), connecting isoperimetry, mixing, and efficient computation. In more recent work, he has contributed to **robust statistics** (“*Agnostic estimation of mean and covariance*,” 2016), **computational lower bounds** in average-case settings (“*Statistical algorithms and a lower bound for detecting planted cliques*,” 2017), and the theory of **sampling/optimization dynamics** (“*Rapid Convergence of the Unadjusted Langevin Algorithm: Isoperimetry*,” 2023). Complementing these themes are contributions to **online optimization** (“*Efficient algorithms for on-line optimization*,” 2005), **random projection** (“*The random projection method*,” 2005), and emerging questions in **responsible and reliable ML**, including fairness in representation and clustering (“*The price of fair PCA: One extra dimension*,” 2018; “*Socially fair k-means clustering*,” 2021) and theoretical limits behind model miscalibration and hallucination (“*Calibrated language models must hallucinate*,” 2024).\n\n---\n\n## Notable Works\n- **Probabilistic foundations for spectral text methods:** In “*Latent semantic indexing: A probabilistic analysis*” (1998), Vempala provided a rigorous explanation of when and why **spectral structure** in term–document matrices enables LSI to recover underlying semantic signals, helping formalize a widely used empirical technique within <u>random matrix</u> and algorithmic analysis.\n- **A theory of clustering quality and spectral guarantees:** “*On clusterings: Good, bad and spectral*” (2004) introduced a **bicriteria** framework for evaluating clustering quality and analyzed algorithms with provable performance, strengthening the theoretical basis for *spectral clustering* and large-scale partitioning beyond purely heuristic objectives.\n- **Geometric sampling and counting in high dimensions:** Through works such as “*Simulated annealing in convex bodies and an O*(n^4) volume algorithm*” (2006) and “*Hit-and-run from a corner*” (2004), Vempala advanced the algorithmic toolkit for <u>random-walk-based sampling</u> and volume computation, linking **isoperimetry**, **mixing time**, and efficient algorithms for convex geometry.\n\n---\n\n## Academic Background\nBased on his long-standing appointment at **Georgia Tech** and a publication trajectory spanning approximation algorithms in the mid-1990s (e.g., work on k-MST and connectivity augmentation) through foundational advances in randomized algorithms, convex geometry, and modern data science, Vempala’s academic profile reflects a career rooted in **theory of computation** with broad interdisciplinary reach. His collaborations across machine learning, optimization, statistics, and applied domains (including notable participation in large community-driven scientific software efforts such as “*Creation and analysis of biochemical constraint-based models using the COBRA Toolbox v. 3.0*,” 2019) indicate sustained affiliation with both core algorithms communities and cross-cutting data-science initiatives. The scale and breadth of his citation impact (over **26,000 citations**) and repeated influence on canonical topics—**random projection**, **online optimization**, **spectral methods**, and <u>logconcave sampling</u>—are consistent with a researcher recognized for foundational contributions and for building bridges between rigorous algorithmic theory and the methodological needs of contemporary data analysis."}, "hvEtgbYAAAAJ": {"keywords": "robotics, computing education, artificial intelligence, bio-inspired engineering, geotechnical engineering, granular materials, discrete element method (DEM), porous media flow, earthworm-inspired robotic locomotion, AI-assisted instruction and intelligent tutoring systems", "summary": "## Overview\nRodrigo Borela is a **Lecturer** at the **Georgia Institute of Technology**, where he operates at the intersection of *engineering systems* and *learning systems*, connecting foundational work in geomechanics and bio-inspired robotics with emerging efforts in **computing education** and **artificial intelligence**. His scholarship centers on <u>bio-inspired interaction with granular media</u>—how organisms and robots move through soil—and on <u>scalable, equitable instruction</u> in high-enrollment computing courses, including the design of *AI-augmented* supports for students and instructional teams.\n\n---\n\n## Research Areas\nBorela’s research spans three tightly linked areas. First, in **bio-inspired geotechnical engineering**, he synthesizes organism–soil strategies into engineering principles, mapping biological functions to geotechnical needs and outlining research frontiers in “**Bio-inspired geotechnical engineering: principles, current work, opportunities and challenges**” (2022). Second, he contributes to **granular microstructure and pore-scale characterization**, using computational geometry and image-based methods to connect microstructure to transport and constitutive behavior, as demonstrated in “**Pore Size Distribution in Granular Material Microstructure**” (2017), which leverages the Euclidean Distance Transform to quantify effective pore sizes in granular assemblies. Third, he advances **soil–robot interaction and subsurface locomotion**, experimentally interrogating how sand deforms and supports propulsion in “**Earthworm-inspired robotic locomotion in sand: an experimental study with X-ray tomography**” (2021), a line of work that informs both robotic probe design and geomechanical interpretation. In parallel, his computing-education portfolio emphasizes **instructional labor, equity, and AI-enabled pedagogy**—from understanding the *humanistic* and relational dimensions of CS teaching assistants in “**Exploring the Humanistic Role of Computer Science Teaching Assistants across Diverse Institutions**” (2025), to operationalizing fairness in assessment via “**Creating Equitable Grading Practices with Rubrics: A Teaching Assistant Training Activity**” (2023), and extending curriculum conversations to <u>sustainability in computing</u> in “**Beyond Buzzwords: Making Sustainability a Pillar of the Computing Curriculum**” (2025). Recent work further explores <u>LLM-mediated instructional support</u> (e.g., real-time misconception detection and the comparative effects of targeted versus Socratic AI hints) and experiential learning approaches that use **robotics projects** to strengthen CS1 learning outcomes.\n\n---\n\n## Notable Works\n- **Conceptual synthesis of bio-inspired geotechnics:** In “*Bio-inspired geotechnical engineering: principles, current work, opportunities and challenges*” (2022), Borela articulates a unifying framework that translates biological soil-interaction strategies into geotechnical design opportunities, helping define an agenda for <u>bio-inspiration as an engineering methodology</u> in subsurface systems.\n- **Quantitative pore-scale characterization of granular media:** “*Pore Size Distribution in Granular Material Microstructure*” (2017) advances computational approaches for extracting effective pore-size measures from granular microstructures, strengthening links between micro-geometry, flow through porous media, and macroscale constitutive modeling.\n- **Experimental soil–robot interaction for subsurface mobility:** In “*Earthworm-inspired robotic locomotion in sand: an experimental study with X-ray tomography*” (2021), Borela and collaborators use imaging-driven experimentation to reveal how sand responds to annelid-inspired actuation, informing the design of self-motile probes and the geomechanics of locomotion in granular substrates.\n\n---\n\n## Academic Background\nBorela’s publication trajectory indicates an academic formation grounded in **geotechnical engineering and granular mechanics**, with early contributions on stochastic and discrete-element modeling of subsidence and geomaterial processes (2016) and subsequent work on pore-scale microstructure characterization (2017). His later research expands into **bio-inspired robotics in soil** (2019–2021) and integrative geotechnical synthesis (2022), alongside applied studies of ground failure and subsidence (e.g., the long-term assessment of land subsidence at Bicycle Basin, 2019). Since joining **Georgia Tech** as a Lecturer, he has developed a substantial profile in **computing education research**, particularly around <u>teaching assistant development</u>, equitable assessment practices, and the design of **AI tools** for large courses beyond introductory CS. With approximately **340 citations**, his impact reflects both highly cited integrative scholarship in bio-inspired geotechnics and sustained contributions to granular microstructure and soil–robot interaction, complemented by growing influence in the computing-education community through multi-institutional studies and curriculum-focused work on equity, sustainability, and AI-augmented instruction."}, "hw23CTQAAAAJ": {"keywords": "Computational Seismology, Bayesian Inference, Digital Twins for Geological Carbon Storage, Full-Waveform Inversion (FWI), Compressive Sensing, Curvelet Transform, Seismic Data Interpolation (Matrix Completion), PDE-Constrained Optimization, Reverse-Time Migration (RTM), High-Performance Finite-Difference Modeling (Devito)", "summary": "## Overview\nFelix J. Herrmann is a Professor in the Schools of **Earth and Atmospheric Sciences** and **Computational Science and Engineering**, where he leads research at the intersection of *computational geophysics* and *large-scale inverse problems*. His work centers on **computational seismology** and **Bayesian inference**, with a sustained emphasis on <u>sparsity-promoting methods</u>, <u>randomized algorithms</u>, and <u>PDE-constrained optimization</u> for seismic imaging and inversion. Across foundational contributions to curvelet-based representations and modern high-performance tooling (notably **Devito**), Herrmann’s research advances *scalable* and *uncertainty-aware* workflows that increasingly connect to emerging applications such as <u>digital twins for geological carbon storage</u>.\n\n---\n\n## Research Areas\nHerrmann’s research portfolio spans the theory and practice of seismic data processing, imaging, and inversion under severe computational and acquisition constraints. A major thread is <u>sparse representations</u> and <u>compressive sensing</u> for seismic acquisition and reconstruction, exemplified by his influential work on curvelet-frame formulations for missing-trace interpolation and denoising, including “**Non-parametric seismic data recovery with curvelet frames**” (2008) and “**Simply denoise: Wavefield reconstruction via jittered undersampling**” (2008), as well as earlier developments in “**Seismic denoising with nonuniformly sampled curvelets**” (2006). In seismic imaging and inversion, he has addressed nonconvexity and cycle-skipping in full-waveform inversion (FWI) by reformulating the optimization landscape—most notably in “**Mitigating local minima in full-waveform inversion by expanding the search space**” (2013)—and by developing robust and constrained strategies that incorporate edge-preserving structure and prior information (e.g., total-variation and constraint/penalty perspectives). Complementing these algorithmic advances, Herrmann has contributed to computational infrastructure for PDE-based geophysics, including the **Devito** domain-specific language for high-performance finite-difference solvers (“Devito (v3.1.0)…” 2019; “Architecture and performance of Devito…” 2020), enabling scalable implementations of reverse-time migration and FWI. More recently, his trajectory integrates <u>uncertainty quantification</u> for inverse problems (“Uncertainty quantification for inverse problems with weak PDE constraints,” 2018) and operator-learning approaches (“Model-parallel Fourier neural operators…” 2023), aligning methodological advances with the needs of monitoring and decision support in subsurface applications such as carbon storage.\n\n---\n\n## Notable Works\n- **Curvelet-based seismic data recovery and interpolation:** Established widely adopted sparsity-promoting recovery methods for incomplete acquisition geometries in “**Non-parametric seismic data recovery with curvelet frames**” (2008), building a practical bridge between curvelet frames and seismic reconstruction at scale.  \n- **Reformulations to reduce nonconvexity in full-waveform inversion:** Proposed a strategy to counter local minima by enlarging the inversion search space in “**Mitigating local minima in full-waveform inversion by expanding the search space**” (2013), influencing subsequent work on robust and constrained FWI workflows.  \n- **High-performance software for PDE-based geophysical inversion:** Co-developed **Devito**, an embedded DSL that automates optimized stencil-code generation for finite-difference PDE solvers, as presented in “**Devito (v3. 1.0): an embedded domain-specific language for finite differences and geophysical exploration**” (2019) and extended in “**Architecture and performance of Devito, a system for automated stencil computation**” (2020), enabling reproducible, scalable seismic modeling and inversion.\n\n---\n\n## Academic Background\nHerrmann’s publication record—spanning early work on multiscale representations and wave phenomena through a long sequence of highly cited contributions in curvelet methods, compressive sensing, and wave-equation inversion—reflects a career shaped by sustained engagement with both *mathematical signal processing* and *computational geophysics*. His affiliations with the Schools of **Earth and Atmospheric Sciences** and **Computational Science and Engineering** indicate a deliberately interdisciplinary academic path, integrating geophysical modeling with modern optimization, scientific computing, and scalable software design. The breadth of his collaborations—ranging from foundational curvelet-based seismic processing (e.g., denoising, interpolation, multiple suppression) to PDE-constrained optimization frameworks and community-facing computational tools—suggests leadership in cross-domain research programs and training environments. With **11,730** total citations, Herrmann’s work has achieved substantial international visibility and influence, consistent with a senior academic profile recognized for both methodological innovation and enabling infrastructure that shapes contemporary practice in seismic imaging, inversion, and uncertainty-aware subsurface decision-making."}, "jGqGKGMAAAAJ": {"keywords": "scientific computing, high-performance computing, numerical methods, molecular dynamics simulations, sparse linear solvers, preconditioning (ILU and sparse approximate inverse), Krylov subspace methods, parallel and asynchronous iterative methods, sparse matrix-vector multiplication (SpMV) optimization, multigrid and domain decomposition methods", "summary": "## Overview\nEdmond Chow is a researcher at the Georgia Institute of Technology, where he works at the intersection of **scientific computing** and **high-performance computing** with an emphasis on **numerical methods** for large-scale simulation and data-intensive computation. His scholarship centers on designing *scalable* algorithms and implementations that make demanding workloads feasible on modern parallel architectures, with particular attention to <u>parallel sparse linear algebra</u>, <u>preconditioning</u>, and <u>communication- and synchronization-aware solvers</u>. Across a highly cited body of work (over **11,000** citations), Chow has contributed foundational ideas that connect algorithmic theory to practical performance in real applications, ranging from molecular dynamics to graph analytics.\n\n---\n\n## Research Areas\nChow’s research spans several tightly connected areas of computational mathematics and HPC systems design. A major thread is <u>preconditioning and iterative methods for sparse linear systems</u>, including robust incomplete factorization strategies for challenging nonsymmetric/indefinite problems (e.g., *“Experimental study of ILU preconditioners for indefinite matrices”* (1997) and *“Crout versions of ILU for general sparse matrices”* (2003)) and the development of <u>sparse approximate inverse</u> methodologies that avoid unstable triangular factors (e.g., *“Approximate inverse preconditioners via sparse-sparse iterations”* (1998) and *“A priori sparsity patterns for parallel sparse approximate inverse preconditioners”* (2000)). A second theme is <u>parallelization of sparse kernels and preconditioner application</u> on many-core and accelerator-oriented platforms, including optimized sparse matrix–vector multiplication (*“Efficient sparse matrix-vector multiplication on x86-based many-core processors”* (2013)) and parallel/iterative approaches to sparse triangular solves and incomplete factorizations (*“Fine-grained parallel incomplete LU factorization”* (2015); *“Iterative sparse triangular solves for preconditioning”* (2015); *“Using Jacobi iterations and blocking for solving sparse triangular systems…”* (2018)). More recently, he has advanced <u>asynchronous and decentralized solvers</u> that reduce global synchronization costs at scale (*“Performance of asynchronous optimized Schwarz with one-sided communication”* (2019); *“Scalable asynchronous domain decomposition solvers”* (2020); *“Asynchronous multigrid methods”* (2019)). In parallel, Chow has made influential contributions to <u>large-scale molecular simulation</u> and specialized architectures, helping enable long-timescale biomolecular dynamics through scalable algorithms and low-latency communication techniques (*“Scalable algorithms for molecular dynamics simulations on commodity clusters”* (2006); *“Millisecond-scale molecular dynamics simulations on Anton”* (2009); *“Exploiting 162-nanosecond end-to-end communication latency on Anton”* (2010)). His portfolio also includes scalable graph algorithms (*“A scalable distributed parallel breadth-first search algorithm on BlueGene/L”* (2005)) and structured/dense-kernel approximation methods (*“SMASH: Structured matrix approximation by separation and hierarchy”* (2018); *“Interpolative decomposition via proxy points for kernel matrices”* (2020)), reflecting a consistent focus on algorithmic structure that unlocks performance.\n\n---\n\n## Notable Works\n- **Scalable molecular dynamics on commodity clusters:** In *“Scalable algorithms for molecular dynamics simulations on commodity clusters”* (2006), Chow contributed to algorithmic and implementation techniques that significantly improved the scalability of biomolecular MD, helping push long-timescale simulation closer to practical reach on widely available parallel systems.\n- **Approximate inverse preconditioning for difficult sparse systems:** In *“Approximate inverse preconditioners via sparse-sparse iterations”* (1998), he advanced sparse approximate inverse approaches as an alternative to ILU when incomplete factors become unstable, shaping subsequent work on parallel-friendly preconditioners and pattern selection (including the later *a priori* pattern ideas in 2000 and parallel implementations in 2001).\n- **Scalable graph traversal on leadership-class machines:** In *“A scalable distributed parallel breadth-first search algorithm on BlueGene/L”* (2005), he helped demonstrate distributed BFS at extreme scale—up to billions of vertices—anticipating the centrality of graph analytics in modern large-scale computing.\n\n---\n\n## Academic Background\nBased on his long-running publication record from the mid-1990s onward and sustained contributions across sparse numerical linear algebra, parallel solver libraries, and application-driven HPC, Edmond Chow’s academic trajectory reflects deep training in computational science and applied mathematics with a strong systems orientation. His early and influential work on ILU stability for indefinite matrices (1997) and approximate inverse preconditioners (1998–2001) suggests formative engagement with the core challenges of Krylov methods and preconditioning, while his later emphasis on fine-grained parallel factorization, asynchronous methods, and performance modeling indicates continued leadership aligned with evolving architectures (many-core CPUs, GPUs, and large distributed systems). His participation in software- and infrastructure-relevant efforts—such as the solver-library design perspective in *“Design of the hypre preconditioner library”* (1999)—and in flagship HPC venues (including SC-related proceedings and widely cited application papers) signals strong professional affiliation with the international scientific computing and supercomputing communities. The breadth and impact of his work, evidenced by **11,465** total citations and multiple highly cited papers spanning numerical methods and large-scale applications, indicate recognition consistent with a senior, widely influential contributor in <u>scalable scientific computing</u> at Georgia Tech."}, "jnHI7wQAAAAJ": {"keywords": "Scientific machine learning, Model reduction, Multi-fidelity methods, Uncertainty quantification, Physics-informed machine learning, Operator inference, Reduced-order modeling for nonlinear PDEs, Bayesian inverse problems, Ensemble Kalman inversion, Reduced basis methods for PDE-constrained optimization", "summary": "## Overview\nElizabeth Qian is a researcher at the Georgia Institute of Technology, where she develops **scientific machine learning** methods for *computationally efficient* simulation and inference in complex engineered systems. Her work centers on <u>structure-preserving reduced-order modeling</u> for large-scale dynamical systems and nonlinear PDEs, integrating *physics-informed learning* with rigorous numerical analysis to produce surrogates that are both accurate and reliable. A defining theme of her scholarship is the unification of **model reduction**, **multi-fidelity methods**, and **uncertainty quantification** to enable many-query workflows in design, control, and Bayesian inverse problems.\n\n---\n\n## Research Areas\nQian’s research spans the theory and practice of data-driven and physics-informed surrogates for high-dimensional dynamical systems, with a particular emphasis on learning reduced models that respect governing equations and latent structure. In “**Lift & learn: Physics-informed machine learning for large-scale nonlinear dynamical systems**” (2020), she introduces a physics-informed approach that leverages knowledge of the governing equations to identify coordinate transformations that reveal *quadratic structure*, enabling efficient low-dimensional modeling for nonlinear dynamics. This line of work is complemented by “**Transform & Learn: A data-driven approach to nonlinear model reduction**” (2019), which explicitly bridges projection-based model reduction and machine learning in order to obtain practical nonlinear surrogates for many-query settings. Her contributions to operator learning and surrogate fidelity are further developed in “**The cost-accuracy trade-off in operator learning with neural networks**” (2022), which analyzes how computational expense and approximation quality interact when neural networks are used to learn operators arising from PDE models. Alongside scientific machine learning, Qian has made substantial contributions to <u>multi-fidelity uncertainty quantification</u>, including “**Multifidelity Monte Carlo estimation of variance and sensitivity indices**” (2018), which addresses the sample-cost bottleneck of variance-based sensitivity analysis by combining models of differing fidelities. Her broader portfolio extends to certified reduced-basis methods for PDE-constrained optimization (“**A certified trust region reduced basis approach to PDE-constrained optimization**,” 2017), scalable simulation strategies for turbulent flows (“**Towards scalable parallel-in-time turbulent flow simulations**,” 2013), and emerging work at the intersection of reduced models and Bayesian inference (e.g., balancing-based reduction for inference and likelihood-informed reduction in 2022–2025).\n\n---\n\n## Notable Works\n- **“Lift & learn: Physics-informed machine learning for large-scale nonlinear dynamical systems” (2020)** — Introduced the *Lift & Learn* framework, using physics-informed coordinate transformations to expose quadratic structure and enable accurate, efficient reduced modeling of large-scale nonlinear dynamics.\n- **“The cost-accuracy trade-off in operator learning with neural networks” (2022)** — Provided a detailed study of surrogate modeling for PDE-driven operators, clarifying how neural-operator approximation quality scales with computational cost in scientific computing contexts.\n- **“Multifidelity Monte Carlo estimation of variance and sensitivity indices” (2018)** — Advanced multi-fidelity Monte Carlo methodology for uncertainty quantification by reducing the cost of estimating variance and sensitivity indices while retaining statistical reliability.\n\n---\n\n## Academic Background\nBased on her publication trajectory, Qian’s academic development reflects a progression from early systems and aerospace-oriented research—such as autonomy and persistent UAV operations (“**Autonomous battery swapping system for small-scale helicopters**,” 2010) and simulation frameworks for turbulent flow (“**Towards scalable parallel-in-time turbulent flow simulations**,” 2013)—to a sustained, highly cited program in computational science and engineering focused on reduced-order modeling, multi-fidelity computation, and scientific machine learning. Her mid-career contributions emphasize rigor and certification in computational workflows, exemplified by her work on **certified reduced-basis** techniques for PDE-constrained optimization (2016–2017), and later expand into physics-informed and operator-learning paradigms for nonlinear PDEs (2019–2024), including structure- and stability-aware reduced models. With over **1,000 citations** (reported total: 1048), she is recognized as a significant contributor to <u>physics-informed reduced modeling</u> and <u>multi-fidelity UQ</u>, and her recent papers indicate growing engagement with Bayesian inversion and inference-oriented reduction (2022–2025), aligning her Georgia Tech affiliation with an interdisciplinary research profile spanning machine learning, numerical analysis, and engineering applications."}, "kK3b1SAAAAAJ": {"keywords": "computer science, cybersecurity, education, algorithms, mathematics, software testing, mutation analysis, test data generation, program debugging and slicing, fault injection cryptanalysis", "summary": "## Overview\nRichard DeMillo is **Professor of Computing** at the **Georgia Institute of Technology**, where he has been a leading voice in the intersection of **software reliability**, **cybersecurity**, and *computing-informed institutional change*. Working within Georgia Tech’s broader computing research ecosystem, his scholarship centers on <u>trustworthy computing</u>: how complex software systems can be tested, debugged, and secured in ways that remain effective under real-world constraints. Across a career spanning foundational theory and applied practice, DeMillo has helped shape modern thinking about **test adequacy**, **fault models**, and *the limits of purely formal approaches* to assurance, while also contributing influential perspectives on <u>technology-enabled education</u> and the future of universities.\n\n---\n\n## Research Areas\nDeMillo’s research portfolio is anchored in **software testing and analysis**, particularly mutation-based approaches and principled test-data design. His widely cited work on the *coupling effect* and practical test selection—most notably “**Hints on test data selection: Help for the practicing programmer**” (2006)—articulates why tests that expose simple faults often detect more complex ones, providing an empirical and conceptual basis for cost-effective testing strategies. Complementing this, his earlier line of work on **constraint-based testing**—including “**Constraint-based automatic test data generation**” (1991) and related experimental tool work—frames test generation as a fault-based process in which program behaviors are encoded as algebraic constraints to systematically target defect classes. A second major thread is **debugging and fault localization**, where dynamic program analysis methods such as slicing and execution backtracking (“**Debugging with dynamic slicing and backtracking**,” 1993; “Critical slicing for software fault localization,” 1996) aim to reduce the cognitive burden of diagnosing failures in complex codebases. In **cybersecurity and cryptography**, DeMillo is known for advancing *fault attack* perspectives on cryptographic implementations: “**On the importance of checking cryptographic protocols for faults**” (1997) and “On the importance of eliminating errors in cryptographic computations” (2001) model how random or induced hardware faults can compromise protocols and signature schemes (e.g., RSA/Rabin), establishing a security lens in which <u>implementation integrity</u> is as crucial as mathematical soundness. Underlying these applied contributions is a longstanding engagement with **mathematical logic and the philosophy of verification**, exemplified by “**Social processes and proofs of theorems and programs**” (1979), which argues that program verification does not occupy the same social and methodological role in computing as proof does in mathematics—an argument that has influenced debates about assurance, evidence, and evolving software artifacts. In later work, he extends his impact to **computing and education policy**, addressing structural change in higher education (e.g., “Abelard to Apple,” 2011; “Revolution in higher education,” 2015) and practice-oriented scholarship on blended learning (2019), connecting technological capability with institutional design and public trust.\n\n---\n\n## Notable Works\n- **Practical test selection and the coupling effect**: In “**Hints on test data selection: Help for the practicing programmer**” (2006), DeMillo synthesizes evidence and guidance showing how tests that catch simple defects can also expose complex ones, shaping pragmatic approaches to <u>test adequacy</u> and developer-facing testing strategy.  \n- **Fault attacks on cryptographic protocols and implementations**: “**On the importance of checking cryptographic protocols for faults**” (1997) provides a theoretical model demonstrating how random hardware faults can be exploited to break implementations of RSA/Rabin signatures and compromise authentication protocols, helping establish <u>fault-based cryptanalysis</u> as a critical implementation-security concern.  \n- **Constraint-based automatic test generation**: In “**Constraint-based automatic test data generation**” (1991), he advances a fault-based method that uses algebraic constraints—connected to mutation analysis—to generate test data that approximates relative adequacy, influencing subsequent automated testing and analysis toolchains.\n\n---\n\n## Academic Background\nDeMillo’s academic trajectory reflects a sustained, high-impact presence in computer science research, evidenced by a citation record exceeding **14,000** and a publication arc that begins with foundational contributions in the late 1970s on program testing, logic, and security (“Mutation Analysis,” 1979; “Social processes and proofs of theorems and programs,” 1979; early work on secure computation and protocol concepts). His long-term affiliation with **Georgia Tech** positions him within one of the leading U.S. computing environments, where his work bridges **theory**, **systems practice**, and *institutional-scale questions* about how computing reshapes education and public infrastructure. The breadth of his topics—from mutation operator design and the Mothra testing environment (late 1980s) to dynamic slicing and debugging (1990s), cryptographic fault models (late 1990s–2000s), and higher-education transformation (2010s)—suggests a career marked by cross-domain leadership and sustained collaboration with software engineering and security communities. While specific degrees and named awards are not enumerated here, the influence and longevity of his contributions indicate deep standing within the field, with research that has helped define how practitioners and scholars reason about <u>software assurance</u>, <u>implementation security</u>, and the evolving role of computing in education."}, "kM95eWgAAAAJ": {"keywords": "Computational Behavior Analysis, Activity Recognition, Ubiquitous Computing, Health, Machine Learning, Wearable Sensor Analytics, Deep Learning (CNN/LSTM/Attention) for HAR, Self-Supervised Representation Learning (contrastive/masked reconstruction), Digital Health Biomarkers (Parkinson’s disease & mental health), Animal Behavior Monitoring (precision livestock farming)", "summary": "## Overview\nThomas Ploetz is a researcher at the **Georgia Institute of Technology**, where he leads and contributes to work at the intersection of **ubiquitous computing** and **machine learning** with a sustained emphasis on <u>computational behavior analysis</u>. His scholarship centers on *sensor-driven inference of human (and animal) behavior*—especially <u>wearables-based human activity recognition (HAR)</u>—and on translating methodological advances in **deep learning** and **representation learning** into robust, deployable systems for **health** and real-world monitoring. With a citation record exceeding **14,700**, Ploetz is widely recognized for shaping how the community models time-series sensor data, evaluates generalization, and builds *clinically and ecologically valid* behavioral measurement pipelines.\n\n---\n\n## Research Areas\nPloetz’s research spans core methodological contributions to activity recognition and broader applications in health and welfare monitoring. A major thread is the shift from hand-engineered pipelines to learned representations for wearable sensing, articulated in “**Deep, convolutional, and recurrent models for human activity recognition using wearables**” (2016) and extended through sequence modeling and ensembling in “**Ensembles of Deep LSTM Learners for Activity Recognition using Wearables**” (2017). Complementing these deep architectures, his earlier work on representation learning—e.g., “**Feature learning for activity recognition in ubiquitous computing**” (2011)—helped formalize feature learning as an alternative to heuristic feature engineering in pervasive sensing. He has also advanced the field’s understanding of evaluation and data issues that affect real-world validity, including sampling and deployment constraints (“**Optimising sampling rates for accelerometer-based human activity recognition**,” 2016) and methodological pitfalls in model assessment (“**Let’s (not) Stick Together: Pairwise Similarity Biases Cross-Validation in Activity Recognition**,” 2015). In parallel, Ploetz has pursued health-facing behavior analytics at scale and in naturalistic settings, notably through accelerometry-based population measurement in “**Large scale population assessment of physical activity using wrist worn accelerometers: the UK biobank study**” (2017) and disease monitoring via wearables and deep learning in “**PD disease state assessment in naturalistic environments using deep learning**” (2015). His portfolio further extends to computational monitoring of animal welfare, leveraging automated behavioral change detection in commercial farming contexts (e.g., “**Early detection of health and welfare compromises through automated detection of behavioural changes in pigs**,” 2016; and subsequent tracking-based approaches in 2017), underscoring a unifying agenda: *learning reliable behavioral signatures from imperfect, large-scale sensor data*.\n\n---\n\n## Notable Works\n- **Catalyzing deep learning for wearable HAR:** “**Deep, convolutional, and recurrent models for human activity recognition using wearables**” (2016) synthesized and validated CNN/RNN-based approaches for sensor time series, helping establish end-to-end deep architectures as a practical replacement for hand-crafted feature pipelines in ubiquitous computing.\n- **Enabling population-scale physical activity measurement:** “**Large scale population assessment of physical activity using wrist worn accelerometers: the UK biobank study**” (2017) provided a landmark methodological blueprint for extracting objective physical activity signals from wrist accelerometers at cohort scale, supporting downstream epidemiological associations with health outcomes.\n- **Advancing robust sequence modeling and generalization:** “**Ensembles of Deep LSTM Learners for Activity Recognition using Wearables**” (2017) demonstrated how ensemble strategies and recurrent learners can improve performance and stability in HAR, reinforcing a broader research program attentive to reliability, variability across users, and real-world deployment constraints.\n\n---\n\n## Academic Background\nBased on his publication trajectory and long-running contributions across machine learning, pervasive sensing, and health analytics, Ploetz’s academic profile reflects a sustained, interdisciplinary career bridging **pattern recognition** and **ubiquitous computing**. His earlier highly cited survey work on sequence models in pattern recognition—“**Markov models for offline handwriting recognition: a survey**” (2009)—signals foundational training in probabilistic modeling and sequential inference, which later informed his influential work on wearable time-series modeling and deep recurrent architectures. Over time, his affiliations and collaborations expanded into large-scale digital health and epidemiology (e.g., the UK Biobank accelerometry program) and clinically oriented sensing for neurological and mental health assessment (e.g., Parkinson’s disease state estimation in naturalistic environments). Now at the Georgia Institute of Technology, he is strongly associated with the international HAR and pervasive health communities, with impact evidenced by both methodological papers (feature learning, deep architectures, evaluation rigor) and application-driven studies spanning <u>objective behavior measurement</u> in humans and animals—an integrated record that typically accompanies recognition through competitive funding, invited visibility, and leadership roles in the field’s scholarly venues and cross-disciplinary collaborations."}, "kgFnNewAAAAJ": {"keywords": "Cyber-Physical Systems Security, Industrial Control Systems (ICS) Security, Smart Grid Security, Programmable Logic Controllers (PLC) Security, Cyber-Physical Resilience, False Data Injection Attacks, Side-Channel Control-Flow Monitoring (EM/Power), Physics-Based Attestation and Runtime Verification, Privacy-Preserving Biometrics and Encrypted Identification, Cloud Security and Secure Search over Encrypted Data", "summary": "## Overview\nSaman Zonouz is an Associate Professor at **Georgia Tech**, where he leads research at the intersection of **cybersecurity** and **critical cyber-physical infrastructures**. His work centers on *defending systems whose digital compromises can trigger real-world consequences*, with a sustained emphasis on **smart grid** and **industrial control systems (ICS)**. Across his publications, Zonouz advances methods for <u>cyber-physical security assessment</u>, <u>intrusion detection and response</u>, and <u>trustworthy control execution</u>, bridging security engineering with power systems, embedded control, and data-driven analytics.\n\n---\n\n## Research Areas\nZonouz’s research spans multiple layers of cyber-physical systems security, from system-level risk modeling to device-level integrity monitoring. A major thread of his scholarship develops **security-oriented modeling and assessment** for the power grid, including cyber-physical vulnerability/risk quantification in *CPIndex: Cyber-physical vulnerability assessment for power-grid infrastructures* (2014) and integrated dependency modeling in *A cyber-physical modeling and assessment framework for power grid infrastructures* (2015). Complementing assessment, he has contributed to **resilience and response** mechanisms, such as automated decision-making under attack in *RRE: A game-theoretic intrusion response and recovery engine* (2013) and conceptual/metric foundations in *Cyber-physical resilience: Definition and assessment metric* (2017). On the embedded/ICS side, his work examines how adversaries exploit the physics-control interface—most notably in *Hey, My Malware Knows Physics! Attacking PLCs with Physical Model Aware Rootkit* (2017)—and proposes defensive verification and monitoring approaches for controllers, including safety verification (*A Trusted Safety Verifier for Process Controller Code*, 2014) and contactless side-channel monitoring (*Watch me, but don't touch me! contactless control flow monitoring via electromagnetic emanations*, 2017). He has also addressed **data integrity and detection** problems in grid telemetry and AMI, including multi-sensor theft detection (*A multi-sensor energy theft detection framework for advanced metering infrastructures*, 2013) and security-oriented state estimation (*SCPSE*, 2012). Beyond grid/ICS, his portfolio includes **privacy-preserving cloud security** for biometrics (*CloudID*, 2015; *Identification using encrypted biometrics*, 2013) and later work touching efficient learning systems (e.g., network compression in *Chip: Channel independence-based pruning for compact neural networks*, 2021), reflecting an expanding toolkit for trustworthy computation in resource- and safety-constrained settings.\n\n---\n\n## Notable Works\n- **Trustworthy cloud and privacy-preserving identification**: In *CloudID: Trustworthy cloud-based and cross-enterprise biometric identification* (2015), Zonouz advanced cloud-enabled biometric identification with an emphasis on trust and cross-enterprise use, extending a line of work on confidentiality in *Identification using encrypted biometrics* (2013).\n- **Cyber-physical threat detection for the smart grid**: In *A multi-sensor energy theft detection framework for advanced metering infrastructures* (2013), he introduced a multi-sensor framework for identifying energy theft and malicious behavior in AMI—an influential contribution to practical detection in grid-edge cyber-physical deployments.\n- **Physics-aware attacks on industrial controllers (and implications for defense)**: In *Hey, My Malware Knows Physics! Attacking PLCs with Physical Model Aware Rootkit* (2017), he helped establish how malware can exploit knowledge of physical process models to evade detection while manipulating PLC behavior, shaping subsequent work on controller integrity and monitoring.\n\n---\n\n## Academic Background\nZonouz’s publication record indicates an early foundation in **reliability and probabilistic risk analysis** (e.g., *A fuzzy-monte carlo simulation approach for fault tree analysis*, 2006) and **power-system estimation and coordination** (e.g., *A kalman-based coordination for hierarchical state estimation*, 2008), which later evolved into a sustained research agenda in **cyber-physical systems security**. By the early 2010s, his work increasingly focused on smart grid and ICS protection—spanning security-oriented state estimation (*SCPSE*, 2012), contingency/risk analysis (*SOCCA*, 2013; *CPIndex*, 2014), and automated response (*RRE*, 2013)—suggesting deep interdisciplinary affiliation with both the **power/energy systems** and **computer security** research communities. His high scholarly impact (over **5,900 citations**) reflects broad adoption of his ideas across CPS security, smart grid defense, and trustworthy embedded control, and his role as an Associate Professor at Georgia Tech positions him as a key contributor to academic and applied efforts aimed at securing <u>critical infrastructure</u> against evolving cyber-physical threats."}, "kiR4luIAAAAJ": {"keywords": "artificial intelligence, cognitive science, online education, AI ethics, virtual teaching assistants, theory of mind, education at scale, online graduate computer science programs, learning analytics, assessment and grading at scale", "summary": "## Overview\nBobbie Lynn Eicher is a researcher at the Georgia Institute of Technology whose work sits at the intersection of **artificial intelligence**, **cognitive science**, and *online education*. Working within Georgia Tech’s broader ecosystem of large-scale computing education and learning-at-scale initiatives, Eicher investigates how AI-mediated educational systems can be designed, evaluated, and governed responsibly—particularly where automation meets high-stakes student experience. Their scholarship emphasizes <u>empirical grounding</u> for **AI ethics** and the practical realities of <u>education at scale</u>, from virtual teaching assistants to assessment systems and student decision-making in large online graduate programs.\n\n---\n\n## Research Areas\nEicher’s research spans three tightly connected areas: (1) **AI ethics in educational AI**, (2) **student experience and decision-making in online graduate education**, and (3) **assessment and feedback at scale**. A central thread is the use of empirical studies to surface ethical and social implications of deploying AI in authentic learning contexts, exemplified by the widely cited work on a virtual teaching assistant, “**Jill Watson doesn’t care if you’re pregnant: Grounding AI ethics in empirical studies**” (2018), which foregrounds how seemingly routine automated interactions can produce consequential harms and inequities. Complementing this, Eicher examines who participates in scaled online CS education and why, analyzing enrollment motivations and demographic differences in “**Enrollment motivations in an online graduate CS program: Trends & gender-and age-based differences**” (2020). In parallel, Eicher advances cognitive-science-informed approaches to human–AI collaboration through **theory of mind** as a design foundation for co-creation and intelligent tutoring, including “**Toward mutual theory of mind as a foundation for co-creation**” (2017) and the conceptual extension “**Giving AI a Theory of Mind**” (2018). More recent work extends these interests into the infrastructure of scaled learning—how grading and course policies evolve under automation pressures (“**Components of assessments and grading at scale**,” 2021; “**Making the Grade: Assessments at Scale in a Large Online Graduate Program**,” 2023), how students use informal information channels (“**Student Use of Course Reviews at Scale**,” 2022; “**An Examination of Unofficial Course Reviews in a Graduate Program at Scale**,” 2022), and how intelligent systems can support evaluation of peer feedback (“**Intelligent Feedback and Evaluation for the Assessment and Improvement of Student Peer Reviews**,” 2024). Across these topics, Eicher’s contribution is a coherent agenda focused on <u>human-centered</u>, **evidence-driven** design for AI-enabled education.\n\n---\n\n## Notable Works\n- **Grounding AI ethics in real educational deployments:** In “**Jill Watson doesn’t care if you’re pregnant: Grounding AI ethics in empirical studies**” (2018), Eicher helps establish an influential empirical approach to **AI ethics** in learning technologies by analyzing ethical issues that arise when a virtual teaching assistant is deployed in a live AI course context.\n- **Mapping motivations and demographic patterns in scaled online CS education:** “**Enrollment motivations in an online graduate CS program: Trends & gender-and age-based differences**” (2020) provides a data-driven account of why working professionals pursue online graduate CS degrees, highlighting gender- and age-linked differences that inform program design, advising, and equitable participation.\n- **Bringing cognitive science to co-creation and tutoring with AI:** Through “**Toward mutual theory of mind as a foundation for co-creation**” (2017) and related work (e.g., “**Giving AI a Theory of Mind**,” 2018), Eicher advances <u>mutual theory of mind</u> as a foundation for more effective human–AI collaboration and more *human-like* intelligent tutoring interactions.\n\n---\n\n## Academic Background\nBased at the Georgia Institute of Technology, Eicher’s publication trajectory indicates sustained involvement in Georgia Tech’s research community focused on computing education, learning at scale, and human-centered AI. Their early and continuing emphasis on **theory of mind** and co-creation (2017–2018) aligns with a cognitive-science-informed approach to intelligent systems, while later work expands into program-level questions central to large online graduate education—student motivations (2020), assessment design under scale constraints (2021–2023), and emergent student information practices such as unofficial course reviews (2022). Eicher’s visibility is underscored by a citation profile of **183 total citations**, driven in particular by the highly cited 2018 ethics paper on Jill Watson, suggesting substantial impact and recognition within the learning technologies and AI-in-education communities. Their recent publications (2024–2025) reflect an ongoing agenda that integrates educational measurement, peer-feedback evaluation, and access to research opportunities for online graduate students, consistent with affiliations and collaborations typical of interdisciplinary education-and-AI scholarship at Georgia Tech."}, "kq16N1AAAAAJ": {"keywords": "Cognitive Science, Cognitive Psychology, Cognitive Architecture, Mathematical Cognition, Computational Thinking, Educational Neuroscience, Developmental Dyscalculia, Executive Function and Planning, Discourse and Text Comprehension, Large Language Models and Cognitive Modeling", "summary": "## Overview\nSashank Varma is a faculty researcher in the **Schools of Interactive Computing and Psychology** at Georgia Tech, where he works at the intersection of **cognitive science** and *computational approaches to mind and learning*. His scholarship centers on <u>computational modeling of cognition</u>—especially **cognitive architectures** and their links to brain function—and on <u>mathematical cognition</u> with an emphasis on how people represent, learn, and struggle with number concepts. Across this agenda, Varma’s work connects **theory**, *empirical measurement* (including neuroimaging), and **educational implications**, aiming to clarify how complex thinking emerges from resource-limited cognitive systems and how that knowledge can responsibly inform instruction.\n\n---\n\n## Research Areas\nVarma’s research spans three tightly connected areas. First, he has advanced **cognitive architecture** approaches that formalize high-level cognition under capacity limits, including work on architecture design criteria and interpretive frameworks (e.g., “Criteria for the design and evaluation of cognitive architectures,” 2011; “The subjective meaning of cognitive architecture: A Marrian analysis,” 2014) and earlier architecture development linking computation to neuroimaging (“Computational modeling of high‐level cognition and brain function,” 1999; “4CAPS: An Adaptive Architecture for Human Information Processing,” 2006). Second, he has made sustained contributions to **neurocognitive accounts of complex problem solving and executive function**, using converging methods such as fMRI, functional connectivity, and computational modeling in classic planning tasks (“Frontal and parietal participation in problem solving in the Tower of London,” 2003) and extending these ideas to neurocognitive theories of developmental conditions (“Autism as a neural systems disorder: a theory of frontal-posterior underconnectivity,” 2012). Third, Varma’s work in **mathematical cognition and learning** investigates how numerical concepts become meaningful, how representations shift from concrete to abstract, and how these processes relate to educational outcomes—ranging from foundational deficits in number sense (“Dyscalculia: from brain to education,” 2011) to representational accounts of integers, decimals, and irrational numbers (“The mental representation of integers,” 2011; “Understanding decimal proportions,” 2013; “How the abstract becomes concrete: Irrational numbers…,” 2018), as well as connections between executive functions and math achievement (“Linking inhibitory control to math achievement…,” 2021). In parallel, he has helped shape **educational neuroscience** as a field by articulating conceptual and pragmatic constraints on translating brain research to classrooms (“Scientific and pragmatic challenges for bridging education and neuroscience,” 2008; “The principles and practices of educational neuroscience…,” 2016) and by empirically studying educators’ neuroscience understanding and misconceptions (“Taking an educational psychology course improves neuroscience literacy…,” 2018).\n\n---\n\n## Notable Works\n- **Reframing dyscalculia from neural mechanisms to educational implications**: In “**Dyscalculia: from brain to education**” (2011), Varma synthesizes evidence for a core deficit in understanding sets and numerosity and traces how this foundational impairment cascades into broader mathematical difficulties, providing a bridge from **cognitive/developmental neuroscience** to <u>intervention-relevant</u> educational considerations.  \n- **Computational-neurocognitive modeling of autism via systems connectivity**: “**Autism as a neural systems disorder: a theory of frontal-posterior underconnectivity**” (2012) advances the underconnectivity account and pairs it with a computational model of executive task performance, exemplifying Varma’s approach of using **formal models** to connect *behavioral signatures* with <u>brain-systems hypotheses</u>.  \n- **Triangulating planning and perception in problem solving with fMRI and modeling**: In “**Frontal and parietal participation in problem solving in the Tower of London**” (2003), Varma integrates neuroimaging, connectivity analyses, individual differences, and computational modeling to characterize how **frontal–parietal systems** jointly support planning and high-level visuospatial processing, establishing a template for <u>model-based cognitive neuroscience</u> of complex cognition.\n\n---\n\n## Academic Background\nBased on his long-running publication record spanning foundational work in discourse comprehension and capacity-constrained processing (e.g., “CAPping the construction-integration model of discourse comprehension,” 1995; “A hybrid architecture for working memory,” 2002) through computational cognitive neuroscience and educational applications, Varma’s academic trajectory reflects deep training in **cognitive psychology** and **computational modeling**, with sustained collaboration across neuroscience, education, and learning sciences communities. His affiliation with Georgia Tech’s Schools of Interactive Computing and Psychology is consistent with a career positioned between **methodological innovation** (computational architectures, model-based inference, and more recent evaluation of AI/LLMs on cognitive phenomena such as numeric magnitude and garden-path comprehension) and **theoretical synthesis** about how cognition maps onto brain systems (“The organization of thinking…,” 2007). With a citation record exceeding **5,700**, his influence is particularly visible in research programs on <u>mathematical learning and disability</u>, <u>systems-level accounts of cognition</u>, and the careful articulation of what educational neuroscience can—and cannot—claim for instructional practice, including teacher-facing work on neuroscience literacy and professional development (“Infusing neuroscience into teacher professional development,” 2013; “A place for neuroscience in teacher knowledge and education,” 2022)."}, "kqfLNK8AAAAJ": {"keywords": "AI, Machine Learning, Data Mining, Temporal Graph Representation Learning, Dynamic Embeddings, Signed Networks, Misinformation and Disinformation Detection, Online Abuse and Antisocial Behavior, Recommender Systems, Adversarial Robustness and AI Safety", "summary": "## Overview\nSrijan Kumar is an **Assistant Professor of Computer Science** at **Georgia Tech**, where he leads a research program at the intersection of **AI**, **machine learning**, and **data mining** with a sustained emphasis on *computational social systems*. His work develops methods for learning from complex, evolving data—especially networks and online platforms—to understand and mitigate harms such as misinformation, fraud, and manipulation. Across highly cited contributions, Kumar is particularly known for advancing <u>temporal representation learning</u> for interaction networks, <u>trust and signed-network modeling</u>, and <u>platform integrity</u> research that connects algorithmic innovation to real-world sociotechnical impact.\n\n---\n\n## Research Areas\nKumar’s research spans (i) **temporal and dynamic graph learning**, (ii) **online trust, integrity, and adversarial behavior**, and (iii) **misinformation and disinformation measurement and interventions**. In dynamic network modeling, his work on *learning evolving user/item representations*—notably “**Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks**” (2019)—targets sequential interactions in settings such as e-commerce and social platforms, framing representation learning around time-varying trajectories rather than static embeddings. In network inference and relational modeling, he has contributed to **signed and weighted social network** analysis through “**Edge Weight Prediction in Weighted Signed Networks**” (2016), addressing like/dislike and trust/distrust relationships and the predictive challenges posed by polarity and magnitude. Complementing these learning-centric efforts, Kumar has built a substantial body of work on **digital deception and platform abuse**, including fraud detection in ratings (“**Rev2: Fraudulent user prediction in rating platforms**,” 2018), identity manipulation (“**An Army of Me: Sockpuppets in Online Discussion Communities**,” 2017), and early detection of malicious behavior on Wikipedia (“**Vews: A wikipedia vandal early warning system**,” 2015). His misinformation scholarship includes both foundational synthesis (“**False information on web and social media: A survey**,” 2018) and empirical studies of specific phenomena such as Wikipedia hoaxes (2016), inter-community conflict (“**Community Interaction and Conflict on the Web**,” 2018), and hate/counterspeech dynamics during COVID-19 (“**Racism is a virus**,” 2021). More recently, his publication trajectory reflects emerging concerns around **LLMs and safety**—including cross-lingual evaluation and hallucinations in healthcare queries (2024), as well as benchmark and guardrail efforts for model safety (2024)—extending his long-standing focus on *reliability and harm reduction* in online information ecosystems.\n\n---\n\n## Notable Works\n- **Temporal interaction modeling and dynamic embeddings:** Introduced influential approaches for representing evolving entities in time-dependent interaction data in “**Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks**” (2019), a widely cited contribution shaping subsequent work in temporal graph representation learning.  \n- **Signed/weighted network prediction:** Advanced methods for inference in complex social graphs with polarity and intensity in “**Edge Weight Prediction in Weighted Signed Networks**” (2016), helping formalize predictive tasks over trust/distrust and like/dislike relations.  \n- **Misinformation scholarship and synthesis:** Consolidated and structured the research landscape in “**False information on web and social media: A survey**” (2018), providing a frequently cited reference point for characterization, spread mechanisms, and detection strategies for online false information.\n\n---\n\n## Academic Background\nAs a faculty member at **Georgia Tech**, Kumar has developed an internationally visible research agenda evidenced by **9,125 citations** and a publication record spanning premier topics in machine learning for networks, platform integrity, and computational social science. His scholarly footprint suggests sustained collaboration across **computer science** subfields—particularly data mining, network science, and applied ML—along with strong ties to interdisciplinary problems in information quality and online safety (e.g., misinformation, harassment, and health-related information reliability). The arc of his work—from early systems for Wikipedia vandal early warning (2015) and network inference in signed graphs (2016), to large-scale studies of deception and community conflict (2017–2021), and more recent efforts on **LLM evaluation, hallucinations, and safety benchmarks** (2023–2024)—reflects a consistent pattern of translating methodological advances into tools and empirical insights for real-world platforms. His affiliations and output indicate an active role in the broader AI and data mining research community, with contributions that bridge foundational modeling (dynamic embeddings, signed networks) and societally consequential applications (fraud detection, disinformation, and AI safety)."}, "lHmMK_4AAAAJ": {"keywords": "Computer Security, Cryptography, Policy, Vulnerability Discovery, Election Security, Internet Voting, Blockchain Voting, Cryptographic Library Vulnerabilities, Mobile and IoT Security, Privacy-Preserving Protocols", "summary": "## Overview\nMichael A. Specter is a computer security and cryptography researcher at **Georgia Tech**, where he works within the university’s security research ecosystem on problems at the intersection of **technical security**, **public policy**, and *real-world system failure modes*. Drawing on an **EECS PhD from MIT**, his work emphasizes <u>empirical vulnerability discovery</u> and <u>security for high-stakes civic and communications infrastructure</u>, including elections, widely deployed cryptographic libraries, and privacy-preserving protocols. Across academic and public-facing venues, Specter is known for connecting rigorous security analysis to *actionable* governance and deployment realities—especially where incentives, institutions, and engineering constraints collide.\n\n---\n\n## Research Areas\nSpecter’s research spans several tightly connected areas of modern security. A central theme is <u>election and democratic infrastructure security</u>, where he has examined the security limits of internet and mobile voting and the risks introduced by “blockchain” framings of election technology, including *Going from Bad to Worse: From Internet Voting to Blockchain Voting* (2021), *The Ballot is Busted Before the Blockchain* (2020) on the Voatz mobile voting application used in U.S. federal elections, and *Security Analysis of the Democracy Live Online Voting System* (2021) on OmniBallot. In parallel, he has pursued <u>cryptography in deployed systems</u> through empirical work on how cryptographic libraries fail in practice—e.g., *You really shouldn’t roll your own crypto* (2021) and the follow-on *Cryptography in the wild* (2024)—as well as work probing the institutional trust fabric of the web in *The economics of cryptographic trust: understanding certificate authorities* (2016). Specter also contributes to <u>privacy-preserving protocols and measurement</u>, including pandemic-era proximity/contact tracing specifications and mechanisms (e.g., *The PACT protocol specification* (2020) and *SonicPACT* (2020)), and more recent protocol designs for *Robust, privacy-preserving, transparent, and auditable on-device blocklisting* (2023). His broader security portfolio includes analyses of consumer and mobile ecosystems—such as *Security analysis of wearable fitness devices (fitbit)* (2014)—and emerging measurement work on tracking and fingerprinting supply chains in *Fingerprinting SDKs for Mobile Apps and Where to Find Them* (2025). Notably, his publication record also includes the widely cited survey *Explaining Explanations: An Overview of Interpretability of Machine Learning* (2018), reflecting engagement with interpretability as a security-adjacent concern in complex, high-impact systems.\n\n---\n\n## Notable Works\n- **Election technology security and the limits of online voting**, including *The Ballot is Busted Before the Blockchain: A Security Analysis of Voatz* (2020) and *Security Analysis of the Democracy Live Online Voting System* (2021), which articulated concrete technical risks and deployment pitfalls in real systems used or proposed for binding elections.  \n- **Security-policy analysis of cryptographic access mandates**, most prominently *Keys under doormats* (2015), a highly influential treatment of how exceptional-access requirements can *systemically* weaken security and reshape threat models for everyone.  \n- **Empirical study of cryptographic-library vulnerabilities in practice**, including *You really shouldn’t roll your own crypto* (2021) and *Cryptography in the wild* (2024), which investigate how vulnerabilities arise in widely reused crypto code and what those patterns imply for engineering practice, review processes, and ecosystem risk.\n\n---\n\n## Academic Background\nSpecter earned his **PhD in EECS from MIT**, grounding his career in a research tradition that blends cryptographic rigor with systems security and careful attention to real-world deployment incentives. His publication trajectory—spanning certificate authority trust economics (2016), exceptional-access policy (2015), and high-impact applied security analyses of election systems (2020–2023)—suggests sustained engagement with both academic communities and policy-facing audiences concerned with critical infrastructure integrity. Now at **Georgia Tech**, he continues to build an interdisciplinary profile that links <u>vulnerability discovery</u>, <u>cryptographic protocol design</u>, and <u>public-interest security</u>, with influence reflected in a substantial citation record (over **5,000** citations) and recurring contributions to timely, societally consequential security questions."}, "mHwonDEAAAAJ": {"keywords": "Computer Networking, Network Algorithmics, Data Streaming Algorithms, Performance Evaluation, Network Traffic Measurement, Sketching and Bloom Filters, Flow Size Distribution Estimation, DDoS Defense and IP Traceback, Prefix-Preserving IP Anonymization, Datacenter Network Failure Prediction and Syslog Mining", "summary": "## Overview\nJun Xu is a Professor of Computer Science at Georgia Tech, where he leads and collaborates within a research program centered on **computer networking** and **network algorithmics**. His work emphasizes *practical, high-impact measurement and security systems* grounded in rigorous algorithmic design, with particular strength in **data streaming algorithms** and **performance evaluation**. Across decades of contributions and more than **7,045 citations**, Xu has advanced <u>high-speed network measurement</u>, <u>streaming sketching techniques</u>, and <u>operational network reliability</u>, producing methods that translate theoretical efficiency into deployable mechanisms for monitoring, diagnosing, and defending modern networks.\n\n---\n\n## Research Areas\nXu’s research spans the algorithmic foundations and systems realities of networked infrastructure, with a sustained focus on streaming and sketch-based measurement under tight memory and line-rate constraints. In traffic measurement, he introduced compact data structures and streaming estimators for per-flow visibility, including the *Space-Code Bloom Filter* approach to efficient flow measurement (2003) and streaming methods for estimating **flow size distributions** (2004) and **traffic entropy** (2006), enabling anomaly detection and traffic characterization from limited observations. Complementing measurement, he has made major contributions to network security and privacy, including **IP traceback** and online mitigation against DDoS attacks (e.g., “IP traceback-based intelligent packet filtering,” 2003; “Large-scale IP traceback in high-speed Internet,” 2004) and influential work on <u>prefix-preserving IP anonymization</u> that balances trace utility with privacy risk through measurement-based evaluation and cryptography-based design (2002/2004). His interests also extend to scalable networked systems and operations: he studied fundamental tradeoffs in peer-to-peer routing table size versus diameter (“On the fundamental tradeoffs…,” 2003) and developed data-driven approaches to datacenter reliability, including predicting and diagnosing switch failures from logs (“Prefix: Switch failure prediction in datacenter networks,” 2018; “Syslog processing…,” 2017), reflecting an enduring commitment to *operationally grounded* networking research.\n\n---\n\n## Notable Works\n- **Sketch- and Bloom-filter-based network measurement at scale:** Xu’s “**Space-code bloom filter for efficient traffic flow measurement**” (2003) introduced a compact mechanism for per-flow measurement suitable for high-speed links, shaping subsequent work on memory-efficient monitoring and on-chip data structures for networking.\n- **Streaming inference of traffic structure and anomalies:** In “**Data streaming algorithms for efficient and accurate estimation of flow size distribution**” (2004) and “**Data streaming algorithms for estimating entropy of network traffic**” (2006), he helped establish streaming-based estimation as a practical foundation for traffic engineering, workload characterization, and anomaly detection using distributional and information-theoretic summaries.\n- **Internet security and privacy mechanisms for real deployments:** Through “**IP traceback-based intelligent packet filtering**” (2003) and the anonymization line of work “**Prefix-preserving ip address anonymization…**” (2002/2004), Xu advanced both *active defense* against DDoS and <u>privacy-preserving data sharing</u> for Internet measurement, addressing the dual needs of security response and trace publication.\n\n---\n\n## Academic Background\nAs a long-standing faculty member at Georgia Tech, Xu has built an academic profile characterized by sustained, highly cited contributions bridging **algorithms**, **systems**, and **network operations**. His publication trajectory—from early work on high-performance packet processing and filtering architectures (e.g., ATM firewall switching and packet classification in the late 1990s/early 2000s) to foundational studies in traffic measurement, anonymization, and DDoS defense in the early-to-mid 2000s, and later to datacenter reliability analytics and machine-learning-driven syslog processing (2010s)—suggests a career shaped by close engagement with evolving Internet and datacenter practice. The breadth of venues and topics in his record indicates strong interdisciplinary affiliations across networking, security, and data management communities, while the impact and continuity of his research themes reflect a scholar recognized for combining *theoretical efficiency* with <u>deployable performance evaluation</u> and measurement methodology."}, "mSw8KfIAAAAJ": {"keywords": "Computer Architecture, Computer Systems, Databases, AI, Serverless Computing (FaaS), Machine Learning Inference Serving, Interference-Aware Scheduling, Heterogeneous Systems, Video Analytics Systems, Model Selection and Auto-Tuning", "summary": "## Overview\nFrancisco Romero is a researcher at the Georgia Institute of Technology whose work sits at the intersection of **computer architecture**, **computer systems**, **databases**, and **AI**. His research program focuses on building *practical, high-performance systems* for modern cloud workloads—especially **serverless computing** and **machine-learning inference**—with an emphasis on <u>automated resource management</u>, <u>latency-critical serving</u>, and <u>end-to-end data-to-AI pipelines</u>. Across his publications, Romero repeatedly targets the operational friction that prevents scalable deployment—e.g., manual tuning, model/hardware complexity, and interference—by designing **model-less** and **managed** abstractions that deliver predictable performance and cost efficiency at scale.\n\n---\n\n## Research Areas\nRomero’s research spans three tightly connected areas: (1) **serverless systems** for elastic, transient compute; (2) **inference serving** for latency- and cost-constrained ML deployment; and (3) **data systems for video/IoT analytics** that integrate heterogeneous accelerators and evolving ML models. In serverless computing, he has advanced mechanisms for running everyday workloads on large fleets of ephemeral containers (e.g., *“From laptop to lambda”*), as well as state management primitives—most notably an auto-scaling cache layer for Function-as-a-Service environments (*“Faa$t”*) that mitigates the performance penalties of remote storage. In inference serving, Romero’s work centers on reducing the burden of selecting models, configuring deployments, and meeting strict SLOs under changing load; *“{INFaaS}: Automated model-less inference serving”* and *“A case for managed and model-less inference serving”* articulate and operationalize the idea that inference platforms should automatically navigate the space of model variants, hardware targets, and cost/latency trade-offs. He further addresses multi-tenant realities through interference-aware scheduling (*“Interference-aware scheduling for inference serving”*) and continues this line with *“Model selection for latency-critical inference serving”* (2024), which frames model selection and scheduling as a stochastic, load-varying decision problem. Finally, Romero contributes to **interactive video analytics** and heterogeneous pipeline optimization, including serverless auto-tuning of video DAGs (*“Llama”*), declarative relationships among models to accelerate analytics (*“Optimizing video analytics with declarative model relationships”*), and end-to-end systems for interactive querying (*“VIVA”*), extending recently to vision-language model–based video analytics (*“Zelda”*) and natural-language interfaces for IoT stream querying with LLMs (*“Flash-Fusion”*). Complementing these systems contributions, earlier work applies language-feature modeling and machine learning to behavioral and clinical settings (e.g., *“Predicting therapist empathy…”*), illustrating a long-standing engagement with applied ML that later informs his systems-for-AI agenda.\n\n---\n\n## Notable Works\n- **{INFaaS}: Automated model-less inference serving (2021)** — Introduced a high-impact approach to <u>automated inference serving</u> that removes the need for developers to manually select among thousands of model variants, instead managing hardware/latency/cost trade-offs within the serving system itself.\n- **From laptop to lambda: Outsourcing everyday jobs to thousands of transient functional containers (2019)** — Developed *gg*, demonstrating how <u>serverless parallelism</u> can make everyday workloads (e.g., compilation, testing, encoding) near-interactive by exploiting thousands of short-lived cloud-function containers.\n- **Faa$t: A transparent auto-scaling cache for serverless applications (2021)** — Proposed a transparent caching layer enabling <u>stateful serverless</u> execution patterns by reducing reliance on remote storage and improving performance for a broader class of FaaS applications.\n\n---\n\n## Academic Background\nBased on his publication trajectory and affiliation, Romero’s academic career has been anchored in **computer systems research** with strong cross-disciplinary ties to **databases** and **AI**, culminating in his role at the Georgia Institute of Technology. His record—totaling **1,457 citations**—reflects sustained influence in the systems community, particularly in serverless computing, inference serving, and video analytics, with multiple widely cited papers from 2018–2024 that indicate continuous leadership in emerging cloud and AI infrastructure themes. His collaborations and topic evolution suggest affiliations with research groups focused on cloud systems and data-centric AI, and his early applied-ML publications (2015) point to formative experience in machine learning for language and behavioral analysis before pivoting toward large-scale, production-oriented infrastructure for ML and analytics."}, "mqpjAt4AAAAJ": {"keywords": "Artificial Intelligence, Computer Vision, Deep Learning, Machine Learning, Unsupervised Domain Adaptation, Adversarial Learning, Domain Generalization, Transfer Learning, Vision Transformers, Synthetic-to-Real (Sim2Real) Adaptation", "summary": "## Overview\nJudy Hoffman is an Associate Professor at the University of California, Irvine, where she leads a research program at the intersection of **Artificial Intelligence**, **Computer Vision**, and **Machine Learning**. Her work centers on building *reliable and transferable* visual systems that can operate beyond curated training conditions, with a particular emphasis on <u>domain adaptation</u>, <u>robust representation learning</u>, and the practical realities of dataset bias and distribution shift. Across highly cited contributions spanning deep feature reuse, adversarial learning, and modern transformer efficiency, Hoffman’s research advances **deep learning** methods that remain effective when models encounter new environments, sensors, or populations.\n\n---\n\n## Research Areas\nHoffman’s research is best known for foundational and widely adopted approaches to <u>visual domain adaptation</u> and <u>domain-invariant representation learning</u>. Early work on deep transfer demonstrated that even strong supervised CNNs retain measurable dataset bias, motivating objectives that explicitly encourage invariance across domains (e.g., *“Deep Domain Confusion: Maximizing for Domain Invariance”*). She then helped establish adversarial formulations for unsupervised adaptation—most prominently in *“Adversarial Discriminative Domain Adaptation”*—showing how adversarial learning can align source and target feature distributions without target labels. Her contributions also bridge representation alignment with image-space translation, as in *“CyCADA: Cycle-Consistent Adversarial Domain Adaptation,”* which couples cycle-consistent mappings with task losses to adapt across unpaired domains (notably for synthetic-to-real transfer). Complementing algorithmic advances, Hoffman has shaped community evaluation through benchmark and challenge efforts such as *VisDA*, providing large-scale testbeds that reflect realistic domain shift. Beyond adaptation, her work spans dense prediction under shift (e.g., *“FCNs in the Wild”*), robustness and stability (e.g., *Jacobian regularization* and adversarial training variants), fairness and accountability in vision (e.g., *“Predictive Inequity in Object Detection”*), and modern efficiency/architecture research in transformers (e.g., *Token Merging (ToMe)* and related acceleration for diffusion models). More recently, she has contributed to large-scale multimodal video understanding and embodied learning datasets/benchmarks (e.g., *Ego-Exo4D*), extending her long-standing focus on generalization to complex, real-world activity settings.\n\n---\n\n## Notable Works\n- **Adversarial unsupervised domain adaptation via discriminative alignment** — In *“Adversarial Discriminative Domain Adaptation”* (2017), Hoffman helped define a highly influential adversarial framework for aligning feature distributions across domains without target labels, establishing a practical recipe for addressing <u>domain shift</u> in recognition systems.  \n- **Cycle-consistent image-space + feature-space adaptation for synthetic-to-real transfer** — In *“CyCADA: Cycle-Consistent Adversarial Domain Adaptation”* (2018), she advanced adaptation by integrating cycle-consistent translation with task-driven constraints, enabling effective transfer between unpaired visual domains and strengthening performance in challenging simulation-to-reality settings.  \n- **Deep feature reuse for generic recognition** — In *“DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition”* (2013), Hoffman contributed to early evidence that deep CNN activations can serve as powerful, transferable features for diverse downstream tasks, helping catalyze the modern paradigm of **pretrained representation** reuse.\n\n---\n\n## Academic Background\nHoffman’s publication trajectory reflects sustained leadership in academic computer vision and machine learning, with influential work spanning early deep feature transfer (*DeCAF*, 2013) through principled domain adaptation objectives (*Deep Domain Confusion*, 2014), adversarial adaptation (*ADDA*, 2017), and benchmark-building for the field (*VisDA*, 2017–2018). Her role in creating widely used datasets and toolkits, alongside high-impact methodological contributions, indicates strong engagement with the broader research community through collaborative, multi-institution efforts and shared evaluation infrastructure. As an Associate Professor at UC Irvine with more than **37,000 citations**, she is widely recognized for shaping how researchers study and measure <u>generalization under distribution shift</u>, while also expanding into adjacent areas such as fairness in detection (*Predictive Inequity in Object Detection*, 2019), diagnostic evaluation (*TIDE*, 2020), and efficient transformer-based vision systems (*Token Merging*, 2022) that support scalable deployment and experimentation."}, "n6zdTxUAAAAJ": {"keywords": "Learning Sciences, Participatory Design, Co-design with Families and Children, Learning Technologies in Low-Resource Contexts, MOOCs and Online Learning Accessibility, English Language Learners (ELL/ESL), K-12 Artificial Intelligence Education, Culturally Responsive Computing Education, Educational Games and Robotics for CS Learning, Computing Ethics and Responsible AI", "summary": "## Overview\nJudith Odili Uchidiuno is a researcher at the **Georgia Institute of Technology**, where she works at the intersection of the **Learning Sciences** and human-centered computing to design, study, and evaluate technologies that broaden participation in learning across diverse settings. Her scholarship emphasizes *context-sensitive* design and implementation, with particular attention to <u>equity</u>, <u>co-design</u> with learners and community stakeholders, and the ways social relationships shape learning experiences. Across school, home, afterschool, and global online environments, Uchidiuno’s work foregrounds **learner motivation**, **engagement**, and *culturally grounded* pedagogical practice as essential design constraints for effective educational technology.\n\n---\n\n## Research Areas\nUchidiuno’s research spans three tightly connected areas. First, she advances <u>participatory and co-design methodologies</u> that account for the social dynamics of learning partnerships, especially in adult–child and community-based design contexts. In “**The evolution of engagements and social bonds during child-parent co-design**” (2016), she examines how engagement and relational bonds develop over time in family co-design, complementing later work that interrogates how institutional culture and identity shape co-design participation and outcomes (e.g., “**‘I’m a little less inclined to do it’: How Afterschool Programs’ Culture Impact Co-Design Processes and Outcomes**,” 2023; “**‘What’s Your Name Again?’: How Race and Gender Dynamics Impact Codesign Processes and Output**,” 2023; and reflections on bias negotiation in “**Negotiating Systemic Racial and Gender Bias as a Minoritized Adult Design Researcher**,” 2021). Second, she investigates <u>learning at scale and global accessibility</u> in online environments, focusing on English Language Learners (ELLs) in MOOCs—linking motivational factors and behavioral traces in video learning (e.g., “**Going global: Understanding English language learners’ student motivation in English-language MOOCs**,” 2018; “**How do English language learners interact with different content types in MOOC videos?**,” 2018; and related work on identifying ESL speakers via “**Browser language preferences as a metric for identifying ESL speakers in MOOCs**,” 2016). Third, she develops and studies <u>equitable learning technologies</u> across under-resourced and culturally distinct contexts, including rural Tanzanian classrooms and homes, where she analyzes how setting, pedagogy, and teacher practices condition technology uptake and student engagement (e.g., “**Designing Appropriate Learning Technologies for School vs Home Settings in Tanzanian Rural Villages**,” 2018; “**Learning from African Classroom Pedagogy to Increase Student Engagement in Education Technologies**,” 2019; and teacher-centered perspectives in 2021 work on rural classrooms). More recently, her portfolio extends to <u>K–12 AI and computing education</u> through co-designed curricula and learning activities (e.g., “**Co-designing an AI curriculum with university researchers and middle school teachers**,” 2022; “**Lessons learned from teaching artificial intelligence to middle school students**,” 2022), alongside emerging contributions to computing ethics education and inclusive curricular design (2024–2025).\n\n---\n\n## Notable Works\n- **Family-centered co-design processes and relational learning dynamics:** In “**The evolution of engagements and social bonds during child-parent co-design**” (2016), Uchidiuno provides an empirical account of how engagement patterns and social bonds change during extended family co-design, establishing a methodological foundation for studying *relationship-building* as a design outcome in learning technology development.\n- **MOOC accessibility for English Language Learners through motivation and behavioral evidence:** Through “**Going global: Understanding English language learners’ student motivation in English-language MOOCs**” (2018) and “**How do English language learners interact with different content types in MOOC videos?**” (2018), she connects motivational constructs to observable interaction patterns, informing interventions aimed at <u>equitable participation</u> in large-scale online learning.\n- **Context-responsive educational technology design in rural East Africa:** In “**Designing Appropriate Learning Technologies for School vs Home Settings in Tanzanian Rural Villages**” (2018) and “**Learning from African Classroom Pedagogy to Increase Student Engagement in Education Technologies**” (2019), she demonstrates how differences in setting and pedagogy shape engagement, arguing for *locally grounded* design principles rather than “one-size-fits-all” deployments.\n\n---\n\n## Academic Background\nAffiliated with the **Georgia Institute of Technology**, Uchidiuno’s academic trajectory reflects sustained interdisciplinary engagement across the Learning Sciences, HCI, and education technology research communities, with a publication record spanning connected learning (e.g., work involving ScienceKit, 2015), global online learning and MOOC accessibility (2016–2018), and field-based design research in Sub-Saharan Africa—particularly Tanzania—focused on early literacy, classroom pedagogy, and rural learning ecologies (2018–2021). Her more recent scholarship signals an expansion into **K–12 AI education** via long-term partnerships with middle school teachers and districts (2022–2025) and into **computing ethics education** (2024–2025), consistent with a broader commitment to <u>responsible</u> and <u>inclusive</u> computing curricula. With **563 citations**, her work has achieved notable scholarly uptake, especially in research on participatory/co-design with families, equitable learning technology design in under-resourced contexts, and improving access for ELL learners in MOOCs—areas that collectively position her as a contributor to both methodological and applied questions in *equity-centered* learning sciences research."}, "nfkH5V4AAAAJ": {"keywords": "security, systems, networking, machine learning, applied cryptography, intrusion detection systems (IDS), anomaly detection, botnet detection, malware analysis, DNS-based threat intelligence", "summary": "## Overview\nWenke Lee is a **Regents' Professor** and the **John P. Imlay Jr. Chair** in the College of Computing at **Georgia Tech**, where he leads research at the intersection of **computer security**, **systems**, and **networking**. His work has consistently advanced *data-driven* and *operationally grounded* defenses, with a primary focus on <u>intrusion detection</u>, <u>malware and botnet analysis</u>, and the use of **machine learning** and **data mining** to build *adaptive* security mechanisms. With **51,961 citations**, Lee’s scholarship has helped shape how the community measures, models, and mitigates attacks in real-world networked environments.\n\n---\n\n## Research Areas\nLee’s research spans foundational and applied security problems, unified by an emphasis on learning from system and network telemetry. A major throughline is <u>data mining for intrusion detection</u>, where he developed systematic approaches for extracting behavioral patterns from audit and traffic data to produce detection models that are accurate and extensible, as articulated in “**Data mining approaches for intrusion detection**” (1998) and “**A data mining framework for building intrusion detection models**” (1999). His work also addresses security in *mobile and decentralized networks*, including vulnerabilities unique to MANETs and wireless settings, explored in “**Intrusion detection in wireless ad-hoc networks**” (2000) and later expanded in “**Intrusion detection techniques for mobile wireless networks**” (2003) and cooperative detection architectures. In the malware domain, Lee has made influential contributions to <u>botnet detection</u> and <u>network-based malware analytics</u>, proposing protocol- and structure-independent approaches in “**BotMiner**” (2008) and command-and-control channel identification in “**BotSniffer**” (2008), alongside infection-dialog correlation in “**BotHunter**” (2007). Complementing detection, he advanced *secure monitoring and analysis* via virtualization—e.g., “**Ether: malware analysis via hardware virtualization extensions**” (2008) and related work on VM introspection—while also contributing to DNS-based threat intelligence and DGA/malicious domain detection (e.g., Kopis and subsequent domain-focused systems). More recently, his portfolio includes security implications of modern ML and media synthesis, reflected in “**The creation and detection of deepfakes: A survey**” (2021), which synthesizes attack vectors and defenses for a rapidly evolving threat landscape.\n\n---\n\n## Notable Works\n- **Pioneering data mining for intrusion detection**: Established a systematic, learning-based paradigm for IDS construction in “**Data mining approaches for intrusion detection**” (1998) and “**A data mining framework for building intrusion detection models**” (1999), emphasizing feature discovery from audit data and model updates to address evolving attacks and environments.  \n- **Protocol- and structure-independent botnet detection**: Introduced clustering-based detection that generalizes across botnet command-and-control designs in “**BotMiner: clustering analysis of network traffic for protocol-and structure-independent botnet detection**” (2008), significantly influencing later work on behavior-based network defense.  \n- **Comprehensive synthesis of deepfake threats and defenses**: Co-authored “**The creation and detection of deepfakes: A survey**” (2021), a highly cited reference that organizes generative media methods, detection strategies, and security risks such as misinformation and impersonation.\n\n---\n\n## Academic Background\nAs a senior faculty member at **Georgia Tech’s College of Computing**, Lee’s career reflects sustained leadership in security research across **networking**, **systems**, and **machine learning–driven** defense. His early and highly cited work in the late 1990s and early 2000s—spanning audit-based intrusion detection, information-theoretic anomaly detection, and cost-sensitive/security-management considerations—suggests deep roots in *data mining* and *statistical modeling* applied to operational security problems, followed by major contributions to malware, botnets, and virtualization-based monitoring as those threats rose to prominence. His appointment as a **Regents' Professor** and endowed chair holder indicates significant recognition for research impact and scholarly distinction, reinforced by extensive citation influence and long-running contributions to core security topics (IDS, botnet C&C, malware analysis, DNS abuse, and emerging media manipulation). Across these phases, his affiliations and collaborations are anchored in Georgia Tech’s security research ecosystem, with a publication trajectory that tracks—and often anticipates—key shifts in the threat landscape and defensive methodology."}, "nuiUbpsAAAAJ": {"keywords": "Networking, Computer Networks, Systems, Mobile Computing, Device-free Passive Localization, WiFi Sensing, Datacenter Transport and Congestion Control, Software Packet Scheduling and Traffic Shaping, Overload Control for Microservices RPCs, LEO Satellite Network Performance Modeling", "summary": "## Overview\nAhmed Saeed is an Assistant Professor at **Georgia Tech**, where he leads a research program at the intersection of **computer networking** and **systems** with an emphasis on *high-performance, dependable networked computing*. His work spans end-host networking, datacenter transport and scheduling, and emerging network environments, with a sustained focus on turning foundational insights into practical mechanisms. Across his publications, Saeed has advanced <u>scalable traffic shaping</u>, <u>congestion control</u>, and <u>device-free wireless sensing</u>, combining rigorous measurement with system design to improve performance, robustness, and deployability in real-world networks.\n\n---\n\n## Research Areas\nSaeed’s research portfolio bridges wireless sensing and modern datacenter/Internet infrastructure. In wireless and mobile computing, he is widely known for pioneering **WLAN device-free passive (DfP) localization** and motion detection, where entities are localized without carrying any device, leveraging ambient changes in WiFi signals; this line includes *Nuzzer* (2012), *RASID* (2012), and *Ichnaea* (2013), which collectively tackle accuracy, robustness to environmental dynamics, and operational overhead in real deployments. In systems and networking, he has developed mechanisms for **end-host traffic shaping** and **software packet scheduling**, exemplified by *Carousel* (2017) and *Eiffel* (2019), addressing the scalability constraints of pacing, rate limiting, and scheduling under high flow counts and demanding performance targets. His work on cloud and datacenter performance further includes dual-loop control for traffic aggregates (*Annulus*, 2020) and overload control for microservice RPCs (*Breakwater*, 2020), targeting tail latency and stability under overload. More recently, he has contributed to *formal and verification-inspired approaches* to network control (*Toward formally verifying congestion control behavior*, 2021; *A Performance Verification Methodology for Resource Allocation Heuristics*, 2023) and to measurement-driven understanding of <u>LEO satellite network variability</u> (*A Characterization of Route Variability in LEO Satellite Networks*, 2023; *Astrolabe*, 2023), expanding networking theory and tooling to new operational regimes.\n\n---\n\n## Notable Works\n- **Device-free WiFi localization at scale:** Introduced and validated large-scale DfP localization using existing WLAN infrastructure in **“Nuzzer: A large-scale device-free passive localization system for wireless environments” (2012)**, establishing a foundational approach for practical, device-free indoor localization.\n- **Robust DfP motion detection and localization:** Advanced robustness to environmental changes and reduced operational friction in **“RASID: A robust WLAN device-free passive motion detection system” (2012)** and **“Ichnaea: A low-overhead robust WLAN device-free passive localization system” (2013)**, helping move DfP techniques from controlled settings toward deployable systems.\n- **Scalable end-host traffic shaping and scheduling:** Developed efficient, scalable mechanisms for shaping and ordering packets at end hosts, notably **“Carousel: Scalable traffic shaping at end hosts” (2017)** and **“Eiffel: Efficient and Flexible Software Packet Scheduling” (2019)**, influencing how modern systems implement pacing, rate limiting, and policy-driven scheduling in software.\n\n---\n\n## Academic Background\nSaeed’s publication trajectory indicates an early foundation in **mobile and wireless systems**—including multi-interface scheduling (*DNIS*, 2010) and a highly cited sequence of WLAN device-free sensing papers (2012–2013)—followed by an expansion into **networked systems for clouds and datacenters**, where he has addressed traffic shaping, packet scheduling, congestion control, and overload management (2017–2021). His affiliations and collaborations, reflected in work spanning IoT architecture (*Vision: The Case for Symbiosis in the Internet of Things*, 2015), spectrum/TV white spaces (2014–2017), and cyber-physical sensing with UAVs (2014–2019), suggest sustained interdisciplinary engagement across networking, mobile computing, and systems. With **1913 total citations**, his impact is anchored by influential contributions to device-free wireless localization and to scalable datacenter/end-host networking mechanisms, and his more recent work—on verification methodologies and LEO satellite network characterization—signals an ongoing commitment to building *principled, measurement-grounded* tools and models for emerging network environments."}, "oZNdAREAAAAJ": {"keywords": "Networking, Systems, Security, Internet traffic classification, Network measurement, Traffic generation and workload modeling, BGP routing security and hijacking, Denial-of-Service (DDoS) attacks, Internet censorship and shutdown measurement, Internet background radiation and darknet analysis", "summary": "## Overview\nAlberto Dainotti is a researcher at **Georgia Tech**, where he contributes to an Internet measurement–driven research agenda at the intersection of **networking**, **systems**, and **security**. His work centers on turning large-scale, heterogeneous network data into actionable understanding of Internet behavior and risk, with particular emphasis on *empirical* and *operationally grounded* methods. Across highly cited contributions on traffic identification, routing security, and outage analysis, Dainotti has advanced the study of <u>Internet-scale measurement</u> and <u>data-driven network security</u>, developing both foundational analyses and widely used tooling that helps researchers and operators observe, diagnose, and mitigate real-world network phenomena.\n\n---\n\n## Research Areas\nDainotti’s research spans three tightly connected themes: (1) **traffic classification and modeling**, (2) **Internet security measurement**, and (3) **routing and infrastructure resilience**. In traffic identification, he has examined the methodological and practical limits of classification in modern networks—highlighting challenges, evaluation pitfalls, and future needs in “**Issues and future directions in traffic classification**” (2012) and synthesizing the field in “**Reviewing traffic classification**” (2013). He also proposed privacy- and cost-aware approaches that move beyond deep packet inspection, such as “**PortLoad: taking the best of two worlds in traffic classification**” (2010), and explored statistical and sequence-based methods via “**Internet traffic modeling by means of Hidden Markov Models**” (2008) and “**Classification of network traffic via packet-level hidden markov models**” (2008). Complementing these analytical directions, he has contributed measurement and experimentation infrastructure, notably the D-ITG line of work—e.g., “**Multi-protocol and multi-platform traffic generation and measurement**” (2007) and “**A tool for the generation of realistic network workload for emerging networking scenarios**” (2012)—which addresses the difficulty of reproducing realistic Internet workloads for systems and networking studies. In security and resilience, Dainotti has produced Internet-wide characterizations of adversarial ecosystems and disruptions, including macroscopic perspectives on denial-of-service in “**Millions of targets under attack: a macroscopic characterization of the DoS ecosystem**” (2017) and politically salient disruption events in “**Analysis of country-wide internet outages caused by censorship**” (2011). His later work extends to inter-domain routing security and operational defenses, contributing to measurement frameworks and rapid mitigation approaches such as “**BGPStream: a software framework for live and historical BGP data analysis**” (2016) and “**ARTEMIS: Neutralizing BGP hijacking within a minute**” (2018), reflecting a sustained focus on <u>operational security for Internet infrastructure</u>.\n\n---\n\n## Notable Works\n- **Shaping the research agenda for modern traffic identification** through a widely cited synthesis of open problems, deployment constraints, and methodological challenges in “**Issues and future directions in traffic classification**” (2012), which helped define how the community evaluates and operationalizes traffic classification for QoS, accounting, and security use cases.\n- **Building practical, reusable measurement infrastructure** for realistic experimentation and active measurements, most prominently via “**A tool for the generation of realistic network workload for emerging networking scenarios**” (2012) (and the broader D-ITG ecosystem), enabling controlled yet representative testing across diverse protocols and platforms.\n- **Establishing empirical methods to analyze large-scale disruptions and their causes**, exemplified by “**Analysis of country-wide internet outages caused by censorship**” (2011), which leveraged multiple large-scale data sources to study national connectivity disruptions and laid groundwork for later outage and shutdown measurement efforts.\n\n---\n\n## Academic Background\nBased at **Georgia Tech**, Dainotti’s publication record reflects a sustained trajectory in Internet measurement and security spanning from early work on packet-level characterization and anomaly detection (e.g., wavelet-based DoS detection) to later contributions on Internet-wide security ecosystems, censorship/outage analysis, and BGP security and mitigation systems. His high-impact output—supported by substantial scholarly uptake (reported **6,596 citations**)—suggests strong standing within the networking and security research communities and consistent engagement with venues and communities focused on passive/active measurement and operational security. The combination of methodological surveys (e.g., traffic classification), open and community-oriented platforms (e.g., D-ITG-related tooling and frameworks such as BGPStream), and empirically grounded security studies indicates a profile characterized by both *foundational scholarship* and *operator-relevant systems research*, with affiliations and collaborations naturally aligned with leading Internet measurement and network security ecosystems."}, "of5e53cAAAAJ": {"keywords": "human-computer interaction, usability engineering, user-centered design, human factors, supervisory control, large-scale systems monitoring, fault diagnosis and troubleshooting, human performance measurement, cognitive modeling of operators, interaction design education", "summary": "## Overview\nRichard Henneman is a researcher affiliated with the **College of Computing** at the Georgia Institute of Technology, with professional ties to Georgia Tech’s *Human-Computer Interaction* community (including the Master of Science in Human-Computer Interaction ecosystem). His work centers on **human-computer interaction (HCI)** and **human factors**, with a sustained emphasis on <u>usability</u> and the *measurement of human performance* in complex, high-stakes operational settings. Across a publication record spanning foundational studies of supervisory control in large-scale systems to later contributions on design education and usability practice, Henneman has advanced an evidence-driven approach to <u>human performance measurement</u> and **user-centered design**, reflected in an overall citation impact of **~440 citations**.\n\n---\n\n## Research Areas\nHenneman’s research areas bridge *applied usability engineering* and *cognitive/behavioral measurement* for complex sociotechnical systems. A central thread is the articulation of usability as a measurable system attribute—defined through effectiveness, efficiency, and satisfaction—and the organizational consequences of poor usability (e.g., error rates, training time, and support costs), as synthesized in “**Design for usability: Process, skills, and tools**” (1999). Complementing this practice-oriented lens, he developed and reviewed rigorous approaches for evaluating human performance in demanding diagnostic and control contexts. In “**Measures of human problem solving performance in fault diagnosis tasks**” (2012), he surveys and operationalizes a wide set of measures—spanning ability/aptitude, cognitive style, and task outcomes—grounded in experimental fault-diagnosis studies. He also examines how system properties shape operator workload and strategy in supervisory control, notably in “**On measuring the complexity of monitoring and controlling large-scale systems**” (2007), which contrasts behavioral and nonbehavioral complexity measures using simulated network environments. In later work, he extends his HCI focus to the pedagogical and disciplinary infrastructure of design, helping define a research agenda for teaching interaction design in “**Design in the HCI classroom: Setting a research agenda**” (2019), aligning educational practice with the expanding professional landscape of UX and interaction design.\n\n---\n\n## Notable Works\n- **Codified a pragmatic, outcomes-focused framework for usability practice** in “*Design for usability: Process, skills, and tools*” (1999), clarifying usability in terms of effectiveness, efficiency, and satisfaction while linking poor usability to measurable organizational costs (e.g., errors, training, and support burden).  \n- **Advanced measurement methodology for diagnostic problem solving** through “*Measures of human problem solving performance in fault diagnosis tasks*” (2012), evaluating a broad portfolio of performance and individual-difference measures using evidence from controlled experiments in fault diagnosis.  \n- **Formalized and compared approaches to system complexity in supervisory control** in “*On measuring the complexity of monitoring and controlling large-scale systems*” (2007), integrating literature across behavioral and structural perspectives and demonstrating measurement strategies via a simulated large-scale network task environment.\n\n---\n\n## Academic Background\nBased on his long-running publication trajectory—from early studies of human supervision in hierarchical, large-scale dynamic systems (1980s) to usability engineering, interface technologies, and HCI education (1990s–2010s)—Henneman’s academic profile reflects deep training and sustained scholarship in **human factors**, **cognitive engineering**, and **HCI**. His early work on monitoring, control, and fault diagnosis suggests strong interdisciplinary grounding in experimental methods and applied systems research, while later publications on usability process, cost-justification (e.g., “Design of a human factors cost-justification tool,” 1994), and organizational practice indicate engagement with industry-facing research and the institutionalization of user-centered methods. His affiliation with Georgia Tech’s **College of Computing** aligns his contributions with a leading academic HCI environment, and his citation record (~440) indicates recognized influence across both *research* and *practice* communities, spanning usability engineering, performance measurement, and the evolving agenda for design instruction in HCI programs."}, "p2fibvQAAAAJ": {"keywords": "Computer networks, Network security, Internet measurements, Internet policy, BGP routing security, BGP hijacking detection, RPKI (ROA/ROV) deployment, Internet routing registries (IRR) and routing policy, Autonomous system (AS) and organization mapping, Internet governance and routing security incentives", "summary": "## Overview\nCecilia Testart is a researcher at **Georgia Tech**, where she works at the intersection of **computer networks** and **network security**, with an emphasis on *empirical, measurement-driven* understanding of the modern Internet’s control plane. Her scholarship centers on <u>inter-domain routing security</u>—especially the practical realities of **BGP** vulnerabilities and defenses—and on translating technical evidence into *actionable* insights for operators and decision-makers. Across her body of work, Testart combines **Internet measurements**, longitudinal data analysis, and policy-aware framing to evaluate why routing incidents persist, how security mechanisms are (and are not) adopted, and what interventions can meaningfully improve <u>global routing resilience</u>.\n\n---\n\n## Research Areas\nTestart’s research is anchored in measuring and improving the security of the Internet’s routing ecosystem, particularly **BGP hijacking** and the socio-technical barriers that slow deployment of defenses. In “**Profiling BGP serial hijackers: capturing persistent misbehavior in the global routing table**” (2019), she advances a longitudinal perspective on hijacking by characterizing recurring offenders and persistent anomalous behavior visible in global routing data. A complementary line of work evaluates <u>cryptographic route origin authorization</u> via **RPKI** and operational filtering: “**To Filter or not to Filter: Measuring the Benefits of Registering in the RPKI Today**” (2020) quantifies the security benefits and practical tradeoffs of RPKI registration and filtering choices, while later studies such as “**IRR Hygiene in the RPKI Era**” (2022) and “**Irregularities in the internet routing registry**” (2023) analyze the continued importance—and quality problems—of **IRR** databases that underpin real-world routing policy and filtering. She also measures community-led and institutional interventions, including “**Mind your MANRS: measuring the MANRS ecosystem**” (2022), which assesses adoption signals and effectiveness of operator norms intended to improve routing security. Beyond routing mechanisms, Testart contributes to Internet governance and policy-facing analysis—e.g., “**Understanding ICANN's Complexity in a Growing and Changing Internet**” (2014)—and to bridging technical systems with societal needs, as in “**Explaining explanations to society**” (2019), which critiques mismatches between XAI explanation practices and the forms of explanation demanded by policymakers and the public. More recent work extends her measurement agenda into organizational mapping and Internet economics/operations, including analyses of AS “administrative vs. operational” realities (“**The parallel lives of autonomous systems: ASN allocations vs. BGP**,” 2021) and emergent markets such as IPv4 leasing (“**Sublet your subnet: Inferring IP leasing in the wild**,” 2024), reinforcing a consistent focus on <u>data-driven Internet security and policy</u>.\n\n---\n\n## Notable Works\n- **Profiling persistent routing attackers:** In “**Profiling BGP serial hijackers: capturing persistent misbehavior in the global routing table**” (2019), Testart introduces a longitudinal lens on BGP hijacking, highlighting recurring misbehavior patterns and enabling a more systematic understanding of repeat offenders than purely event-by-event detection.\n- **Quantifying real-world benefits of RPKI adoption:** “**To Filter or not to Filter: Measuring the Benefits of Registering in the RPKI Today**” (2020) provides evidence-based evaluation of RPKI registration and filtering, clarifying when and how <u>RPKI/ROA deployment</u> translates into practical security gains.\n- **Measuring ecosystem interventions and operational hygiene:** Through “**Mind your MANRS: measuring the MANRS ecosystem**” (2022) and “**IRR Hygiene in the RPKI Era**” (2022), she assesses both voluntary norms (MANRS) and foundational routing registries (IRR), documenting adoption, gaps, and data-quality challenges that shape day-to-day routing security outcomes.\n\n---\n\n## Academic Background\nBased at **Georgia Tech**, Testart’s publication record indicates an academic trajectory grounded in **Internet measurement**, **routing security**, and Internet governance, with sustained engagement across both technical venues and policy-adjacent conversations. Her early work on institutional and governance complexity (e.g., ICANN-focused analysis in 2014 and broader institutional landscape studies in 2016) foreshadows a career-long emphasis on how decentralized authority and incentives shape security outcomes. From 2018 onward, her scholarship concentrates on BGP’s longstanding vulnerabilities (“**Reviewing a Historical Internet Vulnerability: Why Isn't BGP More Secure and What Can We Do About it?**,” 2018) and on empirically evaluating deployment realities for defenses such as **RPKI** and registry-based filtering, reflecting close alignment with operator practices and standardization concerns. With **368 total citations** and multiple highly cited papers on routing security and adoption measurement, she is affiliated with a research community that spans networking, security, and Internet policy, and her recent dataset- and tooling-oriented outputs (e.g., organizational mapping and RPKI adoption studies) suggest an ongoing commitment to building <u>reproducible measurement foundations</u> for the field."}, "ptGYJiEAAAAJ": {"keywords": "machine learning, computational and statistical neuroscience, Bayesian neural networks, variational inference, Gaussian processes, latent variable models, neural population dynamics, spike train analysis, semi-supervised animal pose tracking, inverse reinforcement learning", "summary": "## Overview\nAnqi Wu is an Assistant Professor in **Computational Science and Engineering** at **Georgia Tech**, where she leads a research program at the intersection of **machine learning** and *computational and statistical neuroscience*. Her work centers on building **probabilistic** and **generative** modeling frameworks that can extract <u>low-dimensional latent structure</u> from complex biological and behavioral data while providing <u>principled uncertainty quantification</u>. Across neural spiking, fMRI, and video-based behavior, Wu’s lab develops *scalable inference* and *structured priors* that turn high-dimensional observations into interpretable latent dynamics and actionable scientific hypotheses.\n\n---\n\n## Research Areas\nWu’s research spans three tightly connected areas: (1) **Bayesian deep learning and variational inference**, (2) **latent variable modeling for neural population activity**, and (3) **computational methods for behavior and brain–behavior alignment**. In Bayesian deep learning, her highly cited work on *robust Bayesian neural networks* introduced **deterministic variational inference** strategies that make <u>approximate Bayesian inference</u> more stable and practical for deep models (“Deterministic variational inference for robust bayesian neural networks,” 2018). In computational neuroscience, she has repeatedly advanced **nonlinear latent structure discovery** using **Gaussian processes** to model both latent dynamics and nonlinear observation mappings in multivariate spike trains (“Gaussian process based nonlinear latent structure discovery in multivariate spike train data,” 2017), and later extended these ideas toward discovering neural dynamics with hybrid GP–RNN formulations (“Neural dynamics discovery via gaussian process recurrent neural networks,” 2020). A complementary thread addresses *community-wide evaluation and reproducibility* in neural data science through benchmark design and model comparison (“Neural latents benchmark'21: evaluating latent variable models of neural population activity,” 2021). Wu also develops methods that connect neural signals to behavior: semi-supervised generative approaches for parsing variability in behavioral videos (“Partitioning variability in animal behavioral videos using semi-supervised variational autoencoders,” 2021), and structured graphical/deep models for pose tracking that exploit temporal and spatial constraints (“Deep Graph Pose: a semi-supervised deep graphical model for improved animal pose tracking,” 2020). More recently, her work expands to <u>multi-region neural communication</u> with scalable state-space GP formulations (“Multi-region markovian gaussian process…,” 2024) and to emerging generative paradigms (e.g., diffusion-based models) for alignment and disentanglement in neural dynamics (“Extraction and recovery of spatio-temporal structure… with diffusion models,” 2023; “Exploring behavior-relevant and disentangled neural dynamics with generative diffusion models,” 2024). Parallel contributions in **Bayesian optimization** (“Exploiting gradients and Hessians in Bayesian optimization and Bayesian quadrature,” 2017) and earlier work in **time-series learning** with DTW constraint learning (“Dynamic time warping constraint learning…,” 2011) highlight a long-standing focus on *structured learning for sequential data*.\n\n---\n\n## Notable Works\n- **Robust uncertainty-aware deep learning via variational Bayes:** Developed practical, stable <u>deterministic variational inference</u> methods for **Bayesian neural networks**, strengthening robustness and uncertainty estimation in deep models (Wu et al., “**Deterministic variational inference for robust bayesian neural networks**,” 2018).\n- **Nonlinear latent variable modeling for spike trains:** Introduced a doubly nonlinear **Gaussian process** framework to uncover <u>low-dimensional latent structure</u> in multivariate neural spiking with nonlinear dynamics and nonlinear observation mappings (Wu et al., “**Gaussian process based nonlinear latent structure discovery in multivariate spike train data**,” 2017).\n- **Benchmarking latent variable models for neural population activity:** Co-developed a widely used evaluation suite that systematizes comparison across latent variable models, datasets, and metrics, accelerating reproducible progress in <u>neural representation learning</u> (Wu et al., “**Neural latents benchmark'21: evaluating latent variable models of neural population activity**,” 2021).\n\n---\n\n## Academic Background\nWu’s publication trajectory suggests a multidisciplinary academic development spanning **machine learning**, **time-series analysis**, and **computational neuroscience**, with early influential work on *sequence learning* and distance-based methods for time series (e.g., DTW constraint learning in 2011) and subsequent deep engagement with **probabilistic modeling** and **Gaussian process** methodology (including Bayesian optimization and structured inference). Her later body of work reflects sustained collaboration with neuroscience and neuroethology communities, addressing neural spiking, fMRI, and behavioral video—often through **semi-supervised** and **Bayesian** frameworks that emphasize <u>interpretability</u> and <u>uncertainty</u>. Now at Georgia Tech as an **Assistant Professor**, she is affiliated with the broader computational and data-driven neuroscience ecosystem through contributions such as **Neural Latents Benchmark’21**, and her impact is evidenced by substantial scholarly uptake (approximately **1,290 citations**) across both methodological machine learning and applied neural data science."}, "puNJcv0AAAAJ": {"keywords": "Human-Computer Interaction, Computer-Supported Cooperative Work, Autism spectrum disorder (ASD), Neurodiversity and inclusive workplaces, Assistive technology for independent living, Virtual reality (VR) social skills training, Medical crowdfunding and online support communities, Wearable physiological sensing (EDA/skin conductance), Human-AI interaction with LLM chatbots in mental health, Participatory design and co-design with disabled users", "summary": "## Overview\nJennifer G. Kim is a researcher at the Georgia Institute of Technology whose work sits at the intersection of **Human-Computer Interaction (HCI)** and **Computer-Supported Cooperative Work (CSCW)**. Across her scholarship, she designs and studies sociotechnical systems that enable *supportive, dignified, and effective* participation in everyday life—particularly for autistic and other neurodivergent people and for individuals navigating health- and care-related challenges online. Her contributions emphasize <u>participatory and person-centered design</u>, combining qualitative inquiry with system-building to translate lived experience into actionable design knowledge for <u>social computing</u>, <u>accessibility</u>, and <u>technology-mediated care</u>.\n\n---\n\n## Research Areas\nKim’s research spans three tightly connected areas. First, she advances **technology to support autistic and neurodivergent people** during major life transitions and in daily independence-building, drawing from early work on social support networks such as “Designing a social network to support the independence of young adults with autism” (2012) and “Investigating the use of circles in social networks to support independence of individuals with autism” (2013), as well as recent workplace-focused studies including “Designing for strengths: Opportunities to support neurodiversity in the workplace” (2024) and “Understanding online job and housing search practices of neurodiverse young adults to support their independence” (2024). Second, she examines **online health communities and medical crowdfunding** as sites where credibility, identity, and social support are negotiated—articulated in “The power of collective endorsements: credibility factors in medical crowdfunding campaigns” (2016), “Not by Money Alone: Social Support Opportunities in Medical Crowdfunding Campaigns” (2017), “Understanding identity presentation in medical crowdfunding” (2018), and “Enriched social translucence in medical crowdfunding” (2020). Third, Kim contributes methodological and system-oriented work on **data-informed and immersive interventions**, including physiological sensing and analytics (“Skin Conductance as an In Situ Marker for Emotional Arousal…” (2017); “BEDA: Visual analytics for behavioral and physiological data” (2013)) and VR-based training and reflection tools to foster workplace understanding and self-efficacy (“The Workplace Playbook VR” (2022); “Toward inclusive mindsets…” (2023); “V-DAT…” (2023); “Promoting self-efficacy… using virtual reality and physiological sensors” (2024)). More recently, she has extended these concerns to **AI-mediated support in sensitive domains**, interrogating benefits and risks of LLM systems in mental health and neurodivergent support contexts (“Private yet social: How LLM chatbots support and challenge eating disorder recovery” (2025); “Understanding Human-AI Misalignment in LLM-Based Job-Seeking Support for Neurodivergent Users” (2025)).\n\n---\n\n## Notable Works\n- **Social-network design for autistic young adults’ independence and support**: In “Designing a social network to support the independence of young adults with autism” (2012) and the follow-up field study “Investigating the use of circles in social networks to support independence of individuals with autism” (2013), Kim helped establish design strategies for structuring trusted networks and enabling practical, everyday support for transition-age autistic individuals and their caregivers.\n- **Credibility and social dynamics in medical crowdfunding**: Through “The power of collective endorsements: credibility factors in medical crowdfunding campaigns” (2016), complemented by “Not by Money Alone: Social Support Opportunities in Medical Crowdfunding Campaigns” (2017) and “Understanding identity presentation in medical crowdfunding” (2018), Kim articulated how endorsements, identity work, and non-monetary contributions shape participation and perceived legitimacy in health-related fundraising.\n- **Immersive and data-informed interventions for neurodiversity and workplace inclusion**: Kim’s VR and sensing-informed line of work—spanning “The Workplace Playbook VR” (2022), “Toward inclusive mindsets…” (2023), and “Promoting self-efficacy… using virtual reality and physiological sensors” (2024)—maps a design space for building <u>inclusive workplace mindsets</u> and supporting autistic people’s agency, reflection, and confidence in high-stakes social settings.\n\n---\n\n## Academic Background\nBased at the Georgia Institute of Technology and publishing consistently in HCI/CSCW-adjacent venues across more than a decade, Kim’s academic trajectory reflects sustained engagement with **user-centered and participatory design** for social computing and health-related contexts. Her early contributions (2011–2013) pair system prototyping with formative and field-based studies (e.g., crowd support for social-skill scripts and independence-oriented social networking), while mid-career work (2016–2020) demonstrates deepening specialization in **online medical support and crowdfunding** as consequential social infrastructures. In recent years (2022–2025), her scholarship shows expanded methodological range—integrating VR, multimodal sensing, and AI/LLM-centered inquiry—while maintaining a consistent commitment to <u>equity-oriented design</u> and the redistribution of responsibility from individuals to the sociotechnical systems around them (e.g., shifting inclusivity work toward neurotypical coworkers and institutions). With **784 total citations**, her publication record indicates significant visibility and influence across HCI, CSCW, and technology-for-care communities, alongside interdisciplinary collaborations touching education, health, accessibility, and workplace studies."}, "q1bkP7AAAAAJ": {"keywords": "modeling and simulation, parallel computing, distributed computing, distributed simulation, parallel discrete event simulation (PDES), Time Warp optimistic synchronization, High Level Architecture (HLA) and RTI, large-scale network simulation, vehicular ad hoc networks (VANETs) and V2V data dissemination, rollback and reverse computation techniques", "summary": "## Overview\nRichard Fujimoto is a **Regents’ Professor** at the **Georgia Institute of Technology**, where he has long been a central figure in the study of **modeling and simulation** with a particular emphasis on *scalable execution* of complex models. Working at the intersection of **parallel computing** and **distributed computing**, his research advances the theory and practice of <u>parallel and distributed discrete-event simulation</u>, enabling large, high-fidelity simulations for domains ranging from computer networks to defense training systems. Across a highly cited body of work (≈19,797 citations), he is especially recognized for foundational contributions to **parallel discrete event simulation (PDES)** and the engineering of interoperable, time-managed simulation infrastructures.\n\n---\n\n## Research Areas\nFujimoto’s research spans the core algorithmic and systems challenges that arise when discrete-event simulations must run efficiently across multiple processors or machines. A major theme is synchronization in <u>parallel discrete-event simulation</u>, including both conservative and optimistic approaches, as articulated in “**Parallel discrete event simulation**” (1990) and synthesized in the influential monograph “**Parallel and Distributed Simulation Systems**” (2000). Within optimistic simulation, he explored performance and correctness tradeoffs in Time Warp systems—e.g., “**Performance of Time Warp under synthetic workloads**” (1990), “**Time Warp on a Shared Memory Multiprocessor**” (1989), and the design of practical executors such as “**GTW: a time warp system for shared memory multiprocessors**” (1994)—along with key runtime mechanisms like <u>global virtual time</u> computation (“**Computing global virtual time in shared-memory multiprocessors**,” 1997). He also pushed state management forward through techniques that reduce rollback overhead, notably “**Efficient optimistic parallel simulations using reverse computation**” (1999), and investigated hardware/software co-design directions (“**Design and evaluation of the rollback chip**,” 2002). In distributed simulation interoperability, Fujimoto contributed to the **DoD High Level Architecture (HLA)**—including “**The department of defense high level architecture**” (1997) and “**Time management in the high level architecture**” (1998)—clarifying the semantics and services required for <u>time management</u>, repeatability, and federation-level coordination. Another prominent line of work addresses large-scale network simulation and its parallelization, including “**Large-scale network simulation: how big? how fast?**” (2003), “**A generic framework for parallelization of network simulations**” (1999), and federated approaches (“**A federated approach to distributed network simulation**,” 2004). Extending simulation to emerging cyber-physical and mobile systems, he co-developed mobility- and propagation-aware models for vehicular networking and data dissemination, including “**MDDV: a mobility-centric data dissemination algorithm for vehicular networks**” (2004) and analytical studies such as “**Analytical models for information propagation in vehicle-to-vehicle networks**” (2004).\n\n---\n\n## Notable Works\n- **Foundational synthesis of PDES and distributed simulation systems:** *Parallel and Distributed Simulation Systems* (2000) consolidated decades of results into a coherent treatment of synchronization, performance, and practical system design for <u>parallel/distributed simulation</u>, becoming a standard reference for researchers and practitioners.\n- **Core framework for parallel discrete-event simulation:** “*Parallel discrete event simulation*” (1990) articulated the central synchronization problem and the pragmatic motivations for PDES, helping define the field’s research agenda and serving as a widely cited entry point for subsequent work on conservative and optimistic protocols.\n- **Interoperability and time management in defense-grade distributed simulation:** “*The department of defense high level architecture*” (1997) and “*Time management in the high level architecture*” (1998) shaped understanding of **HLA** and its <u>time management</u> services, providing key conceptual and technical grounding for interoperable federations of simulations.\n\n---\n\n## Academic Background\nAs a senior academic leader at the **Georgia Institute of Technology**—holding the distinguished title of **Regents’ Professor**—Fujimoto’s career reflects sustained, field-defining impact in computer science and engineering aspects of simulation. The trajectory of his publications, from early work on multicomputer communication performance and PDES/Time Warp mechanisms in the late 1980s and early 1990s through major contributions to **HLA** standardization in the mid-to-late 1990s and onward to large-scale network and vehicular simulation in the 2000s, indicates deep, continuous engagement with both foundational theory and deployable systems. His extensive citation record and authorship of a seminal 2000 book underscore broad scholarly influence and community recognition. His collaborations and repeated contributions to defense-oriented interoperability (HLA) and large-scale network simulation further suggest strong affiliations with major research programs and standards-driven communities in modeling and simulation, including close alignment with the U.S. defense simulation ecosystem and the wider parallel/distributed systems research community."}, "qGeaSCYAAAAJ": {"keywords": "Computer security, Control-flow integrity (CFI), Address Space Layout Randomization (ASLR), Code-reuse attacks (ROP/JOP), Mobile security (Android/iOS), UI security and clickjacking, Software debloating and attack surface reduction, Memory safety (use-after-free mitigation), Privacy-preserving biometrics and authentication, Intrusion detection and malware analysis", "summary": "## Overview\nSimon Pak Ho Chung is a **Research Scientist** at the **Georgia Institute of Technology**, where he contributes to systems-oriented **computer security** research spanning operating systems, mobile platforms, and applied privacy. Working within Georgia Tech’s broader security research ecosystem, he focuses on defending real-world software against modern exploitation and abuse, with particular emphasis on *practical deployability* and measurable security impact. His work repeatedly targets foundational weaknesses at the boundary between software and adversaries—especially <u>memory-corruption exploitation</u>, <u>control-flow hijacking</u>, and <u>mobile UI deception</u>—and develops defenses that strengthen platform guarantees such as **ASLR**, **control-flow integrity**, and *post-deployment hardening*.\n\n---\n\n## Research Areas\nChung’s research centers on strengthening the security of commodity systems by addressing both low-level exploitation primitives and higher-level platform abuse. A major thread of his work tackles <u>code-reuse and control-flow attacks</u> by hardening core mitigations: **ASLR-Guard: Stopping Address Space Leakage for Code Reuse Attacks** (2015) addresses the often-overlooked prerequisite of *address disclosure* that undermines ASLR, while **Enforcing unique code target property for control-flow integrity** (2018) advances CFI by tightening the precision of indirect control-flow transfer validation to better approximate the “only legitimate target” ideal. Complementing these defenses, his research also examines how attackers subvert *mobile platform trust models* and user interaction: **Jekyll on {iOS}: When Benign Apps Become Evil** (2013) demonstrates how malicious behavior can be concealed to defeat app review and code signing assumptions, and **Cloak and dagger: from two permissions to complete control of the UI feedback loop** (2017) shows how seemingly limited Android permissions can enable powerful UI manipulation that breaks users’ mental models of consent and safety. In parallel, Chung has contributed to reducing attack surface and improving resilience through software transformation and debugging infrastructure, including post-deployment debloating in **{RAZOR}: A framework for post-deployment software debloating** (2019) and production-oriented exploit understanding via symbolic root-cause analysis in **{ARCUS}: symbolic root cause analysis of exploits in production systems** (2021). His portfolio further extends into privacy and security at scale—e.g., analyzing mobile ad ecosystems in **The price of free: Privacy leakage in personalized mobile in-apps ads.** (2016) and developing privacy-preserving biometric search mechanisms in **Fuzzy labeled private set intersection with applications to private Real-Time biometric search** (2021).\n\n---\n\n## Notable Works\n- **ASLR-Guard: Stopping Address Space Leakage for Code Reuse Attacks** (2015): Introduced a defense aimed at preventing <u>address space leakage</u>, directly reinforcing ASLR against the disclosure step that enables practical ROP-style code reuse in real deployments.\n- **Cloak and dagger: from two permissions to complete control of the UI feedback loop** (2017): Demonstrated that limited Android permissions can be composed to seize control over <u>UI-mediated security decisions</u>, reshaping how the community evaluates permission risk and UI integrity.\n- **Enforcing unique code target property for control-flow integrity** (2018): Advanced **CFI** by enforcing a stronger uniqueness property for indirect control-flow transfers, reducing over-approximation that attackers exploit when CFI policies admit too many valid targets.\n\n---\n\n## Academic Background\nChung’s publication record shows sustained contributions to top-tier, systems-focused security topics from the mid-2000s onward, beginning with early work on intrusion detection and adversarial robustness (e.g., **On random-inspection-based intrusion detection** (2005) and the **allergy attack** line of work on automatic signature generation (2006–2007)). His later trajectory reflects deep engagement with mobile and OS security—particularly iOS and Android threat models (e.g., **Jekyll on {iOS}** (2013), **Mactans** (2013), and **A11y attacks** (2014))—followed by influential systems defenses for exploitation mitigation (e.g., **ASLR-Guard** (2015) and strengthened **CFI** in 2018). His more recent work at Georgia Tech connects defensive systems research with privacy-preserving authentication and biometric search (e.g., **Fuzzy labeled private set intersection…** (2021) and **rtcaptcha** (2023)), indicating active collaboration within an interdisciplinary security community. With **2174 citations**, his impact reflects both foundational security contributions and applied, ecosystem-relevant studies; his affiliation with the **Georgia Tech Research Corporation** is also evidenced through technology-transfer and patent-linked publications (e.g., **rtcaptcha** and privacy-preserving biometric/authentication methods), consistent with a research profile that bridges academic dissemination and deployable security innovation."}, "q_K8iFQAAAAJ": {"keywords": "Cognitive Systems, Artificial Intelligence in Education, Human-AI Teaming, Concept Formation, Intelligent Tutoring Systems, Apprentice Learner Models, Simulated Students (SimStudent), Educational Data Mining, Interactive Task Learning, Large Language Models for Tutoring", "summary": "## Overview\nChristopher J. MacLellan is an Assistant Professor at the Georgia Institute of Technology, where he leads a research program at the intersection of **cognitive systems** and **artificial intelligence in education** with an emphasis on *human-centered* AI. His work advances <u>interactive learning</u> and <u>concept formation</u> methods that enable AI systems to acquire knowledge incrementally from people, then apply that knowledge to support learning and collaboration. Across projects spanning **intelligent tutoring systems (ITS)**, **human–AI teaming**, and computational models of learning, MacLellan’s research aims to close the gap between *theory-driven cognitive modeling* and *deployable educational technologies* that can be authored, evaluated, and improved in real-world settings.\n\n---\n\n## Research Areas\nMacLellan’s research centers on building and evaluating AI systems that learn in ways aligned with human instruction, educational data, and cognitive theory. A major thread is **apprenticeship-based tutor authoring** and simulated learners: his work on the *Apprentice Learner* line—including “**The Apprentice Learner Architecture: Closing the loop between learning theory and educational data**” (2016) and “**Domain-general tutor authoring with apprentice learner models**” (2022)—develops mechanisms by which tutor behavior and student modeling can be learned from examples, enabling more scalable ITS development. Complementing this, he contributes to **concept formation** and incremental learning in structured and perceptual domains (e.g., “**TRESTLE: A Model of Concept Formation in Structured Domains**” (2016), “**TRESTLE: Incremental Learning in Structured Domains using Partial Matching and Categorization**” (2015), and “**Convolutional cobweb: A model of incremental learning from 2d images**” (2022)), emphasizing interpretable representations and continual learning without catastrophic forgetting. Another prominent area is **human–machine teaming** and the competencies required for effective collaboration, articulated in “**Improving teamwork competencies in human-machine teams: Perspectives from team science**” (2021), which draws from team science to propose directions for designing machine teammates. More recently, he has extended interactive learning and tutoring to modern generative models, exploring natural language interfaces for task learning (e.g., “**Val: Interactive task learning with gpt dialog parsing**” (2024)), educator-driven creation of tutor interfaces (“**Towards educator-driven tutor authoring: generative AI approaches for creating intelligent tutor interfaces**” (2024)), and rigorous evaluation of LLMs as tutors beyond correctness alone (“**Beyond final answers: Evaluating large language models for math tutoring**” (2025)). His broader publication record also reflects sustained engagement with learning analytics and learner modeling (e.g., “**Accounting for Slipping and Other False Negatives in Logistic Models of Student Learning**” (2015)) and foundational cognitive questions about knowledge organization (e.g., “**Developmental changes in semantic knowledge organization**” (2016)).\n\n---\n\n## Notable Works\n- **Human–AI Teaming Competencies:** In “*Improving teamwork competencies in human-machine teams: Perspectives from team science*” (2021), MacLellan synthesizes team science constructs to outline how machine agents can be designed and trained to better participate in collaborative work, shaping research agendas around <u>human–machine teaming</u> and measurable teamwork competencies.  \n- **Apprentice Learner and Scalable Tutor Authoring:** Through “*The Apprentice Learner Architecture: Closing the loop between learning theory and educational data*” (2016) and the follow-on “*Domain-general tutor authoring with apprentice learner models*” (2022), he advances a framework for learning tutor behavior from instruction and data, supporting <u>domain-general</u> pathways for building ITSs with reduced expert engineering.  \n- **Incremental Concept Formation in Structured Domains:** With “*TRESTLE: A Model of Concept Formation in Structured Domains*” (2016) (and related work on incremental structured learning), he contributes algorithms for probabilistic, hierarchical concept formation that support prediction, categorization, and interpretable knowledge acquisition under limited data—capabilities central to <u>concept formation</u> in cognitive systems.\n\n---\n\n## Academic Background\nMacLellan’s scholarly trajectory reflects a sustained focus on cognitively grounded AI for learning and collaboration, with research contributions spanning simulated learners and tutor authoring (e.g., the SimStudent/Apprentice Learner lineage, including “Authoring Tutors with SimStudent: An Evaluation of Efficiency and Model Quality” (2014)), learner modeling and educational data mining (e.g., “Accounting for Slipping and Other False Negatives in Logistic Models of Student Learning” (2015)), and computational accounts of knowledge representation and development (e.g., “Developmental changes in semantic knowledge organization” (2016)). His publication history indicates interdisciplinary affiliations bridging **AI**, **learning sciences**, and **human-centered computing**, culminating in his appointment as **Assistant Professor** at **Georgia Tech**, where he continues to integrate theory, algorithms, and applied systems. With approximately **965 citations**, his work has achieved broad visibility across the **AIEd**, cognitive systems, and human–AI interaction communities, and his recent papers on LLM-enabled tutoring and interactive task learning suggest an expanding agenda that connects established cognitive modeling traditions to contemporary generative AI and deployable educational interfaces."}, "r9K3MQwAAAAJ": {"keywords": "Cryptography, Security, Searchable Symmetric Encryption, Encrypted Database Search, Secure Messaging Protocols, Key Ratcheting, Adaptive Compromise Security, Memory-Tight Reductions, Algorithm-Substitution Attacks, Post-Quantum Cryptography", "summary": "## Overview\nJoseph Jaeger is an **Assistant Professor** at the **Georgia Institute of Technology** (School of Computer Science), where he leads research at the intersection of **cryptography** and **computer security**. His work focuses on designing *provably secure* yet *practically deployable* protocols and reductions, with particular emphasis on <u>secure messaging</u>, <u>searchable encryption</u>, and <u>security under compromise</u>. Across both systems-driven and theory-driven contributions, Jaeger is known for bridging implementation realities with rigorous models—developing **formal definitions**, **tight security analyses**, and **constructions** that anticipate modern threat models such as state exposure, memory-bounded adversaries, and covert subversion.\n\n---\n\n## Research Areas\nJaeger’s research spans several core areas of modern cryptography, often motivated by real-world deployment constraints and adversarial capabilities. A major thread is **searchable encryption**, exemplified by “*Dynamic searchable encryption in very-large databases: Data structures and implementation*” (2014), which designs and implements dynamic symmetric searchable encryption capable of handling *tens of billions* of record–keyword pairs, emphasizing both privacy and scalability. A second foundational area is <u>secure messaging</u> and channel security: in “*Ratcheted encryption and key exchange: The security of messaging*” (2017) and “*Optimal channel security against fine-grained state compromise: The safety of messaging*” (2018), he helps formalize ratcheting and state-compromise resilience, providing definitions and constructions that clarify what security is achievable when an adversary can repeatedly learn internal state. Jaeger also investigates <u>cryptographic subversion</u> and algorithm-substitution attacks in “*Mass-surveillance without the state: Strongly undetectable algorithm-substitution attacks*” (2015), expanding the scope of ASAs to randomized schemes broadly and sharpening undetectability guarantees. Complementing these system-facing topics, he develops rigorous *concrete-security* and *resource-sensitive* proof techniques—e.g., expected-time adversaries (“*Expected-time cryptography*,” 2020) and memory-bounded tightness results for symmetric encryption and authenticated encryption (“*Tight time-memory trade-offs for symmetric encryption*,” 2019; “*The memory-tightness of authenticated encryption*,” 2020; “*Hiding in plain sight: memory-tight proofs via randomness programming*,” 2022). More recent work extends these themes into emerging settings, including **post-quantum considerations** (“*Quantum key-length extension*,” 2021), adaptive compromise and composability (“*Handling adaptive compromise for practical encryption schemes*,” 2020; “*Let attackers program ideal models*,” 2023), and modern group messaging and signcryption formulations (“*Symmetric signcryption and E2EE group messaging in keybase*,” 2024; “*Analyzing Group Chat Encryption in MLS, Session, Signal, and Matrix*,” 2025).\n\n---\n\n## Notable Works\n- **Dynamic searchable encryption at massive scale** — In “*Dynamic searchable encryption in very-large databases: Data structures and implementation*” (2014), Jaeger advances dynamic symmetric searchable encryption with an emphasis on efficient data structures and real implementation considerations, enabling private search over encrypted datasets at the scale of *tens of billions* of record–keyword pairs.\n- **Formal foundations for modern secure messaging under compromise** — Through “*Ratcheted encryption and key exchange: The security of messaging*” (2017) and “*Optimal channel security against fine-grained state compromise: The safety of messaging*” (2018), he helps define and achieve strong security notions for messaging channels that must remain robust even when adversaries can repeatedly compromise endpoint state.\n- **Cryptographic subversion and undetectable algorithm-substitution attacks** — In “*Mass-surveillance without the state: Strongly undetectable algorithm-substitution attacks*” (2015), Jaeger contributes attacks and analyses that broaden ASA applicability beyond prior constraints (e.g., coin-injectivity), clarifying how surveillance-capable subversion can be engineered to remain strongly undetectable.\n\n---\n\n## Academic Background\nAs a faculty member at **Georgia Tech**, Jaeger is affiliated with the institute’s cryptography and security research ecosystem and contributes to the broader academic community through sustained publication in leading venues spanning *cryptographic theory and systems*. His publication trajectory—from large-scale encrypted search (2014), to subversion-resilient perspectives (2015–2016), to rigorous secure-messaging foundations (2017–2019), and onward to resource-tight proofs, adaptive compromise frameworks, and contemporary group-messaging analyses (2020–2025)—reflects a research program shaped by both **practical deployment pressures** and **formal security rigor**. With **1,807 citations**, his work has achieved substantial impact, particularly in searchable encryption and secure messaging theory, and his recent papers indicate continued leadership in <u>tight reductions</u>, <u>composable security under compromise</u>, and the cryptographic analysis of widely deployed end-to-end encrypted communication protocols."}, "rp8dOfAAAAAJ": {"keywords": "Databases, Database Systems, Data Management, Self-driving databases, Non-volatile memory (NVM) DBMS, Transaction processing (OLTP) and MVCC, Logging and recovery, Indexing and latch-free data structures, Hybrid row-column stores (HTAP), Video database systems and video analytics query optimization", "summary": "## Overview\nJoy Arulraj is a researcher at the **Georgia Institute of Technology**, where he leads and collaborates within a systems-and-data-management research environment focused on building *high-performance, dependable* database infrastructure. His work centers on **database systems** and **data management**, with an emphasis on <u>hardware-conscious DBMS design</u> and <u>automation</u>—spanning **self-driving database management systems**, **non-volatile memory (NVM) architectures**, and modern reliability/performance tooling that reduces the need for manual DBA intervention while improving system robustness under evolving workloads and platforms.\n\n---\n\n## Research Areas\nArulraj’s research agenda bridges foundational DBMS components with emerging hardware and data modalities. A major thrust is <u>autonomous database operation</u>, articulated in *“Self-Driving Database Management Systems” (2017)*, which reframes classical tuning (physical design, configuration, and runtime adaptation) as a pipeline of automated decision-making that can close the loop between observation and action. In parallel, he has made sustained contributions to <u>persistent memory / NVM-aware database design</u>, including storage and recovery mechanisms (*“Let’s talk about storage & recovery methods for non-volatile memory database systems” 2015; “Write-behind logging” 2016; “How to build a non-volatile memory database management system” 2017; “A prolegomenon on OLTP database systems for non-volatile memory” 2014*), and to core access methods such as latch-free indexing for NVM (*“BzTree: A high-performance latch-free range index for non-volatile memory” 2018*). He also investigates transactional performance and concurrency, offering measurement-driven clarity on MVCC trade-offs in modern in-memory engines (*“An empirical evaluation of in-memory multi-version concurrency control” 2017*). Complementing these systems-building efforts, his portfolio includes practical techniques for <u>performance regression detection and diagnosis</u> in complex DBMS codebases (*“APOLLO” 2019; “Automatic detection of performance bugs… using equivalent queries” 2022; “SQLCheck” 2020*) and formal/symbolic methods for <u>query equivalence and optimization</u> (*“Automated verification of query equivalence using SMT” 2019; “SPES” 2022*). More recently, he has extended database principles into <u>video analytics and AI-data systems</u>, developing systems that optimize exploratory video queries and mitigate real-world drift (*“Odin” 2020; “EVA” 2022/2023; “Figo” 2022; “Thia” 2021; “SketchQL” 2024*), as well as characterizing and optimizing <u>GPU database systems</u> (*2023*).\n\n---\n\n## Notable Works\n- **Self-driving DBMS vision and architecture:** In *“Self-Driving Database Management Systems” (2017)*, Arulraj and collaborators synthesize decades of DBA advisory tooling into a more comprehensive <u>closed-loop automation</u> perspective, motivating systems that can observe, diagnose, and act with reduced human oversight.\n- **NVM-native indexing and persistent data structures:** *“BzTree: A high-performance latch-free range index for non-volatile memory” (2018)* advances <u>concurrency-friendly</u> and <u>persistent-memory-aware</u> indexing, enabling scalable range queries and updates while aligning with NVM performance and durability constraints.\n- **Rethinking durability for persistent memory:** Across *“Let’s talk about storage & recovery methods for non-volatile memory database systems” (2015)* and *“Write-behind logging” (2016)*, he helps redefine logging/recovery assumptions inherited from HDD/SSD eras, proposing designs that better exploit near-DRAM latency while preserving correctness and fast recovery.\n\n---\n\n## Academic Background\nBased on his publication trajectory and sustained focus on core DBMS internals, Arulraj’s academic profile reflects deep training in **computer systems** and **data management**, culminating in dissertation-level work on persistent memory DBMS architecture (evidenced by *“The Design and Implementation of a Non-Volatile Memory Database Management System” 2018* and related NVM studies). His research record—spanning high-impact contributions to autonomous tuning, NVM storage/recovery, indexing, concurrency control evaluation, and performance diagnosis—indicates active engagement with the international database and systems research community through repeated work on flagship topics and long-running collaborations. With **3043 citations**, his work has become widely referenced in both academia and practice, particularly where <u>hardware trends</u> (persistent memory, GPUs) and *operational complexity* (regressions, tuning, hybrid workloads) pressure database systems to become more automated, adaptive, and performance-portable."}, "s7kve5EAAAAJ": {"keywords": "Servers, HPC, systems management, computer networking, post-Moore architectures, experimental computing testbeds, heterogeneous computing, Arm A64FX, RISC-V, Open OnDemand", "summary": "## Overview\nWill Powell is a **Research Technologist** at the Georgia Institute of Technology, where he supports and advances experimental computing infrastructure within Georgia Tech’s computing ecosystem, including the *Rogues Gallery* testbed. His work centers on the practical realities of operating **servers** and **HPC** environments while enabling research and education on emerging platforms. Across his publications, Powell’s primary focus is the design and operation of <u>systems management</u> workflows that make *post-Moore* and nontraditional architectures usable at scale—bridging **computer networking**, user onboarding, and day-to-day reliability for heterogeneous research testbeds.\n\n---\n\n## Research Areas\nPowell’s research lies at the intersection of **HPC systems operations** and **experimental architecture enablement**, with an emphasis on making heterogeneous platforms accessible to both researchers and students. In “**Wrangling rogues: A case study on managing experimental post-moore architectures**” (2019) and the related “**Wrangling Rogues: Managing Experimental Post-Moore Architectures**” (2018), he examines the operational challenges of integrating “rogue” devices into a shared facility—highlighting the need for robust provisioning, configuration control, and support practices when architectures span embedded roots and high-performance contexts. His education- and workflow-oriented work extends this management perspective to training and usability: “**Enhancing hpc education and workflows with novel computing architectures**” (2022) discusses how emerging CPU designs (e.g., Arm, RISC-V) and reconfigurable accelerators (e.g., FPGAs) reshape instructional and research workflows beyond traditional CPU/GPU clusters. Complementing this, “**Onboarding users to a64fx via open ondemand**” (2022) focuses on reducing friction for Arm HPC adoption by addressing the practical complexity of compilers, libraries, and user environment setup through web-based access and onboarding processes. Finally, “**Future computing with the rogues gallery**” (2023) situates these technical efforts in a programmatic educational setting, documenting how a vertically integrated research course uses the Rogues Gallery to engage students in large-scale computing projects and real-world infrastructure constraints.\n\n---\n\n## Notable Works\n- **Operationalizing post-Moore testbeds through systems management** — In “*Wrangling rogues: A case study on managing experimental post-moore architectures*” (2019), Powell provides a case-study account of managing the Rogues Gallery, articulating facility-level practices for sustaining heterogeneous, experimental architectures in a shared environment.\n- **Lowering barriers to Arm HPC adoption via user onboarding** — In “*Onboarding users to a64fx via open ondemand*” (2022), he addresses the usability gap for Arm-based HPC systems (notably A64FX) by focusing on onboarding pathways and environment complexity, leveraging Open OnDemand-style interfaces to streamline access.\n- **Integrating novel architectures into HPC education and research workflows** — In “*Enhancing hpc education and workflows with novel computing architectures*” (2022), Powell advances an educational and operational framework for incorporating Arm, RISC-V, and FPGA-based resources into HPC curricula and day-to-day workflows, aligning infrastructure capabilities with learning outcomes.\n\n---\n\n## Academic Background\nBased at **Georgia Tech**, Powell’s publication record reflects an academic-operations profile characteristic of research technologists who co-develop infrastructure while contributing scholarly outputs on facility practice and user enablement. His recurring affiliation with the *Rogues Gallery* effort—spanning early management-focused reporting (2018–2019) through later education and onboarding work (2022–2023)—suggests sustained involvement in Georgia Tech’s experimental computing initiatives and close collaboration with campus HPC and advanced computing stakeholders. His contributions align with institutional efforts to broaden access to emerging architectures through structured programs such as the **Vertically Integrated Projects (VIP)** course described in “*Future computing with the rogues gallery*” (2023), indicating an affiliation with pedagogy-driven research infrastructure and mentorship of student teams working on large-scale systems."}, "sMnocAYAAAAJ": {"keywords": "Software Systems, Internet security measurement, Botnets and malware analysis, Distributed denial-of-service (DDoS) defense, IoT and embedded device security, Internet-wide scanning and attack surface mapping, TLS/HTTPS security and certificate ecosystem (PKI), Darknet/network telescope monitoring, Cloud and virtualization security (covert channels), Abuse and human-centered security (toxicity/harassment)", "summary": "## Overview\nMichael Donald Bailey is a researcher at the Georgia Institute of Technology whose work sits at the intersection of **software systems** and *empirical cybersecurity*. Associated with Internet-scale measurement and security analysis efforts, he is best known for turning large, messy, real-world telemetry into actionable understanding of attacks and defenses. Across studies of botnets, TLS/PKI, and large-scale scanning, Bailey’s research emphasizes <u>measurement-driven security</u>: building evidence about how the Internet actually behaves, how adversaries exploit it, and how defenders can respond with **practical, deployable** techniques.\n\n---\n\n## Research Areas\nBailey’s research spans several tightly connected areas in systems security. A central theme is <u>Internet-wide measurement</u> and its use for understanding emergent threats and ecosystem health. This is visible in work that operationalizes scanning as a scientific instrument—e.g., **“A search engine backed by Internet-wide scanning”** (2015) and **“An Internet-Wide View of Internet-Wide Scanning”** (2014)—to answer concrete questions about exposed services, deployment misconfigurations, and vulnerability prevalence. He has also made influential contributions to the study of large-scale vulnerabilities and their aftermath, including **“The matter of heartbleed”** (2014), which quantified real-world exposure and remediation dynamics following the Heartbleed disclosure. Another major line of work addresses botnets, DDoS, and Internet “background radiation,” ranging from early darknet/telescope-based threat characterization (**“The Internet Motion Sensor”** (2005), **“Internet background radiation revisited”** (2010)) to high-impact analyses of modern IoT-driven disruption, most notably **“Understanding the Mirai Botnet”** (2017), which retrospectively measured Mirai’s growth and operational behavior. Complementing measurement, Bailey has advanced malware analysis and evasion understanding—e.g., **“Automated classification and analysis of internet malware”** (2007) and **“Towards an understanding of anti-virtualization and anti-debugging behavior in modern malware”** (2008)—highlighting how adversaries adapt to analysis tooling. His portfolio also extends to the security of core trust and communication infrastructures, including the HTTPS certificate ecosystem (**“Analysis of the HTTPS certificate ecosystem”** (2013); **“Towards a complete view of the certificate ecosystem”** (2016); **“Tracking Certificate Misissuance in the Wild”** (2018)) and the consequences of interception (**“The Security Impact of HTTPS Interception”** (2017)). More recent work broadens systems security to socio-technical threats and usability at scale, such as online abuse taxonomies (**“SoK: Hate, harassment, and the changing landscape of online abuse”** (2021)) and human-centered security experiments (**“Users really do plug in USB drives they find”** (2016)).\n\n---\n\n## Notable Works\n- **Internet-scale characterization of IoT botnets and DDoS dynamics** — In **“Understanding the Mirai Botnet”** (2017), Bailey and collaborators provided a longitudinal, measurement-based retrospective of Mirai’s growth and impact, helping define how the community studies IoT-enabled DDoS campaigns.\n- **Empirical analysis of a landmark Internet vulnerability and remediation behavior** — **“The matter of heartbleed”** (2014) offered a comprehensive view of Heartbleed’s real-world reach (across HTTPS deployments) and the pace and unevenness of patching and certificate replacement, shaping best practices for post-disclosure measurement.\n- **Foundations for automated, scalable malware ecosystem analysis** — **“Automated classification and analysis of internet malware”** (2007) advanced methods for large-scale malware characterization in the presence of diverse threats (worms, phishing, botnets), influencing how defenders reason about malware families and the limits of signature-driven detection.\n\n---\n\n## Academic Background\nBased at the Georgia Institute of Technology, Bailey has built a research trajectory that reflects deep engagement with the systems and security communities, particularly those centered on <u>network measurement</u>, malware analysis, and Internet infrastructure security. The arc of his publication record—from early work on darknet monitoring, worms, and distributed blackhole placement (2004–2006) through influential studies of malware behavior and evasion (2007–2009), and onward to Internet-wide scanning, PKI/ecosystem measurement, and high-profile incident analyses (2013–2017)—suggests sustained leadership in translating operational Internet data into rigorous scientific findings. His involvement in **“The Menlo Report”** (2012) further indicates an active role in broader community governance around ethical principles for ICT research, aligning methodological innovation with *responsible measurement*. With a citation record exceeding **16,000**, Bailey’s work has achieved wide uptake across academia and practice, reflecting both foundational contributions and continuing relevance to modern security challenges."}, "sfuwsSwAAAAJ": {"keywords": "Neuro-inspired Machine Learning, Network Science, Computational Science, Internet measurement, Available bandwidth estimation, Packet dispersion probing, TCP throughput modeling, HTTP adaptive video streaming (DASH), Quality of Service and differentiated services, Interdomain routing and BGP security", "summary": "## Overview\nConstantine Dovrolis is a Professor of Computer Science at Georgia Tech, where he leads a research program at the intersection of **computer networks** and **data-driven computational science**, with recent expansion into **neuro-inspired machine learning**. His work is characterized by a rigorous blend of *measurement methodology*, *systems thinking*, and *model-driven inference*, with a sustained emphasis on making network behavior observable and explainable. Across decades of highly cited scholarship (over **16,250 citations**), Dovrolis has advanced the foundations of <u>end-to-end network measurement</u>, <u>Internet architecture and ecosystem analysis</u>, and <u>adaptive streaming performance</u>, while more recently contributing to <u>continual learning</u> through neuro-inspired mechanisms.\n\n---\n\n## Research Areas\nDovrolis’s research spans three tightly connected areas: (1) **Internet measurement and performance inference**, (2) **networked systems and content delivery**, and (3) **network science and computational modeling**, with emerging work in **neuro-inspired learning**. A central thread is the principled estimation of path properties—especially *capacity* and *available bandwidth*—through end-to-end probing and careful interpretation of what measurements truly represent. This is exemplified by his analyses of packet dispersion and probing artifacts in “**What do packet dispersion techniques measure?**” (2001) and the broader methodological synthesis in “**Bandwidth estimation: metrics, measurement techniques, and tools**” (2003), which helped systematize definitions, assumptions, and tool design for bandwidth-related metrics. He also introduced and validated practical methodologies for available bandwidth estimation, including **Self-Loading Periodic Streams (SLoPS)** in “**End-to-end available bandwidth: Measurement methodology, dynamics, and relation with TCP throughput**” (2002/2003) and the widely used tool “**Pathload: A measurement tool for end-to-end available bandwidth**” (2002), while later clarifying common misconceptions in “**Ten fallacies and pitfalls on end-to-end available bandwidth estimation**” (2004) and examining estimator bias (e.g., “**The probe gap model can underestimate the available bandwidth of multihop paths**,” 2006). Beyond measurement, his work addresses *QoS and service differentiation* through the proportional differentiation model (e.g., “**Proportional differentiated services: Delay differentiation and packet scheduling**,” 1999/2002; and “**A case for relative differentiated services and the proportional differentiation model**,” 2002), and tackles practical issues in TCP and router behavior such as passive RTT inference (“**Passive estimation of TCP round-trip times**,” 2002) and buffer sizing debates (“**Buffer sizing for congested internet links**,” 2005; “**Open issues in router buffer sizing**,” 2006). In the 2010s, Dovrolis became a leading voice in **HTTP adaptive streaming** performance, empirically evaluating rate adaptation (“**An experimental evaluation of rate-adaptation algorithms in adaptive streaming over HTTP**,” 2011) and diagnosing multi-player competition pathologies (“**What happens when HTTP adaptive streaming players compete for bandwidth?**,” 2012), including mitigation via server-side shaping (“**Server-based traffic shaping for stabilizing oscillating adaptive streaming players**,” 2013) and the nuanced role of caching (“**Caching in http adaptive streaming: Friend or foe?**,” 2014). Complementing these systems contributions, he has analyzed the evolving **Internet interdomain ecosystem** (“**The internet is flat**,” 2010; “**Ten years…**,” 2008; “**Twelve years…**,” 2011; “**Using peeringDB to understand the peering ecosystem**,” 2014) and engaged foundational debates on architectural evolution versus clean-slate design (“**Future Internet architecture: clean-slate versus evolutionary research**,” 2010; “**What would Darwin think about clean-slate architectures?**,” 2008), including abstract evolutionary models of protocol stacks (“**The evolution of layered protocol stacks leads to an hourglass-shaped architecture**,” 2011). More recently, his neuro-inspired ML work on continual learning, such as “**Nispa: Neuro-inspired stability-plasticity adaptation for continual learning in sparse networks**” (2022), aligns mechanistic learning principles with computational efficiency and robustness, reflecting a broader interest in *adaptive systems* across both networks and learning.\n\n---\n\n## Notable Works\n- **Foundational synthesis of bandwidth estimation**: In “**Bandwidth estimation: metrics, measurement techniques, and tools**” (2003), Dovrolis consolidated definitions and methodological distinctions (capacity vs. available bandwidth vs. throughput), clarifying how measurement tools map to specific metrics and where inference can fail—an anchor reference for <u>end-to-end performance measurement</u>.\n- **Practical available-bandwidth methodology and tooling (SLoPS/Pathload)**: Through “**End-to-end available bandwidth: Measurement methodology, dynamics, and relation with TCP throughput**” (2002/2003) and “**Pathload: A measurement tool for end-to-end available bandwidth**” (2002), he advanced <u>self-loading periodic stream</u> probing and delivered a deployable toolchain that connected measurement dynamics to transport-layer outcomes.\n- **Empirical science of HTTP adaptive streaming and competition**: In “**An experimental evaluation of rate-adaptation algorithms in adaptive streaming over HTTP**” (2011) and “**What happens when HTTP adaptive streaming players compete for bandwidth?**” (2012), Dovrolis provided influential experimental characterizations of player behavior, instability, and fairness under shared bottlenecks—work that shaped subsequent research and operational thinking about <u>adaptive bitrate (ABR)</u> systems.\n\n---\n\n## Academic Background\nAs a long-standing faculty member at the Georgia Institute of Technology (Georgia Tech), Dovrolis has built an internationally recognized research trajectory spanning core networking theory and practice, evidenced by sustained high-impact publications from the late 1990s onward and a citation record exceeding **16,250**. His early and influential contributions to **QoS and differentiated services** (e.g., proportional differentiation in 1999–2002) established a foundation in network control and scheduling, followed by a major body of work in the 2000s on **measurement methodology**, **TCP/traffic dynamics**, and **router/interface behavior** (including probing interpretation, passive RTT estimation, interrupt effects, and buffer sizing). In the 2010s, his agenda broadened to **Internet ecosystem evolution** and **measurement infrastructure** (including community-facing platforms such as “**Measurement lab: Overview and an invitation to the research community**,” 2010), alongside a prominent line of experimental systems research on **HTTP adaptive streaming**. His more recent publications indicate a deliberate expansion toward **computational science** and **neuro-inspired machine learning**, leveraging long-standing expertise in modeling complex adaptive systems. Collectively, these contributions position him as a key academic figure linking *measurement rigor*, *systems realism*, and *theory-informed modeling* across networking and learning communities, consistent with his role as a senior professor and research leader at Georgia Tech."}, "spWVns8AAAAJ": {"keywords": "AI for social impact, machine learning, optimization, multi-agent systems, online learning, decision-focused learning, differentiable optimization, restless multi-armed bandits, Stackelberg security games, reinforcement learning", "summary": "## Overview\nKai Wang is a researcher at the Georgia Institute of Technology, where he develops methods at the intersection of **machine learning** and **optimization** with an emphasis on *AI for social impact*. His work centers on <u>decision-aware learning</u>—training predictive models not merely for accuracy, but to improve the quality of downstream decisions in complex, resource-constrained environments. Across applications spanning public health, sustainability, and security, Wang’s research advances **online learning**, **multi-agent systems**, and *game-theoretic* decision-making, with a particular focus on scalable algorithms that remain reliable under uncertainty.\n\n---\n\n## Research Areas\nWang’s research agenda is organized around integrating learning and decision-making for high-stakes, sequential, and strategic settings. A major theme is <u>decision-focused learning</u> and predict-then-optimize pipelines, including methodological contributions that reduce reliance on expensive end-to-end differentiation through solvers while still optimizing decision quality, as in “**Decision-focused learning without decision-making: Learning locally optimized decision losses**” (2022) and “**Automatically learning compact quality-aware surrogates for optimization problems**” (2020). He extends these ideas to sequential decision problems by connecting feature-based model learning with planning and reinforcement learning in “**Learning MDPs from Features: Predict-Then-Optimize for Sequential Decision Problems by Reinforcement Learning**” (2021). Another core area is <u>online learning in restless multi-armed bandits</u> (RMABs), where he develops scalable learning-and-control approaches with provable or empirically strong performance, including “**Optimistic whittle index policy: Online learning for restless bandits**” (2023) and “**Scalable decision-focused learning in restless multi-armed bandits with application to maternal and child health**” (2023). Complementing these learning-centric contributions, Wang has a sustained line of work in <u>security games</u> and multi-agent coordination—modeling strategic behavior, signaling, and operational constraints in real deployments—exemplified by “**Strategic coordination of human patrollers and mobile sensors with signaling for security games**” (2018) and “**Dual-mandate patrols: Multi-armed bandits for green security**” (2021). More recently, his publication record indicates expanding interests in *modern policy learning and alignment*, including online RL for diffusion policies and control-inspired perspectives on representation editing for LLM alignment.\n\n---\n\n## Notable Works\n- **Locally optimized decision losses for decision-focused learning**: In “*Decision-focused learning without decision-making: Learning locally optimized decision losses*” (2022), Wang advances <u>DFL</u> by proposing training objectives that target downstream decision performance without requiring full decision-making differentiation at every step, improving practicality and scalability of decision-aware training.\n- **Quality-aware surrogate learning for optimization under unknown parameters**: In “*Automatically learning compact quality-aware surrogates for optimization problems*” (2020), he contributes methods for building compact surrogates that better preserve decision quality than purely predictive surrogates, strengthening the bridge between **prediction** and <u>optimization</u> in incomplete-information settings.\n- **Bandit and game-theoretic methods for conservation and security operations**: Through “*Dual-mandate patrols: Multi-armed bandits for green security*” (2021) and related security-game work, Wang develops **online learning** and **multi-agent** decision frameworks that allocate scarce patrol resources while accounting for strategic/adaptive adversaries and real operational constraints.\n\n---\n\n## Academic Background\nBased at the Georgia Institute of Technology, Wang’s scholarly trajectory reflects a sustained, publication-driven engagement with the AI-and-OR interface—spanning differentiable and decision-focused optimization, reinforcement learning for sequential decision-making, and game-theoretic security applications. His early, highly cited work in security games and patrolling (e.g., operationalizable strategies, equilibrium refinement, and sensor-empowered coordination) suggests close collaboration with interdisciplinary teams and domain partners, while subsequent contributions broaden into <u>data-to-decisions</u> learning frameworks and RMAB methods with direct public-health relevance (notably maternal and child health scheduling and engagement). With **602 citations** and multiple well-cited papers across machine learning, optimization, and applied AI venues, Wang is affiliated with a research community that values both theoretical rigor (e.g., first-order methods for constrained bilevel optimization) and real-world impact (e.g., conservation security and healthcare interventions), positioning him as a contributor to *scalable, decision-centric AI* at Georgia Tech."}, "stSffQ8AAAAJ": {"keywords": "Science and Technology Studies, Critical Data Studies, Feminist Technoscience, Human-Computer Interaction (HCI), Postcolonial computing, Environmental data science (Earth AI), Datafication of climate change, Participatory design and making, Digital labor and technoprecarity, Computing and environmental justice (digital infrastructures & energy)", "summary": "## Overview\nCindy Lin Kaiying is a researcher at the Georgia Institute of Technology whose work sits at the intersection of **Science and Technology Studies (STS)**, **Critical Data Studies**, and **Human–Computer Interaction (HCI)**. Working across interdisciplinary collaborations that connect computing with social theory, she examines how *technoscientific futures are made and justified* through design, data, and infrastructures. Her scholarship foregrounds <u>power</u>, <u>political economy</u>, and <u>postcolonial and feminist critiques</u> to analyze how seemingly “neutral” systems—ranging from AI research programs to participatory making cultures—reproduce or contest unequal relations across places, institutions, and environments.\n\n---\n\n## Research Areas\nLin’s research develops a sustained critique of how contemporary computing is organized around normative commitments to *progress*, *productivity*, and *usefulness*, and how these commitments travel globally through design and innovation cultures. In “**Techniques of use: Confronting value systems of productivity, progress, and usefulness in computing and design**” (2021), she interrogates HCI’s inherited value systems and the moral force behind building “useful” technologies, showing how ideals of productivity and improvement often obscure whose lives and labors are centered or discounted. This emphasis on the politics of aspiration extends to “**Scaling techno-optimistic visions**” (2020), where she traces techno-optimism as a historically situated formation tied to modernity and development narratives, including their role in nation-building and economic projects. Alongside these critiques of innovation ideology, Lin contributes to scholarship on *making* and participatory design—most notably through “**HCI’s making agendas**” (2017) and “**Making and its promises**” (2017)—mapping how maker cultures and democratization claims become institutionalized, contested, and unevenly realized. Her work also advances critical approaches to data and AI as socio-environmental formations: “**A review of earth artificial intelligence**” (2022) engages Earth system science’s turn to AI amid expanding datasets, while her later publications connect data practices to accountability and repair in applied contexts (e.g., “**From bias to repair: Error as a site of collaboration and negotiation in applied data science work**,” 2023). Across these topics, Lin’s postcolonial and feminist technoscience commitments remain central, informing ethnographic and historical attention to how data, design, and infrastructures are negotiated in transnational settings, including Southeast Asian contexts (e.g., “Hacking difference in Indonesia,” 2019; “How Forest Became Data,” 2022).\n\n---\n\n## Notable Works\n- **“A review of earth artificial intelligence” (2022)** — A widely cited synthesis of the emerging field of Earth AI, situating the push for AI-enabled Earth system science within pressures of scale, cost, and model performance, while clarifying the institutional and epistemic stakes of integrating AI into environmental knowledge production.\n- **“Techniques of use: Confronting value systems of productivity, progress, and usefulness in computing and design” (2021)** — A foundational critical intervention into HCI’s normative commitments, reframing “usefulness” as a historically and politically loaded value that organizes research agendas, evaluation practices, and design ethics.\n- **“Scaling techno-optimistic visions” (2020)** — A theorization of techno-optimism as a durable, scalable political project tied to modernity and development, demonstrating how narratives of technological hope are mobilized across sites to legitimate particular futures while foreclosing others.\n\n---\n\n## Academic Background\nBased at the Georgia Institute of Technology, Lin’s academic trajectory reflects sustained interdisciplinary engagement across **STS**, **HCI**, and **feminist/postcolonial theory**, with a publication record spanning core computing venues and critical theory-facing outlets. Her early work on ambient intelligence and localized design questions (e.g., “The ambience of ambient intelligence,” 2015) and on transnational hacking/making communities (e.g., “Legitimacy, boundary objects & participation in transnational DIY biology,” 2016; “Indonesian Hacking,” 2015) developed into influential contributions to debates on participatory design and maker politics (“HCI’s making agendas,” 2017; “Making and its promises,” 2017). Over time, her scholarship expanded toward critical data and AI studies—linking methodological concerns (error, bias, accountability) with organizational practice (“From bias to repair,” 2023) and with environmental and infrastructural stakes (e.g., “Towards a Material Ethics of Computing,” 2022; contributions associated with *Digital Energetics*, 2023–2024). Her citation record (690 total citations) indicates substantial impact across multiple communities, and her affiliations and collaborations—visible in co-authored, edited, and workshop-based publications (e.g., “Triangulating race, capital, and technology,” 2022; “Datafication of Climate Change,” 2025)—suggest an active role in shaping transnational, critical computing conversations that bridge academic research with broader questions of justice, environment, and technological governance."}, "tI3m7CMAAAAJ": {"keywords": "theoretical computer science, graph algorithms, dynamic graph algorithms, minimum spanning tree algorithms, maximum matching algorithms, all-pairs shortest paths, string matching algorithms, approximate pattern matching, parallel algorithms (PRAM/NC), expander graphs and superconcentrators", "summary": "## Overview\nZvi Galil is a **Professor of Computer Science** at **Georgia Tech**, where he has built an influential body of work in **theoretical computer science** spanning classic algorithm design, complexity, and the foundations of efficient computation. His scholarship is characterized by a sustained focus on *provably fast algorithms* and the principled use of data structures to achieve strong performance guarantees, with recurring emphasis on <u>graph algorithms</u>, <u>string matching</u>, and <u>dynamic computation</u>. With more than **14,000 citations**, Galil’s research has helped shape how core problems in graphs and pattern matching are formulated, optimized, and surveyed for the broader algorithms community.\n\n---\n\n## Research Areas\nGalil’s research centers on the design and analysis of efficient algorithms and data structures for fundamental discrete problems. In <u>graph algorithms</u>, he contributed to high-performance approaches for minimum spanning structures, notably leveraging advanced priority queues in “**Efficient algorithms for finding minimum spanning trees in undirected and directed graphs**” (1986), which highlights the algorithmic impact of Fibonacci heaps on classical optimization tasks. His work also systematized the algorithmic landscape of matchings through “**Efficient algorithms for finding maximum matching in graphs**” (1986), a widely cited synthesis of techniques for both bipartite and general-graph matching and a guidepost for subsequent improvements and parallel directions. A major thread of his later work advances <u>dynamic graph algorithms</u>—algorithms that maintain properties under updates—most prominently via “**Sparsification—a technique for speeding up dynamic graph algorithms**” (1997), which develops data-structural methods to maintain connectivity and spanning forests efficiently as edges are inserted and deleted, and complements broader perspectives in “**Dynamic graph algorithms**” (1999). In <u>string algorithms</u> and *stringology*, Galil contributed foundational results and surveys on pattern matching, including worst-case improvements to Boyer–Moore (“**On improving the worst case running time of the Boyer-Moore string matching algorithm**,” 1979), tight resource tradeoffs (“**Time-space-optimal string matching**,” 1983), and comprehensive treatments such as “**Pattern matching algorithms**” (1997) and surveys on approximate matching (1988). His work also extends to <u>explicit network constructions</u> in complexity and interconnection networks, exemplified by “**Explicit constructions of linear size superconcentrators**” (1979) and later refinements in expander/superconcentrator design (1987), connecting combinatorial constructions to theoretical guarantees relevant to computation and communication.\n\n---\n\n## Notable Works\n- **Minimum spanning trees with advanced heaps:** In “*Efficient algorithms for finding minimum spanning trees in undirected and directed graphs*” (1986), Galil helped demonstrate how **Fibonacci heaps** enable faster (amortized) priority-queue operations, yielding improved approaches to classic <u>minimum spanning tree</u> and related optimization problems.\n- **Sparsification for dynamic graph maintenance:** “*Sparsification—a technique for speeding up dynamic graph algorithms*” (1997) introduced a broadly influential <u>sparsification</u> framework for maintaining key graph properties—such as connectivity and spanning forests—under edge insertions and deletions, helping define the modern toolkit for *dynamic* algorithm design.\n- **Linear-size superconcentrator constructions:** “*Explicit constructions of linear size superconcentrators*” (1979) provided explicit, efficient <u>network constructions</u> with strong connectivity properties using only linear size, a cornerstone contribution linking combinatorial design to theoretical computer science.\n\n---\n\n## Academic Background\nGalil’s academic profile is anchored in a long-term appointment as **Professor of Computer Science at the Georgia Institute of Technology**, where his publication record indicates decades of sustained leadership across algorithms, complexity, and parallel computation. His early and enduring impact is evidenced by highly cited foundational work from the late 1970s and 1980s—ranging from NP-completeness results such as “**NP completeness of finding the chromatic index of regular graphs**” (1983) and “**Cyclic ordering is NP-complete**” (1977), to seminal contributions in string matching and graph optimization—followed by influential 1990s advances in dynamic graph algorithms and algorithmic techniques for pattern matching and shortest paths. The breadth of venues and topics—surveys (e.g., maximum matching; pattern matching), core algorithmic results (MST, dynamic updates), and explicit constructions (superconcentrators/expanders)—reflects deep affiliations with the international theoretical computer science community and sustained recognition through extensive citation impact and continued relevance of his work in both foundational theory and practical algorithmic paradigms."}, "u0TKjhIAAAAJ": {"keywords": "Program Analysis, Context-Free Language Reachability, Dyck-Reachability, Alias Analysis, Points-to Analysis, Compiler Testing, Program Reduction, Android GUI Testing, Program Debloating, Hybrid Type Inference for Python", "summary": "## Overview\nQirun Zhang is an Associate Professor at the Georgia Institute of Technology, where he leads research at the intersection of **program analysis** and *software reliability*. His work centers on designing scalable, principled techniques for analyzing and testing complex software systems—particularly by casting core analysis tasks as <u>graph reachability</u> and <u>formal-language</u> problems, and by building practical tools that translate theory into impact. Across a publication record with substantial community uptake (over **1,400 citations**), Zhang has advanced both foundational algorithms (e.g., <u>Dyck/CFL-reachability</u>) and end-to-end systems for *compiler testing*, *Android GUI testing*, and *hybrid static–learning inference*.\n\n---\n\n## Research Areas\nZhang’s research spans multiple pillars of modern program analysis, unified by a focus on rigorous modeling and scalability. A major thread develops language-theoretic formulations of static analysis, including fast algorithms for <u>Dyck-CFL-reachability</u> and their applications to alias analysis (e.g., “Fast algorithms for Dyck-CFL-reachability with applications to alias analysis,” 2013) as well as practical improvements to CFL-based reasoning such as graph simplification and redundancy control (e.g., “Fast graph simplification for interleaved Dyck-reachability,” 2020; “Taming transitive redundancy for context-free language reachability,” 2022; and “Recursive State Machine Guided Graph Folding for Context-Free Language Reachability,” 2023). In parallel, he has contributed to *rigorous compiler testing and debugging infrastructure*, combining empirical understanding of real-world compiler failures (“Toward understanding compiler bugs in GCC and LLVM,” 2016) with systematic test generation and reduction methods (“Skeletal program enumeration for rigorous compiler testing,” 2017; “Perses: syntax-guided program reduction,” 2018) and reliability of tooling around optimized code (“Debug information validation for optimized code,” 2020). Zhang also pursues automation for software quality in application domains, including fully automated, model-based Android GUI testing that emphasizes evolving abstractions (“Practical GUI testing of Android applications via model abstraction and refinement,” 2019). More recently, his work explores hybrid approaches that combine *static inference* with *machine learning* to overcome the limitations of either alone, exemplified by “Static inference meets deep learning: a hybrid type inference approach for Python” (2022), and extends analysis-driven optimization ideas into adjacent reasoning tools (e.g., “Speeding up SMT Solving via Compiler Optimization,” 2023). Complementing these themes is an applied security/performance line on <u>program debloating</u>, including studies of generality–reduction tradeoffs and optimization-based debloating strategies (2020–2022).\n\n---\n\n## Notable Works\n- **Automated Android GUI testing with evolving models:** In “**Practical GUI testing of Android applications via model abstraction and refinement**” (2019), Zhang introduced a fully automated model-based testing approach that refines GUI abstractions during exploration, addressing the brittleness of static GUI models and substantially improving the effectiveness of Android app testing.\n- **Empirical foundations for compiler reliability:** In “**Toward understanding compiler bugs in GCC and LLVM**” (2016), he led one of the first in-depth empirical studies of real compiler bugs, establishing actionable insights into bug characteristics and root causes that inform both detection strategies and engineering practice for safety-critical compilation pipelines.\n- **Syntax-guided, practical program reduction for debugging:** In “**Perses: syntax-guided program reduction**” (2018), Zhang advanced the state of the art in automated test-case reduction by leveraging syntactic structure to minimize failure-inducing programs while preserving triggering behavior—directly supporting compiler testing workflows and debugging productivity.\n\n---\n\n## Academic Background\nBased on his publication trajectory, Zhang’s academic formation reflects sustained contributions across both *foundational* and *systems* program analysis. His early work (circa 2010–2014) emphasized developer-facing automation and scalable static analysis, including API-complexity support and recommendation (“Cross-library api recommendation using web search engines,” 2011; “Flow-augmented call graph: A new foundation for taming api complexity,” 2011) and algorithmic advances for alias/points-to analysis via CFL/Dyck reachability (“Fast algorithms for Dyck-CFL-reachability…,” 2013; “Efficient subcubic alias analysis for C,” 2014; “Persistent pointer information,” 2014). This foundation matured into a prominent mid-career focus on compiler testing, reduction, and empirical reliability (2016–2018), followed by influential applied work in automated Android testing (2019) and a broadened portfolio including optimized-code debugging, debloating, and hybrid static–learning inference (2020–2023). Now as an Associate Professor at **Georgia Tech**, Zhang is positioned within a leading research ecosystem in programming languages and software engineering, and his sustained citation impact suggests strong community adoption of both his theoretical frameworks and practical tools—consistent with an active role in major venues spanning program analysis, software engineering, and programming languages."}, "unXtH3IAAAAJ": {"keywords": "Natural Language Processing, Machine Learning, Artificial Intelligence, Dialogue Systems, Neural Text Generation, Reinforcement Learning, Social Media Text Mining, Named Entity Recognition, Event Extraction, Multilingual and Cross-lingual NLP", "summary": "## Overview\nAlan Ritter is a researcher at the Georgia Institute of Technology whose work sits at the intersection of **Natural Language Processing**, **Machine Learning**, and **Artificial Intelligence**. Through a program of research grounded in *real-world language use*—especially noisy, rapidly evolving user-generated content—he has advanced methods for building robust language technologies for social media and interactive systems. Across a highly cited body of work (over **15,416 citations**), Ritter is particularly known for contributions to <u>social media NLP</u> and <u>data-driven dialogue</u>, spanning foundational studies of Twitter language, shared-task leadership, and modern neural approaches to *open-domain conversation* and *responsible deployment*.\n\n---\n\n## Research Areas\nRitter’s research spans several tightly connected areas. A major thread is <u>NLP for noisy user-generated text</u>, where he helped establish rigorous empirical baselines and analyses for core tasks such as **named entity recognition** and tagging in Twitter, notably in “**Named Entity Recognition in Tweets: An Experimental Study**” (2011) and later work that examines domain-specific challenges and remedies (e.g., “Twitter part-of-speech tagging for all: Overcoming sparse and noisy data,” 2013). Closely related is his work on <u>information extraction and event understanding</u> from social streams, including “**Open domain event extraction from twitter**” (2012) and weakly supervised approaches for extracting specialized events (e.g., “Weakly supervised extraction of computer security events from twitter,” 2015), reflecting an emphasis on scalable learning under limited annotation. A second major focus is <u>dialogue and response generation</u>, progressing from early data-driven response ranking and generation in social media (“Data-Driven Response Generation in Social Media,” 2011; “Filter, rank, and transfer the knowledge: Learning to chat,” 2010) to neural and reinforcement-learning-based methods that explicitly optimize long-term conversational outcomes (“**Deep reinforcement learning for dialogue generation**,” 2016) and adversarial training frameworks (“**Adversarial learning for neural dialogue generation**,” 2017). A third, increasingly prominent theme is <u>evaluation, robustness, and societal impact</u> of large-scale language systems, including work on cultural bias in LMs (“Having beer after prayer? measuring cultural bias in large language models,” 2024), security vulnerabilities (“Tensor trust: Interpretable prompt injection attacks from an online game,” 2023), and privacy-preserving behavior (“Can language models be instructed to protect personal information?,” 2023; “Reducing privacy risks in online self-disclosures with language models,” 2024). Complementing these, Ritter has contributed to community infrastructure through shared tasks and datasets (e.g., SemEval Twitter sentiment analysis, W-NUT shared tasks), reinforcing his role in shaping reproducible benchmarks for *social media and noisy-text NLP*.\n\n---\n\n## Notable Works\n- **Long-horizon optimization for open-domain conversation**: In “**Deep reinforcement learning for dialogue generation**” (2016), Ritter and collaborators addressed the shortsightedness of turn-by-turn likelihood objectives by using reinforcement learning to optimize dialogue generation with respect to future conversational outcomes, helping define a line of work on <u>goal-aware, long-term conversational modeling</u>.\n- **Foundational empirical study of Twitter NER and domain mismatch**: “**Named Entity Recognition in Tweets: An Experimental Study**” (2011) provided a widely cited characterization of why standard NLP tools degrade on tweets and established experimental methodology for adapting sequence labeling to noisy, informal text—work that influenced subsequent benchmarks and shared tasks in <u>noisy user-generated text</u>.\n- **Adversarial training for more human-like dialogue**: In “**Adversarial learning for neural dialogue generation**” (2017), Ritter and coauthors cast dialogue generation as a reinforcement learning problem with an adversarial discriminator, advancing methods aimed at producing responses that are harder to distinguish from human utterances and pushing forward research on <u>distributional realism</u> in neural conversation.\n\n---\n\n## Academic Background\nBased at the Georgia Institute of Technology, Ritter has built an academic profile characterized by sustained, high-impact contributions to core NLP problems and to the research infrastructure surrounding social media language. His publication trajectory shows early and influential work on machine reading and semantic inference (e.g., “What Is This, Anyway: Automatic Hypernym Discovery,” 2009; “Machine reading at the University of Washington,” 2010), followed by a long-running emphasis on Twitter as a testbed for robust NLP, including entity recognition, normalization, sentiment evaluation, and event extraction. His leadership in community evaluations—most visibly “SemEval-2013 task 2: Sentiment analysis in twitter” (2013) and the W-NUT shared tasks (2015; 2016)—signals strong professional affiliation with major NLP venues and a commitment to *benchmark-driven progress*. More recently, his work reflects the field’s shift toward large pre-trained models, extending into cross-lingual transfer (e.g., “Gigabert: Zero-shot transfer learning from english to arabic,” 2020), multimodal and retrieval settings (“Can pre-trained vision and language models answer visual information-seeking questions?,” 2023; “Uniir: Training and benchmarking universal multimodal information retrievers,” 2024), and the safety, privacy, and security of deployed systems—an arc consistent with a researcher who both advances modeling techniques and interrogates their real-world consequences."}, "vjZrDKQAAAAJ": {"keywords": "Computer Vision, Robotics, Machine Learning, AI, Object Detection, Scene Understanding, Large-Scale Vision Datasets, Image Geolocalization, Sketch-Based Image Retrieval, Generative Adversarial Networks (GANs)", "summary": "## Overview\nJames Hays is a faculty researcher at **Georgia Tech**, where he leads work at the intersection of **Computer Vision**, **Machine Learning**, and *robotics-oriented perception*. His research group develops data-driven methods for visual understanding, with an emphasis on building and leveraging <u>large-scale datasets</u> that catalyze progress in recognition, scene understanding, and embodied/autonomous systems. Across influential contributions to benchmarks and algorithms, Hays’ scholarship foregrounds **scalable supervision**, **context-aware perception**, and *practical evaluation frameworks* that connect fundamental vision research to real-world deployment.\n\n---\n\n## Research Areas\nHays’ research spans several tightly connected themes in modern vision and AI. A central pillar is <u>dataset-driven scene and object understanding</u>, exemplified by foundational efforts on **MS COCO** (“*Microsoft coco: Common objects in context*,” 2014), which reframed object recognition around contextualized, multi-object everyday scenes, and the **SUN** line of work (“*Sun database: Large-scale scene recognition from abbey to zoo*,” 2010; “*Sun attribute database: Discovering, annotating, and recognizing scene attributes*,” 2012), which broadened scene categorization and introduced attribute-level representations for deeper semantic interpretation. A second pillar is <u>autonomous driving perception and forecasting</u> through rich, map-aware benchmarks, including **Argoverse** (“*Argoverse: 3d tracking and forecasting with rich maps*,” 2019) and **Argoverse 2** (2023), supporting 3D tracking, motion forecasting, and multimodal sensor understanding. Complementing these dataset contributions are algorithmic investigations into *image synthesis and completion* (“*Scene completion using millions of photographs*,” 2007), <u>sketch understanding and sketch-based generation</u> (“*How do humans sketch objects?*,” 2012; “*The Sketchy database*,” 2016; “*Scribbler*,” 2017), and the foundations of modern generative modeling (“*On convergence and stability of GANs*,” 2017), reflecting a broader interest in how learning dynamics, supervision, and representations shape reliable visual intelligence.\n\n---\n\n## Notable Works\n- **Reframing object recognition with contextual, instance-level supervision** through the creation of **MS COCO** in “*Microsoft coco: Common objects in context*” (2014), a landmark benchmark enabling advances in <u>object detection</u>, <u>instance segmentation</u>, and captioning by emphasizing complex everyday scenes and standardized evaluation.\n- **Expanding scene understanding beyond narrow taxonomies** via the **SUN Database** and attribute modeling—“*Sun database: Large-scale scene recognition from abbey to zoo*” (2010) and “*Sun attribute database: Discovering, annotating, and recognizing scene attributes*” (2012)—which established broad coverage of scene categories and introduced <u>semantic attributes</u> as a bridge between low-level appearance and high-level interpretation.\n- **Accelerating self-driving research with map-rich 3D perception benchmarks** in “*Argoverse: 3d tracking and forecasting with rich maps*” (2019) and “*Argoverse 2: Next generation datasets for self-driving perception and forecasting*” (2023), enabling standardized study of <u>3D tracking</u>, <u>forecasting</u>, and multimodal perception under realistic urban conditions.\n\n---\n\n## Academic Background\nBased at **Georgia Tech**, James Hays has developed a research profile strongly associated with the maturation of data-centric computer vision, evidenced by sustained contributions to widely adopted benchmarks and evaluation practices across more than a decade of work. His publication trajectory—from early large-scale, internet-driven visual reasoning (“*Scene completion using millions of photographs*,” 2007; “*Im2gps*,” 2008) to major community resources for recognition and scene understanding (SUN, COCO), and later to autonomous driving datasets (Argoverse, Argoverse 2)—reflects deep engagement with *community-facing infrastructure* as a scholarly output. With **98,374 citations**, his influence is especially pronounced through high-impact, multi-institutional collaborations typical of field-defining dataset efforts, alongside continued algorithmic research in sketch understanding, generative modeling, and evaluation toolkits (e.g., error analysis frameworks for detection). Collectively, these activities position him as a leading academic contributor to <u>benchmark-driven progress</u> in **AI** and **computer vision**, with strong ties to both foundational research communities and robotics-adjacent application domains."}, "vlV_GoEAAAAJ": {"keywords": "Space, AI, Security, Edge Cloud, Distributed Systems, Mobile Edge Computing (MEC), IoT DDoS Mitigation, LEO Satellite Networks, Edge ML Serving, Trusted Execution Environments (TEE)", "summary": "## Overview\nKetan Bhardwaj is a researcher at the Georgia Institute of Technology, where he works on **distributed systems** at the intersection of **edge cloud computing**, **AI/ML systems**, and **security**. His work centers on building *practical, deployable* systems that push computation closer to users and devices while maintaining strong guarantees for performance and trust. Across mobile, IoT, and emerging space-based infrastructures, he advances <u>edge computing architectures</u> that address latency, bandwidth, and multi-tenancy constraints, with recurring emphasis on <u>secure execution</u> and <u>resource-efficient deployment</u> of modern services.\n\n---\n\n## Research Areas\nBhardwaj’s research spans systems mechanisms and end-to-end platforms for the edge, with contributions that connect **security**, **performance**, and **operational scalability**. In edge security and resilience, he has addressed large-scale threats originating from compromised devices, notably proposing edge-assisted defenses for application-layer attacks in *“Towards IoT-DDoS prevention using edge computing”* (2018), which frames prevention as a distributed, near-source capability rather than a purely centralized filtering problem. In edge provisioning and isolation, his work on *AirBox*—*“Fast, scalable and secure onloading of edge functions using airbox”* (2016)—argues for backend-driven “onloading” that enables rapid provisioning of edge functions while preserving strong protection boundaries suited to multi-tenant environments; this direction is complemented by later efforts to preserve end-to-end security properties in edge deployments (e.g., *“SPX: Preserving end-to-end security for edge computing”*, 2018). In AI-at-the-edge, he has explored methods to reduce the resource footprint and latency of DNN-powered services, including model partitioning and containerized deployment strategies in *“Couper: Dnn model slicing for visual analytics containers at the edge”* (2019) and subsequent work on lightweight runtimes and ML serving pressure in constrained edge sites (e.g., *“Toward lighter containers for the edge”*, 2020; *“Pocket: ml serving from the edge”*, 2023). More recently, he has extended edge computing concepts into *space systems*, investigating the dynamics and orchestration challenges of <u>LEO satellite edge/cloud</u> environments in *“Toward Loosely Coupled Orchestration for the LEO Satellite Edge”* (2020) and characterizing the routing variability that complicates protocol design in *“A characterization of route variability in LEO satellite networks”* (2023), alongside newer scheduling abstractions for a LEO compute cloud (e.g., *“Krios…”*, 2024; *“Ushering in the Low Earth Orbit Compute Cloud Era”*, 2025).\n\n---\n\n## Notable Works\n- **Edge-based mitigation of IoT-driven application-layer attacks:** In *“Towards IoT-DDoS prevention using edge computing”* (2018), Bhardwaj advances a systems approach that leverages edge resources to counter **IoT-sourced DDoS** traffic whose *seemingly legitimate* application behavior and distributed volume defeat many traditional defenses.\n- **Secure, scalable provisioning of edge functions:** In *“Fast, scalable and secure onloading of edge functions using airbox”* (2016), he proposes **backend-driven onloading** mechanisms that enable fast edge function deployment while strengthening isolation and security properties needed for multi-tenant edge infrastructure.\n- **Resource-aware AI deployment via DNN slicing at the edge:** In *“Couper: Dnn model slicing for visual analytics containers at the edge”* (2019), he introduces techniques for **DNN model slicing** to better match the heavy resource demands of visual analytics to heterogeneous edge environments, improving feasibility and responsiveness for latency-sensitive workloads.\n\n---\n\n## Academic Background\nAffiliated with the Georgia Institute of Technology, Bhardwaj’s publication trajectory reflects sustained engagement with the systems community spanning mobile/IoT platforms (e.g., *Personal Clouds*, *SOUL*, *AppSachet/AppFlux*), edge-cloud architecture and measurement (e.g., *ECC: Edge cloud composites*; *DNS does not suffice for MEC-CDN*), and security-preserving edge execution (e.g., *AirBox*, *SPX*, and later confidential-computing directions such as *“TGH: A TEE/GC hybrid enabling confidential FaaS platforms”*, 2023). His body of work—accumulating **762 citations**—indicates broad uptake of his ideas in both foundational edge design (function placement, provisioning, and orchestration) and emerging domains such as <u>LEO satellite networking and compute</u>. The progression from early mobile and app-delivery systems to multi-tenant MEC and space-edge scheduling suggests deepening leadership in **edge cloud** research, with collaborations and venues typical of top-tier distributed systems, networking, and edge-computing forums, as evidenced by repeated contributions on orchestration, container efficiency, ML serving, and performance characterization across the edge stack."}, "w55j5-8AAAAJ": {"keywords": "Compilers, compiler optimizations, embedded systems, secure processors, information leakage protection, control-flow obfuscation, hardware-assisted anomaly detection, software debloating, memory leak detection, parallel and distributed computing", "summary": "## Overview\nSantosh Pande is a Professor of Computer Science at Georgia Tech, where he leads research at the intersection of **compilers**, *systems*, and **computer architecture** with a sustained emphasis on <u>security- and performance-aware program transformation</u>. His work spans compiler–hardware co-design for embedded and high-performance platforms, developing *practical* mechanisms that translate program analyses into runtime and microarchitectural enforcement—ranging from protecting execution against information leakage to improving efficiency through architecture-conscious optimization. Across multiple research threads, Pande’s scholarship is unified by the goal of using **compiler intelligence** to make complex platforms more <u>secure</u>, <u>efficient</u>, and *robust* under real-world constraints.\n\n---\n\n## Research Areas\nPande’s research areas are anchored in compiler-driven techniques that directly influence hardware behavior and system-level outcomes. In **secure architectures and leakage mitigation**, he helped advance protection against side-channel exposure on interconnects with “**HIDE: an infrastructure for efficiently protecting information leakage on the address bus**” (2004), extending XOM-like secure execution by addressing address-bus leakage. Complementing confidentiality, his work on **software protection and control-flow integrity** introduced hardware/compiler collaboration for obfuscation in “**Hardware assisted control flow obfuscation for embedded processors**” (2004) and for attack detection in “**Anomalous path detection with hardware support**” (2005) as well as branch-correlation methods in “**Using branch correlation to identify infeasible paths for anomaly detection**” (2006). In **energy- and resource-aware compilation**, he explored compiler/ISA/microarchitecture cooperation to reduce static power via power-gating in “**Optimizing static power dissipation by functional units in superscalar processors**” (2002), and produced a sequence of influential compiler optimizations for embedded DSPs, including storage assignment and address-generation-aware transformations (e.g., “**Storage assignment optimizations to generate compact and efficient code on embedded DSPs**,” 1999; and related coalescence and expression-tree approaches). His portfolio also includes **parallel and distributed compilation/runtime systems**, spanning scheduling functional parallelism for distributed memory (“**A scalable scheduling scheme for functional parallelism on distributed memory multiprocessor systems**,” 2002; and Sisal-focused compilation work) and later work targeting modern heterogeneous systems such as multi-GPU scheduling (“**Case: A compiler-assisted scheduling framework for multi-gpu systems**,” 2022). In **software reliability and production diagnostics**, he contributed low-overhead tooling for real systems, notably “**Automated memory leak detection for production use**” (2014) and its machine-learning extension (2014). More recently, he has been a visible contributor to **security-focused software debloating and measurement**, including “**Carve**” (2019), “**Is less really more?**” (2019), and “**Blankit library debloating**” (2020), as well as work analyzing how compiler optimizations can unintentionally expand code-reuse attack surfaces (“**Not so fast**,” 2021). Together, these threads reflect a consistent agenda: compilers as a lever for <u>cross-layer optimization</u> across security, performance, and dependability.\n\n---\n\n## Notable Works\n- **Address-bus leakage protection via secure execution infrastructure:** In “**HIDE: an infrastructure for efficiently protecting information leakage on the address bus**” (2004), Pande advanced practical defenses for secure processors by targeting information leakage channels not adequately covered by then-current XOM-style mechanisms, helping shape thinking on <u>microarchitectural side-channel exposure</u> and mitigation.\n- **Compiler–architecture co-design for energy efficiency:** “**Optimizing static power dissipation by functional units in superscalar processors**” (2002) proposed coordinated compiler, ISA, and microarchitectural support to power-gate long-idle functional units—an early exemplar of <u>compiler-guided power management</u> for reducing static dissipation.\n- **Security-focused debloating for reducing attack surface in real software stacks:** Through “**Blankit library debloating: Getting what you want instead of cutting what you don’t**” (2020) and closely related work such as “**Carve**” (2019) and “**Is less really more?**” (2019), Pande helped push debloating toward practical deployment while interrogating how security gains should be measured beyond simplistic gadget-count reductions.\n\n---\n\n## Academic Background\nAs a long-standing faculty member at Georgia Tech, Pande has built an academic record characterized by sustained contributions to **compiler research** and its application to architecture, security, and parallel systems, reflected in a citation profile of roughly **2,475** total citations and influential publications spanning from the 1990s to the 2020s. The evolution of his work—from embedded DSP code generation and storage assignment optimizations (late 1990s), to distributed-memory scheduling and compiler/runtime co-design (early 2000s), to hardware-assisted security and anomaly detection (mid-2000s), and later to production-grade diagnostics and security measurement/debloating (2010s–2020s)—suggests deep and continuous engagement with both *foundational* compiler problems and applied systems challenges. His collaborations across compiler construction, microarchitecture, and security communities indicate strong interdisciplinary affiliations typical of leading computer systems researchers, with recurring emphasis on <u>practical, deployable mechanisms</u> that connect program analysis to real platform constraints."}, "xY3zIEAAAAAJ": {"keywords": "sonification, auditory displays, human-computer interaction, driving, psychology, auditory graphs, auditory menu navigation, spearcons, automated driving trust, assistive navigation for visual impairment", "summary": "## Overview\nBruce N. Walker is a researcher at the **Georgia Institute of Technology**, where he has led and collaborated within labs and interdisciplinary teams spanning **psychology**, **human–computer interaction**, and **auditory display** research (see his group materials at http://sonify.psych.gatech.edu/~walkerb/). His scholarship centers on the design, theory, and evaluation of **sonification** and broader **auditory displays**, with a sustained emphasis on *human performance* and *usability* in real-world contexts. Across highly cited foundational work and applied studies, Walker has advanced <u>data-to-sound mapping principles</u>, <u>accessible interaction techniques</u>, and <u>in-vehicle auditory interfaces</u>, contributing to a research portfolio with **9,742 citations** that bridges basic perceptual science and deployable interactive systems.\n\n---\n\n## Research Areas\nWalker’s research spans (1) **foundational frameworks for sonification**, (2) **empirical design guidance for auditory graphs and mappings**, and (3) **auditory interaction in safety-critical and mobile settings**. In field-defining contributions such as *“Sonification report: Status of the field and research agenda”* (2010) and *“Theory of sonification”* (2011), his work helps formalize what sonification is, how it relates to auditory display more broadly, and what methodological standards are needed for cumulative progress. Complementing these agenda-setting efforts, Walker has produced a substantial body of experimental research on how listeners interpret sound as information—e.g., mapping and metaphor choices in *“Mappings and metaphors in auditory displays: An experimental assessment”* (2005), and psychophysical approaches to scaling and expectancy in *“Magnitude estimation of conceptual data dimensions for use in sonification”* (2002) and related scaling studies. A parallel, highly influential thread targets *interaction techniques and accessibility*: auditory graphs and tool support (e.g., *“Sonification Sandbox: A graphical toolkit for auditory graphs”* (2003) and work on adding context such as tick marks and labels), navigation and wayfinding using spatial audio (e.g., *“Swan: System for wearable audio navigation”* (2007) and studies of beacon design and practice effects), and menu interaction using compact speech-like cues (e.g., *“Spearcons (speech-based earcons) improve navigation performance in advanced auditory menus”* (2013), along with related spearcon/spindex work). In applied HCI and transportation, Walker’s research extends to **in-vehicle technologies**, driver workload and emotion (e.g., *“Effects of specific emotions on subjective judgment, driving performance, and perceived workload”* (2014)), and the measurement of trust and risk in automation (e.g., *“Situational trust scale for automated driving (STS-AD)”* (2020) and *“No risk no trust”* (2019)), reflecting a sustained interest in how <u>perception, cognition, and affect</u> shape safe interaction with complex systems.\n\n---\n\n## Notable Works\n- **Field synthesis and research agenda for sonification:** Co-shaping the discipline’s shared vocabulary, priorities, and methodological direction in *“Sonification report: Status of the field and research agenda”* (2010), a widely cited reference that helped consolidate sonification as a coherent research area and articulated key challenges for evaluation and adoption.\n- **Formalization of conceptual foundations:** Advancing definitional clarity and theoretical framing for auditory displays in *“Theory of sonification”* (2011), articulating how nonspeech audio can systematically represent data relations and what distinguishes sonification from other auditory interface strategies.\n- **High-impact auditory interaction technique for menus:** Demonstrating measurable performance gains and improved navigation in auditory interfaces through compact speech-derived cues in *“Spearcons (speech-based earcons) improve navigation performance in advanced auditory menus”* (2013), influencing subsequent work on accessible and eyes-busy/eyes-free interaction.\n\n---\n\n## Academic Background\nBased on his long-standing publication record anchored at the **Georgia Institute of Technology** and the disciplinary blend evident across his papers, Walker’s academic trajectory reflects deep training and affiliation in **experimental psychology/human factors** alongside **HCI** and **auditory perception** research communities. His early, heavily cited work on psychophysical scaling, magnitude estimation, and auditory graph comprehension (2000–2007) indicates a methodological foundation in controlled behavioral experimentation and perceptual measurement, while later contributions show increasing engagement with applied domains such as **wearable navigation**, **mobile interaction**, and **driving/automation**. His influence is also reflected in cross-domain collaborations—ranging from accessibility-oriented auditory graph design to in-vehicle interface evaluation and trust measurement—suggesting active participation in interdisciplinary venues that connect psychology, engineering, and interaction design. While specific degrees and honors are not enumerated here, the breadth and impact of his scholarship (9,742 citations) and his role in producing agenda-setting and theory-building publications strongly indicate recognition as a leading contributor within the **auditory display/sonification** and **human-centered technology** research communities."}, "xkCYK58AAAAJ": {"keywords": "Cybersecurity, Distributed systems, Distributed shared memory, Memory consistency models, Causal consistency, Replicated data management, Quorum consensus protocols, Role-based access control (RBAC), Malware analysis and web threat detection, Telephony security and fraud detection", "summary": "## Overview\nMustaque Ahamad is a faculty researcher at the Georgia Institute of Technology whose work sits at the intersection of **cybersecurity** and **distributed systems**, with a long-standing emphasis on the theory and practice of scalable distributed computing. Across multiple decades of scholarship, he has advanced foundational models for *reasoning about concurrency* and built systems that translate these models into usable platforms. His research is unified by a focus on <u>consistency, replication, and trust</u> in distributed environments—ranging from **weak memory consistency** (e.g., *causal memory*) to **security architectures** for emerging ubiquitous and context-aware applications, and later to empirical security studies of malware, telephony abuse, and misinformation.\n\n---\n\n## Research Areas\nAhamad’s research contributions span several tightly connected areas of distributed computing and security. In distributed systems, he helped formalize and operationalize weak consistency models that improve performance and scalability, notably through the development and articulation of *causal consistency* in “**Causal memory: Definitions, implementation, and programming**” (1995) and related work on implementing causal DSM (“**Implementing and programming causal distributed shared memory**,” 1991). This line of work is complemented by studies of alternative memory models and their guarantees, including “**Slow memory: Weakening consistency to enhance concurrency in distributed shared memories**” (1990) and “**The power of processor consistency**” (1993), reflecting a sustained interest in the <u>tradeoff between correctness guarantees and scalable performance</u>. In replicated data management, his work on quorum methods and replication protocols—such as “**The grid protocol: A high performance scheme for maintaining replicated data**” (1992) and “**Performance characterization of quorum-consensus algorithms for replicated data**” (2002)—addresses availability/latency tensions under failures and network variability. On the systems side, he contributed to distributed operating system design with the **Clouds** object-thread model (“**The Clouds distributed operating system**,” 2002; and earlier design/implementation work from 1990), and to practical distributed middleware performance (e.g., efficient Java RMI, 1998). In security, Ahamad’s research developed access control and policy frameworks for pervasive computing, including “**Generalized role-based access control for securing future applications**” (2000), “**Securing context-aware applications using environment roles**” (2001), and “**A context-aware security architecture for emerging applications**” (2002), which emphasize *context as a first-class input* to authorization and enforcement. His later cybersecurity work broadened into measurement and defense in real-world ecosystems—web malware delivery paths (“**WebWitness**,” 2015), session security (“**One-time cookies**,” 2012), enterprise detection of command-and-control (“**ExecScent**,” 2013), telephony fraud and provenance (e.g., “**Pindr0p**,” 2010; “**Sok: Fraud in telephony networks**,” 2017), and the dynamics of online misinformation (“**The role of the crowd in countering misinformation**,” 2020; and subsequent response-generation work, 2023). Taken together, these themes reflect an agenda centered on <u>building distributed systems that remain secure, usable, and performant under realistic adversarial and operational constraints</u>.\n\n---\n\n## Notable Works\n- **Causal consistency as a scalable shared-memory abstraction:** In “**Causal memory: Definitions, implementation, and programming**” (1995), Ahamad articulated key definitions and implementation strategies for *causal memory*, clarifying how causal ordering can provide useful semantics while avoiding the scalability costs of stronger consistency models.\n- **High-performance replication protocols for availability and low latency:** “**The grid protocol: A high performance scheme for maintaining replicated data**” (1992) introduced a logical-grid organization for replicas to improve response time while maintaining high availability, contributing to the broader study of <u>replication/consistency tradeoffs</u> in distributed data systems.\n- **Context-aware security and generalized roles for emerging pervasive applications:** Through “**Generalized role-based access control for securing future applications**” (2000) and “**Securing context-aware applications using environment roles**” (2001), Ahamad advanced role-based models that incorporate environmental and contextual signals, shaping how *ubiquitous computing* systems can enforce flexible, policy-driven protection.\n\n---\n\n## Academic Background\nBased on a publication record spanning from foundational distributed operating systems and fault tolerance in the late 1980s (e.g., object-based OS resilience and checkpointing/rollback recovery) through major contributions to weak consistency, replicated data, and distributed middleware in the 1990s, Ahamad’s academic trajectory reflects deep training and sustained leadership in core distributed systems research. His long-term affiliation with the Georgia Institute of Technology aligns with a mature research program that bridges theory (consistency definitions, logical clocks such as “**Plausible clocks**,” 1999) and systems-building (the **Clouds** distributed operating system, and practical performance work in Java RMI). The later expansion of his portfolio into cybersecurity—covering access control for context-aware computing (2000–2002), web and enterprise malware measurement and defenses (2012–2015), telephony security and fraud (2010–2017), and misinformation studies (2020–2023)—suggests a pattern of interdisciplinary collaboration and engagement with real-world threat ecosystems. With **9,601 citations**, his work has achieved broad impact across distributed systems and security communities, consistent with a senior academic profile marked by influential publications, sustained research visibility, and long-standing participation in major scholarly networks within these fields."}, "yaCigtkAAAAJ": {"keywords": "Educational Technology, Learning at Scale, AI in Education, Online Education, Computer Science Education, Conversational AI Teaching Assistants, Instructional Video Design, Scalable Assessment and Feedback, Academic Integrity and Plagiarism Detection, Learning Analytics and Sentiment Analysis", "summary": "## Overview\nDavid A. Joyner is a researcher at the **Georgia Institute of Technology**, widely associated with Georgia Tech’s large-scale online learning ecosystem and the broader community advancing *learning at scale*. His work sits at the intersection of **Educational Technology**, **AI in Education**, and **Computer Science Education**, with a sustained focus on how to design, deliver, and evaluate high-quality instruction when enrollments and learner diversity expand dramatically. Across studies of online degrees, course platforms, and instructional media, Joyner’s scholarship foregrounds <u>scalable pedagogy</u>—including assessment, feedback, integrity, and community-building—while also examining how *human-AI interaction* reshapes students’ learning experiences and perceptions of instruction.\n\n---\n\n## Research Areas\nJoyner’s research portfolio spans several tightly connected themes in at-scale education. A central thread is the design of instructional experiences that preserve rigor and learner support in massive online contexts, exemplified by his work on feedback and evaluation systems (e.g., “**Scaling expert feedback: Two case studies**,” 2017; “**Graders as meta-reviewers: Simultaneously scaling and improving expert evaluation for large online classrooms**,” 2016) and on assessment design constraints in large programs (“**Components of assessments and grading at scale**,” 2021). He has also contributed foundational analyses of online degree models and their operational realities, including policy and workflow considerations (“**Squeezing the limeade: policies and workflows for scalable online degrees**,” 2018) and longitudinal perspectives on scalable graduate education (“**Master's at scale: Five years in a scalable online graduate degree**,” 2019; “**Meet Me in the Middle: Retention in a ‘MOOC-Based’ Degree Program**,” 2022). In parallel, Joyner examines instructional media and course design for online learning, particularly the pedagogy of video-based instruction (“**Designing and developing video lessons for online learning: A seven-principle model**,” 2019; “**Designing videos with pedagogical strategies: Online students' perceptions of their effectiveness**,” 2016). Another prominent line of inquiry addresses integrity and authenticity in online CS education, including scalable plagiarism detection and the boundary between collaboration and misconduct (“**TAPS: A MOSS extension for detecting software plagiarism at scale**,” 2016; “**Collaboration versus cheating: Reducing code plagiarism in an online MS computer science program**,” 2019; “**On the necessity (or lack thereof) of digital proctoring**,” 2022). Finally, Joyner’s AI-in-education work investigates how learners interpret and relate to conversational agents and generative AI, from theory-driven analyses of student perceptions of virtual teaching assistants (“**Towards mutual theory of mind in human-ai interaction**,” 2021) to broader reflections on generative systems in schooling (“**ChatGPT in Education: Partner or pariah?**,” 2023) and practitioner-oriented guidance (“**A Teacher’s Guide to Conversational AI**,” 2024).\n\n---\n\n## Notable Works\n- **Mutual Theory of Mind for human–AI educational interaction**: In “*Towards mutual theory of mind in human-ai interaction: How language reflects what students perceive about a virtual teaching assistant*” (2021), Joyner advances a framework for understanding long-term human–AI interaction in learning contexts, showing how students’ language can reveal evolving perceptions of a virtual teaching assistant and informing the design of more natural, community-facing educational agents.\n- **Evidence-based model for instructional video design at scale**: In “*Designing and developing video lessons for online learning: A seven-principle model*” (2019), Joyner and colleagues synthesize and test a principled approach to video lesson creation, addressing a core instructional component of online courses where video often functions as the primary delivery mechanism.\n- **Design lessons from teaching AI online in a scalable graduate program**: In “*Using AI to teach AI: Lessons from an online AI class*” (2017), Joyner documents and analyzes the design of Georgia Tech’s online Knowledge-Based AI course (CS7637), detailing how learning-science-informed practices and course infrastructure can support rigorous AI instruction in an online, large-enrollment environment.\n\n---\n\n## Academic Background\nBased on his long-running publication record anchored at the **Georgia Institute of Technology** and sustained engagement with Georgia Tech’s scalable online initiatives, Joyner’s academic trajectory is closely tied to the rise of the institute’s large-scale online CS education—particularly the OMSCS-era shift toward accredited, low-cost, high-enrollment graduate learning. His work reflects deep interdisciplinary grounding in *learning sciences*, **educational data/analytics**, and **AI-enabled learning support**, with collaborations spanning instructional design, human–computer interaction, and computer science education research. The breadth of his output—from early work on complex systems learning technologies (e.g., ecosystem and modeling toolkits) through mature contributions on at-scale degree operations, assessment, integrity, and conversational agents—suggests an evolving research program that bridges foundational educational theory with pragmatic systems design. With **2,059 citations**, Joyner’s scholarship demonstrates sustained influence across the educational technology and learning-at-scale communities, and his publication history indicates strong institutional affiliation and leadership within Georgia Tech’s online education research and practice ecosystem."}}